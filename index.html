<!DOCTYPE HTML>
<html lang="en">

<head>
   <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Jianming Tong</title>
   <meta name="author" content="Jianming Tong">
   <meta name="description" content="PhD student at
         Georgia Tech.">
   <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E9VGTE8CH0"></script>
<script>
   window.dataLayer = window.dataLayer || [];
   function gtag() { dataLayer.push(arguments); }
   gtag('js', new Date());

   gtag('config', 'G-E9VGTE8CH0');
</script>

<script type="text/javascript">
   function displayid(id){
   var erv = document.getElementById(""+id+"");
   if(erv.style.display=="none"){
   erv.style.display="";
   }
   else{
   erv.style.display="none";
   }
   }
</script>

<body>
   <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
         <tr style="padding:0px">
            <td style="padding:0px">
               <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>
                     <tr style="padding:0px">
                        <td style="padding:2.5%;width:65%;vertical-align:middle">
                           <p style="text-align:center">
                              <name>Jianming Tong</name> <br>
                              <email>jianming [dot] tong [at] gatech [dot] edu</email>
                           </p>
                           <p> Ph.D. at Georgia Tech starting from Spring 2021, Visitng Researcher at MIT </p>
                           <p> Advisor: <a href="https://tusharkrishna.ece.gatech.edu/"> Tushar Krishna </a></p>
                           <p> Main Developer for  <a href="https://github.com/google/jaxite">CROSS</a>,  <a href="https://github.com/maeri-project/FEATHER">FEATHER</a></p>
                           <p> My research is funded by <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america"> Qualcomm Innovation Fellowship </a> and <a href="https://www.src.org/program/jump2/"> SRC Jump 2.0 </a>  </p>
                           <p> FPGA and FHE Lead in Synergy Lab @ Gatech</p>
                           <p style="text-align:center">
                              <a href="data/CV_Jianming_Tong.pdf">CV</a> &nbsp/&nbsp
                              <a href="https://scholar.google.com/citations?user=DaY9qQwAAAAJ&hl=zh-CN/"> Google Scholar </a>&nbsp/&nbsp
                              <a href="https://github.com/JianmingTong/"> GitHub </a> &nbsp/&nbsp
                              <a href="https://www.linkedin.com/in/jianming-tong-a08a59186/"> LinkedIn </a> &nbsp/&nbsp
			                        <a href="https://orcid.org/my-orcid?orcid=0000-0001-8436-2946"> ORCID </a> 
                           </p>
                        </td>
                        <td style="padding:2.5%;width:150%;max-width:150%">
                           <a href="images/profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
                        </td>
                     </tr>
                  </tbody>
               </table>
               <!-- Publications & Manuscripts -->
               <br>

               <table style="width:101%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr>
                     <td style="padding:20px;width:100%;vertical-align:middle">
                     <heading>Research Interest</heading> 
                     <br>
                        <br>
                          I'm a <b>Computer Architect</b>, focusing on <b>improving the efficiency of privacy-preserving AI systems</b> via full-stack optimizations.
                        <br>
                        <li><b>Model (Software):</b> New ML Model with Privacy-preserving Capability by Design, e.g. Crypto-friendly ML Model. [<a href="https://github.com/TorchFHE/SmartPAF/blob/main/SmartPAF.pdf">SmartPAF-MLSys'24</a>][<a href="https://drive.google.com/file/d/16k5ifu6K-r4wshE8Vmun1l9ab80RabEw/view?usp=drive_link">Privatar-UsenixSecurity'25</a>]</li>
                        </li>
                        <li><b>System: </b> Latency/Accuracy/Privacy Navigation for Multi-Query Streams. [<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=40">SUSHI-IEEE Micro'23</a>, <a href="publications/SUSHI_MLSys2023.pdf">MLSys'23</a>]
                        </li>
                        <li><b>Compilation: </b> Convert Crypto Algorithm to be More Efficient on Existing Hardware. [<a href="https://arxiv.org/abs/2501.07047">CROSS</a>]
                        </li>
                        <li><b>Architecture (Hardware):</b> Reconfigurable Dataflow Accelerator for ML and Privacy-preserving ML. [<a href="https://arxiv.org/abs/2405.13170">FEATHER-ISCA'24</a>]
                        </li>
                        <li><b>Performance Modeling:</b> Performance Analysis Tool for ML Accelerators. [<a href="https://github.com/maeri-project/FEATHER/tree/main/LayoutLoop">LayoutLoop-ISCA'24</a>][<a href="https://github.com/scalesim-project/scale-sim-v2">ScaleSim-ISPASS'25</a>][<a href="https://github.com/maeri-project/squareloop">SquareLoop-HASP'25</a>]
                        </li>
                      <br>
                      <img src="./images/research_overview_new.png" width="80%" style="display: block;
                                       margin-left: auto; margin-right: auto;">
               </table>
               <table style="width:101%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <heading>News</heading>  
                    <div style="height:500px;overflow-y:auto">
                    <p></p><ul>      
                    <li> [Oct. 2025] <b style="color:#ff6600;">[talk]</b> I presented a poster on  <a href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for Homomorphic Encryption</a> at <a href="https://acecenter.grainger.illinois.edu/"> ACE Annual Review @ Chicago</a></li> 
                    <li> [Oct. 2025] <b style="color:#ff6600;">[DEMO]</b> I give a <b>multi-university demo</b> on  <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a href="https://acecenter.grainger.illinois.edu/"> ACE Annual Review @ Chicago</a> with Prof. <a href="https://www.csl.cornell.edu/~zhiruz/"> Zhiru Zhang </a>, Prof.  <a href="https://charithmendis.com/"> Charith Mendis </a>, and Prof.  <a href="https://profiles.stanford.edu/subhasish-mitra"> Subhasish Mitra </a>, big thanks to the team <a href="https://devansh-dvj.github.io/"> Devansh Jain</a>, <a href="https://www.zzzdavid.tech/">Niansong Zhang</a>, <a href="https://chhzh123.github.io/">Hongzheng Chen</a> and  <a href="https://scholar.google.com/citations?user=_sPARagAAAAJ&hl=en">Saranyu Chattopadhyay</a>! </li> 
                    <li> [Sep. 2025] <b style="color:#ff6600;">[Talk]</b> I gave a talk on  <a href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for Homomorphic Encryption</a> at <a href="https://www.src.org/calendar/e007206/"> TechCon </a> See u'all at Austin!</li> 
                    <li> [Sep. 2025] <b style="color:#ff6600;">[Talk]</b> I give a talk on  <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a href="https://www.escalab.org/wddsa2024/"> UT Austin </a> hosted by Prof. <a href="https://www.ece.utexas.edu/people/faculty/mattan-erez"> Mattan Erez</a>!</a></li> 
                    <li> [Sep. 2025] <b style="color:#82B366;">[Paper]</b> Our work  <a href="https://github.com/maeri-project/squareloop"><strong>SquareLoop</strong>: Explore Optimal Authentication Block Strategy for ML</a> is being accepted to Hardware and Architectural Support for Security and Privacy (<a href="https://haspworkshop.org/2025/index.html"> HASP'25 </a>), co-located with <a href="https://microarch.org/micro58/"> MICRO'25 </a> at Seoul!</a></li> 
                    <li> [Aug. 2025] <b style="color:#FF2400;">[Award]</b> Our work  <a href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for Homomorphic Encryption</a> won the GT NEXT Award in recognition of our commitment to research and development that has the potential to significantly contribute to societal betterment! Go Yellow Jackets!</li> 
                    <li> [Jul. 2025] <b style="color:#82B366;">[Poster]</b> Our work <a href="https://drive.google.com/file/d/1OlTjT3jmIzLnLiKex30k_ySSOfIFwacL/view?usp=sharing"><strong>Privatar</strong>: Enabling Privacy-preserving Real-time Multi-user VR via Secure Outsourcing</a> is being accepted at <a href="https://www.usenix.org/conference/usenixsecurity25/technical-sessions"> Usenix Security'25 </a> as <a href="https://drive.google.com/file/d/1OlTjT3jmIzLnLiKex30k_ySSOfIFwacL/view?usp=sharing"> poster</a>! See u'all Aug 13~15 at Seattle!</li> 
                    <li> [Jul. 2025] <b style="color:#82B366;">[Poster]</b> Our work <a href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for Homomorphic Encryption and Zero Knowledge Proof</a> is being accepted at <a href="https://www.usenix.org/conference/usenixsecurity25/technical-sessions"> Usenix Security'25 </a> as <a href="https://drive.google.com/file/d/1eOhzb3PBCxfMX0WGCzqqD8dwMqHoRZgV/view?usp=sharing"> poster</a>! See u'all Aug 13~15 at Seattle!</li> 
                    <li> [Jul. 2025] <b style="color:#9673A6;">[Service]</b> I co-interview Prof. Mengjia Yan on behalf of TcuArch and IEEE Micro <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=DaY9qQwAAAAJ&citation_for_view=DaY9qQwAAAAJ:5nxA0vEk-isC">Sipping Matcha of Security: A Fireside Chat With Mengjia Yan</a> goes online! Check out the video recording here <a href="https://www.youtube.com/watch?v=oNj1fI_2tDQ"><strong>Video</strong></a>! </a></li>
                    <li> [Jun. 2025] <b style="color:#FF2400;">[Award]</b> Our work  <a href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for Homomorphic Encryption</a> won 2rd place at Unversity DEMO at <a href="https://www.dac.com/"> DAC'25 </a>! U could run encrypted digit detction serving on Google Cloud with TPUv4 for free today! </li> 
                    <li> [Jun. 2025] <b style="color:#ff6600;">[DEMO]</b> I will give a demo on  <a href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for Homomorphic Encryption</a> at <a href="https://www.dac.com/"> DAC'25 </a> See u all at SF!</li> 
                    <li> [May. 2025] <b style="color:#ff6600;">[Talk]</b> I gave a talk on  <a href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for Homomorphic Encryption and Zero Knowledge Proof</a> at <a href="https://cse.engin.umich.edu/"> UMich </a> hosted by Prof. <a href="https://web.eecs.umich.edu/~austin/"> Todd Austin</a>!</a></li> 
                    <li> [Mar. 2025] <b style="color:#82B366;">[Tool]</b> LayoutLoop from <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> [ISCA'24] has been integrated into <a href="https://github.com/NVlabs/timeloop"><strong>NVlabs/Timeloop</strong></a>, details could be found at this <a href="https://github.com/NVlabs/timeloop/pull/301"><strong>PR</strong></a> and this <a href="https://docs.google.com/presentation/d/1L3QLatmdSMooeVGd1ixgejq0fpmbsWQslSx8S5qZUKI/edit?usp=sharing"><strong>slide</strong></a>, enjoy precise layout modeling!</li>
                    <li> [Mar. 2025] <b style="color:#82B366;">[Paper]</b> Our work <a href="https://github.com/NVlabs/timeloop"><strong>Constrained Dataflow Accelerator for Real-Time Multi-Task Multi-Model Machine Learning Workloads</strong></a> has been accepted by ISPASS'25!</li>
                    <li> [Mar. 2025] <b style="color:#82B366;">[Paper]</b> Our work <a href="https://github.com/scalesim-project/scale-sim-v2"><strong>Scale-sim V3</strong></a> has been accepted by ISPASS'25!</li>
                    <li> [Jan. 2025] <b style="color:#82B366;">[Paper]</b> Our work <a href="https://arxiv.org/pdf/2501.07047"><strong>Leveraging ASIC AI Chips for Homomorphic Encryption</strong></a> has online released now!</li>
                    <li> [Nov. 2024] <b style="color:#9673A6;">[Teaching]</b> I gave a guest lecture of <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at the "Advanced Computer Architecture for Machine Learning" course hosted by Prof. <a href="https://www.tonytgeng.com/"><strong>Tony Geng</strong></a>.</li>
                    <li> [Nov. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on  <a href="https://engineering.nyu.edu/events/2024/11/14/leveraging-tpu-homomorphic-encryption"><strong>Leveraging AI ASIC for Homomorphic Encryption</strong></a> at <a href="https://www.nyu.edu/"> NYU </a> hosted by <a href="https://brandonreagen.com/"><strong>Prof. Brandon Reagon</strong></a> and <a href="https://engineering.nyu.edu/student/karthik-garimella"><strong>Karthik Garimella</strong></a>!</li> 
                    <li> [Nov. 2024] <b style="color:#9673A6;">[Service]</b> I co-organized <strong>JOBS Workshop</strong> to help faciliating new grads for job hunting - go <a href="https://sites.google.com/cornell.edu/jobs-workshop-2024/home?authuser=0"><strong>JOBS</strong></a>! </a></li>
                    <li> [Nov. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on  <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a href="https://www.escalab.org/wddsa2024/"> WDDSA </a> workshop co-located with <a href="https://microarch.org/micro57/"> MICRO'24 </a> at Austin!</a></li> 
                    <li> [Oct. 2024] <b style="color:#ff6600;">[Talk]</b> I demo  <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a href="https://www.src.org/liaison/"> SRC </a> ACE annual review <a href="https://acecenter.grainger.illinois.edu/index.asp"> ACE </a> at Chicago!</a></li> 
                    <li> [Sep. 2024] <b style="color:#82B366;">[Paper]</b> Our work <papertitle> <a href="https://ieeexplore.ieee.org/document/10672547"> Real-time Digital RF Emulation – II: A Near Memory Custom Accelerator </papertitle> is accepted to the IEEE Transactions on Radar Systems (<a href="https://ieee-aess.org/publication/ieee-transactions-radar-systems"><strong>TRadar'24</strong></a>).</li>
                    <li> [Aug. 2024] <b style="color:#FF2400;">[Award]</b> I was selected as the student for ACE Newsletter highlight by SRC!</li> 
                    <li> [Aug. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on  <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a href="https://www.src.org/liaison/"> SRC </a> Liaison Meeting of <a href="https://acecenter.grainger.illinois.edu/index.asp"> ACE </a>  Center!</a></li> 
                    <li> [Aug. 2024] <b style="color:#9673A6;">[Career]</b> I join Google as a student researcher in Phazon team of PSS, more realistic privacy-preserving acceleration are coming, stay tuned!</li>
                    <li> [Jul. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on  <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at NVidia (HQ) and NVidia (Westford)!</a></li> 
                    <li> [Jun. 2024] <b style="color:#ff6600;">[Talk]</b> We debut FEATHER <papertitle href="https://arxiv.org/abs/2405.13170">A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip Dataflow Switching</papertitle> at ISCA, Buenos Aires!</a></li> 
                    <!-- <li> [May. 2024] <b style="color:#ff6600;">[Talk]</b> I give the talk on <papertitle>Leveraging AI ASIC for Fully Homomorphic Encryption</papertitle> at Google, IBM</a></li>  -->
                    <li> [May. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at MIT</a></li> 
                    <li> [May. 2024] <b style="color:#FF2400;">[Award]</b> I am selected as "<a href="https://mlcommons.org/2024/06/2024-mlc-rising-stars/"><strong>ML and System Rising Star</strong></a>" by ML Commons, excited to meet you all at Nvidia HQ at Jul 15~16.</li>
                    <li> [May. 2024] <b style="color:#FF2400;">[Award]</b> Our team "<a href="https://sites.gatech.edu/cipher-flit-fort/"><strong>CipherFlitFort</strong></a>" is awarded Startup Launch by <a href="https://create-x.gatech.edu/launch/startup-launch"><strong>CreateX</strong></a> at Georgia Tech, Go Jackets! </li>
                    <li> [Apr. 2024] <b style="color:#FF2400;">[Award]</b> I am selected as <strong>DAC Young Fellow</strong> for DAC 2024. </li>
                    <li> [Mar. 2024] <b style="color:#82B366;">[Paper]</b> Our work <papertitle> <a href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a>: A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip Dataflow Switching</papertitle> is accepted to the International Symposium on Computer Architecture (<a href="https://iscaconf.org/isca2024/"><strong>ISCA'24</strong></a>).</li>
                    <li> [Feb. 2024] <b style="color:#82B366;">[Paper]</b> Our work <papertitle> <a href="https://arxiv.org/abs/2404.03216"><strong>SmartPAF</strong></a>: Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic Encryption</papertitle> is accepted to the Seventh Conference on Machine Learning and Systems (<a href="https://mlsys.org/"><strong>MLSys'24</strong></a>).</li>
                    <li> [Feb. 2024] <b style="color:#9673A6;">[Service]</b> We started course 6.192 <strong>Constructive Computer Architecture</strong> in three schools together this year (MIT, EPFL, GaTech) - <a href="https://mit.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%226c90dae2-5802-4e54-9e21-b10900d0907d%22"><strong> recordings </strong> </a> available online, Go Architects! </a></li>
                    <li> [Jan. 2024] <b style="color:#9673A6;">[Service]</b> I served <strong>AEC</strong> for <a href="https://iscaconf.org/isca2024/"><strong>ISCA'24</strong></a>.</li>
                    <li> [Nov. 2023] <b style="color:#9673A6;">[Service]</b> I join Computer Architecture Student Association (<a href="https://www.sigarch.org/casa/"><strong> CASA </strong> </a>) steering team, from the architects for the architects. </li> 
                    <li> [Oct. 2023] <b style="color:#ff6600;">[Talk]</b> I gave a talk on <papertitle href="https://arxiv.org/abs/2306.17266">SUSHI</papertitle> and <papertitle href="https://assets.researchsquare.com/files/rs-2910088/v1_covered_0e6c94bd-1499-4b2c-b414-902c232b490c.pdf?c=1683777877">PAF-FHE</papertitle> at <a href="https://hanlab.mit.edu/"><strong> HAN Lab @ MIT.</strong> </a></li> 
                    <li> [Sep. 2023] <b style="color:#FF2400;">[Award]</b> I won <strong>Best Poster Award</strong> for presenting our work <papertitle><a href="https://arxiv.org/abs/2306.17266">SUSHI</a></papertitle> at  (<a href="https://www.industry-academia.org/mit-2023.html"><strong>IAP Workshop@MIT</strong></a>).</li>
                    <li> [Sep. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>Hardware-Software co-design for real-time latency-accuracy navigation in tinyML applications</papertitle> is accepted to the Journal (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=40"><strong>IEEE micro</strong></a>).</li>
                    <li> [Sep. 2023] <b style="color:#9673A6;">[Career]</b> I join MIT as a visiting researcher in CSAIL hosted by Dr. <a href="http://csg.csail.mit.edu/Users/arvind/"><strong>Arvind</strong></a>.</li>
                    <li> [Aug. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>SNATCH: Stealing Neural Network Architecture from ML Accelerator in Intelligent Sensors</papertitle> is accepted to the IEEE SENSORS conference (<a href="https://2023.ieee-sensorsconference.org/"><strong>SENSORS'23</strong></a>).</li>
                    <li> [Jul. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>On Continuing DNN Accelerator Architecture Scaling Using Tightly-coupled Compute-on-Memory 3D ICs</papertitle> is accepted to the IEEE Transactions on Very Large Scale Integration Systems (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=92"><strong>TVLSI'23</strong></a>).</li>
                    <li> [Jul. 2023] <b style="color:#FF2400;">[Award]</b> I win 2023 <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america"><strong>Qualcomm Innovation Fellowship</strong></a>, thank you Qualcomm!</li>
                    <li> [Jul. 2023] <b style="color:#9673A6;">[Service]</b> I serve as <strong>AEC</strong> for <a href="https://www.asplos-conference.org/asplos2024/"><strong>ASPLOS'24</strong></a>.</li>
                    <li> [Jun. 2023] <b style="color:#ff6600;">[Talk]</b> I gave a talk on <papertitle href="https://arxiv.org/abs/2306.17266">SUSHI</papertitle> and <papertitle href="https://assets.researchsquare.com/files/rs-2910088/v1_covered_0e6c94bd-1499-4b2c-b414-902c232b490c.pdf?c=1683777877">PAF-FHE</papertitle> at <a href="https://gr.xjtu.edu.cn/en/web/pengjuren/home"><strong> CAG Lab @ XJTU University.</strong> </a></li> 
                    <li> [May. 2023] <b style="color:#82B366;">[Paper]</b> Our work <a href="publications/LAMBDA_ODIW2023.pdf", style="color: #000000;">  <papertitle>A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip Dataflow Switching</papertitle> </a> accepted to the 3rd On-Device Intelligence Workshop (<a href="https://sites.google.com/g.harvard.edu/on-device-workshop-23">ODIW'23@<strong>MLSys'23</strong></a>).</li>
                    <li> [May. 2023] <b style="color:#82B366;">[Paper]</b> Our work <a href="publications/ReLU_FHE_ODIW2023.pdf", style="color: #000000;">  <papertitle>ReLU-FHE: Low-cost Accurate ReLU polynomial approximation in Fully Homomorphic Encryption Based ML Inference</papertitle> </a> accepted to the 3rd On-Device Intelligence Workshop (<a href="https://sites.google.com/g.harvard.edu/on-device-workshop-23">ODIW'23@<strong>MLSys23</strong></a>) .</li>
                    <li> [Apr. 2023] <b style="color:#82B366;">[Paper]</b> Our work <a href="publications/SUSHI_MLSys2023.pdf", style="color: #000000;">  <papertitle>SUSHI: SubGraph Stationary Hardware-Software Inference Co-design</papertitle> </a> accepted to the Sixth Conference on Machine Learning and Systems (<a href="https://mlsys.org/virtual/2023/calendar?showDetail=true&filter_events=&filter_rooms="><strong>MLSys'23</strong></a>).</li>
                    <li> [Apr. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>FPGA-Based High-Performance Real-Time Emulation of Radar System using Direct Path Compute Model</papertitle> accepted to the International Microwave Symposium (<a href="https://ims-ieee.org/"><strong>IMS'23</strong></a>).</li>
                    <!-- <li> [Jan. 2023] <b style="color:#6C8EBF;">[Service]</b> I serve on the Technical Program Committee of DAC'23, Reviewer of IEEE TCAD, Steering Committee of <a href="https://entrepreneurship.ieee.org/2023_china-region-team-welcome-message/">IEEE Entrepreneurship China</a>.</li>  -->
                    <li> [Mar. 2023] <b style="color:#ff6600;">[Talk]</b> I give a talk on <papertitle>Enable Best ML Inference and Training: A systematic Approach</papertitle> at <a href="https://eiclab.scs.gatech.edu/"><strong>EIC Lab @ Georgia Tech.</strong></a></li> 
                    <li> [Mar. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>A High Performance Computing Architecture for Real-Time Digital Emulation of RF Interactions</papertitle> accepted to the In Proc of IEEE Radar Conference (<a href="https://radar2023.ieee-radarconf.org/"><strong>RadarConf'23</strong></a>).</li>
                    <li> [Nov. 2022] <b style="color:#ff6600;">[Talk]</b> I give a talk on <papertitle>Full-Stack ML Dataflow, Mapping and SW/HW Co-Design and Search</papertitle> at <a href="https://nicsefc.ee.tsinghua.edu.cn/"><strong> NICS-EFC Lab @ Tsinghua University.</strong></a></li> 
                    <li> [Jul. 2022] <b style="color:#D79B00;">[Tutorial]</b> I give a tutorial on <a href="https://maeri-project.github.io/", style="color: #000000;"><papertitle>MAERI 2.0: An End-to-end framework to explore architecture design space on FPGA</papertitle></a> at <a href="https://maeri-project.github.io/"><strong>ICS 2022</strong></a>.</li> 
                    <li> [Jul. 2022] <b style="color:#ff6600;">[Talk]</b> I present our work <papertitle>FastSwtich: Enabling Real-time DNN Switching via Weight-Sharing</papertitle> at the 2nd Architecture, Compiler, and System Support for Multi-model DNN Workloads Workshop Workshop @ <a href="https://research.facebook.com/architecture-compiler-and-system-support-for-multimodel-dnn-workloads-workshop/"> <strong>ISCA'23</strong></a> .</li>
                    <li> [Apr. 2022] <b style="color:#FF2400;">[Award]</b> I receive Finalist in Qualcomm Innovation Fellowship, thank you Qualcomm!</li>
                    <li> [Mar. 2022] <b style="color:#FF2400;">[Award]</b> I win 2nd place in SCS Poster Competition at Georgia Tech, thank you SCS!</li>
                    <li> [Nov. 2021] <b style="color:#82B366;">[Paper]</b> Our work <a href="https://ieeexplore.ieee.org/document/9561328", style="color: #000000;"><papertitle>A Configurable Architecture for Efficient Sparse FIR Computation in Real-time Radio Frequency Systems</papertitle></a> accepted to International Microwave Symposium (<a href="https://ims-ieee.org/"><strong>IMS'21</strong></a>).</li>
                    <li> [Aug. 2021] <b style="color:#82B366;">[Paper]</b> Our work <a href="https://ieeexplore.ieee.org/abstract/document/9609808", style="color: #000000;"><papertitle>ac2SLAM: FPGA Accelerated High-Accuracy SLAM with Heapsort and Parallel Keypoint Extractor</papertitle></a> accepted to <a href="http://www.icfpt.org/"><strong>FPT'21</strong></a>.<b><a href="https://github.com/SLAM-Hardware/acSLAM">[code]</a></b></li>
                    <li> [Mar. 2021] <b style="color:#82B366;">[Paper]</b> Our work <a href="https://ieeexplore.ieee.org/document/9561328", style="color: #000000;"><papertitle>SMMR-explore: Submap-based multi-robot exploration system with multi-robot multi-target potential field exploration method</papertitle></a> accepted to <strong>ICRA'21</strong>.<b><a href="https://github.com/efc-robot/SMMR-Explore">[code]</a></b><b><a href="https://www.youtube.com/watch?v=H1zwRIz8OYs">[demo]</a></b></li>
                    <li> [Mar. 2021] <b style="color:#10739E;">[Book]</b> Our translated book <papertitle>On-chip Network</papertitle></a> publicly released <a href="http://www.zxhsd.com/kgsm/ts/2021/01/29/5329526.shtml"><b>[purchase translated version] </b></a><a href="https://www.morganclaypool.com/doi/abs/10.2200/S00772ED1V01Y201704CAC040"><b>[English version -- Free for University] </b></a></li>
                    <li> [Feb. 2021] <b style="color:#82B366;">[Paper]</b> Our work <a href="https://ieeexplore.ieee.org/abstract/document/9311185", style="color: #000000;"><papertitle>PIT: Processing-In-Transmission with Fine-Grained Data Manipulation Networks</papertitle></a> accepted to  <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=12"><strong>ToC'21</strong></a>.</li>
                    <li> [Jan. 2021] <b style="color:#9673A6;">[Career]</b>  I kick-off my Ph.D. career at Georgia Tech, go Yellow Jackets!</li>
                    <li> [Dec. 2020] <b style="color:#82B366;">[Paper]</b> Our work <a href="https://dl.acm.org/doi/abs/10.1145/3386263.3406924", style="color: #000000;"><papertitle>COCOA: Content-Oriented Configurable Architecture Based on Highly-Adaptive Data Transmission Networks</papertitle></a> accepted to <a href="https://www.glsvlsi.org/archive/glsvlsi21/index.html"><strong>GLSVLSI'21</strong></a>.</li>
                  </div>
                  </td>
                </tr>
               </tbody></table>

              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
               <!-- <table width="100%" align="right" border="0" cellspacing="0" cellpadding="10"> -->
                  <!-- <tbody> -->
                     <heading>Leading Publications (* Equal Contribution)</heading>
                     <br>
                     <b> As Principal Contributor and Leading Author </b>
                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/cross_overview.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%" valign="middle">
                                       <div class="title">
                                          <b>Leveraging ASIC AI Chips for Homomorphic Encryption</b>
                                       </div>
                                       <div class="authors">
                                          <b>Jianming Tong</b>,
                                          <a href="https://www.linkedin.com/in/tianhao-huang-tech/">Tianhao Huang</a>,
                                          <a href="https://www.linkedin.com/in/leo-de-castro-962601190/">Leo De Castro</a>,
                                          <a href="https://www.linkedin.com/in/anirudh-itagi">Anirudh Itagi</a>,
                                          <a href="https://www.linkedin.com/in/jingtian-dang-568615207/">Jingtian Dang</a>,
                                          <a href="https://scholar.google.com/citations?user=QLN76UUAAAAJ&hl=en">Anupam Golder</a>,
                                          <a href="https://github.com/asraa">Asra Ali</a>,
                                          <a href="https://www.linkedin.com/in/jevin-jiang/">Jevin Jiang</a>,
                                          <a href="https://csg.csail.mit.edu/Users/arvind/">Arvind</a>,
                                          <a href="https://tsg.ece.cornell.edu/people/g-edward-suh/">G. Edward Suh</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                                       </div>
                                       <div class="venue">
                                          <i> USENIX Security Symposium (<b style="color:#ff6600;">USENIX Security</b>), Jul 2025.</i>
                                          <!-- <i> International Symposium on Computer Architecture (<b style="color:#ff6600;">ISCA</b>), Jun 2024.</i> -->
                                       </div>
                                       <div class="link">
                                          <b><a onclick="displayid('abs14');">[abstract]</a></b>
                                          <b><a href="https://arxiv.org/abs/2501.07047">[paper]</a></b>
                                          <b><a href="https://drive.google.com/file/d/1eOhzb3PBCxfMX0WGCzqqD8dwMqHoRZgV/view?usp=sharing">[poster]</a></b>
                                          <b><a href="https://github.com/google/jaxite">[code]</a></b>
                                          <!-- <b><a href="https://docs.google.com/presentation/d/1b_yGpuQd70Zo1Lj_AB6o9Y7BOjgpJ0oh8RGTD_7-yG4/edit?usp=sharing">[slide]</a></b> -->
                                          <!-- <b><a href="https://youtu.be/FgTaYiEArrI">[isca_talk]</a></b> -->
                                          <!-- <b><a href="https://youtu.be/FA6IMiRmQmw">[deep_dive_talk]</a></b> -->
                                          <b><a onclick="displayid('bib14');">[bibtex]</a></b>
                                          <div id="abs14" style="display:none;">
                                             Cloud-based services are making the outsourcing of sensitive client data increasingly common. Although homomorphic encryption (HE) offers strong privacy guarantee, it requires substantially more resources than computing on plaintext, often leading to unacceptably large latencies in getting the results. HE accelerators have emerged to mitigate this latency issue, but with the high cost of ASICs.  In this paper we show that HE primitives can be converted to AI operators and accelerated on existing ASIC AI accelerators, like TPUs,  which are already widely deployed in the cloud. Adapting such accelerators for HE requires (1) supporting modular multiplication, (2) high-precision arithmetic in software, and (3) efficient mapping on matrix engines. We introduce the CROSS compiler (1) to adopt Barrett reduction to provide modular reduction support using multiplier and adder, (2)  Basis Aligned Transformation (BAT) to convert high-precision multiplication as low-precision matrix-vector multiplication, (3) Matrix Aligned Transformation (MAT) to covert vectorized modular operation with reduction into matrix multiplication that can be efficiently processed on 2D spatial matrix engine. Our evaluation of CROSS on a Google TPUv4 demonstrates significant performance improvements, with up to 161x and 5x speedup compared to the previous work on many-core CPUs and V100. The kernel-level codes are open-sourced at https://github.com/google/jaxite.git.
                                          </div>
                                          <div id="bib14" style="display:none;">
                                             @misc{tong2025leveragingasicaichips,
                                                title={Leveraging ASIC AI Chips for Homomorphic Encryption}, 
                                                author={Jianming Tong and Tianhao Huang and Leo de Castro and Anirudh Itagi and Jingtian Dang and Anupam Golder and Asra Ali and Jevin Jiang and Arvind and G. Edward Suh and Tushar Krishna},
                                                year={2025},
                                                eprint={2501.07047},
                                                archivePrefix={arXiv},
                                                primaryClass={cs.CR},
                                                url={https://arxiv.org/abs/2501.07047}, 
                                          }
                                          </div>
                                          <div> 
                                            <i><b style="color:#ff6600;">++CROSS is Deployed in Google TPU Cloud</b></i>
                                         </div>
                                          <div> 
                                              <i><b style="color:#ff6600;">++CROSS won 2rd place at DAC university demo</b></i>
                                          </div>
                                          <div> 
                                            <i><b style="color:#ff6600;">++CROSS won the GT NEXT Award</b></i>
                                         </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>
                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/feather.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%" valign="middle">
                                       <div class="title">
                                          <b>FEATHER: A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip Dataflow Switching</b>
                                       </div>
                                       <div class="authors">
                                          <b>Jianming Tong</b>,
                                          <a href="https://www.linkedin.com/in/anirudh-itagi">Anirudh Itagi</a>,
                                          <a href="https://www.linkedin.com/in/pchatarasi/">Prasanth Chatarasi</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                                       </div>
                                       <div class="venue">
                                          <i> International Symposium on Computer Architecture (<b style="color:#ff6600;">ISCA</b>), Jun 2024.</i>
                                       </div>
                                       <div class="link">
                                          <b><a onclick="displayid('abs13');">[abstract]</a></b>
                                          <b><a href="https://arxiv.org/abs/2405.13170">[paper]</a></b>
                                          <b><a href="https://github.com/maeri-project/FEATHER">[code]</a></b>
                                          <b><a href="https://docs.google.com/presentation/d/1b_yGpuQd70Zo1Lj_AB6o9Y7BOjgpJ0oh8RGTD_7-yG4/edit?usp=sharing">[slide]</a></b>
                                          <b><a href="https://youtu.be/FgTaYiEArrI">[isca_talk]</a></b>
                                          <b><a href="https://youtu.be/FA6IMiRmQmw">[deep_dive_talk]</a></b>
                                          <b><a href="https://docs.google.com/presentation/d/1L3QLatmdSMooeVGd1ixgejq0fpmbsWQslSx8S5qZUKI/edit?usp=sharing">[LayoutLoop]</a></b>
                                          <b><a onclick="displayid('bib13');">[bibtex]</a></b>
                                          <div> 
                                            <i><b style="color:#ff6600;">++LayoutLoop is Integrated into NVLabs/Timeloop</b></i>
                                         </div>
                                          <div id="abs13" style="display:none;">
                                             The inference efficiency of diverse ML models over spatial accelerators boils down to the execution of different dataflows (i.e. different tiling, ordering, parallelism, and shapes). Using the optimal dataflow for every layer of workload can reduce latency by up to two orders of magnitude over a suboptimal dataflow. Unfortunately, reconfiguring hardware for different dataflows involves on-chip data layout reordering and datapath reconfigurations, leading to non-trivial overhead that hinders ML accelerators from exploiting different dataflows, resulting in suboptimal performance. To address this challenge, we propose FEATHER, an innovative accelerator that leverages a novel spatial array termed NEST and a novel multi-stage reduction network called BIRRD for performing flexible data reduction with layout reordering under the hood, enabling seamless switching between optimal dataflows with negligible latency and resources overhead. For systematically evaluating the performance interaction between dataflows and layouts, we enhance Timeloop, a state-of-the-art dataflow cost modeling and search framework, with layout assessment capabilities, and term it as Layoutloop. We model FEATHER into Layoutloop and also deploy FEATHER end-to-end on the edge ZCU104 FPGA. FEATHER delivers 1.27~2.89x inference latency speedup and 1.3~6.43x energy efficiency improvement compared to various SoTAs like NVDLA, SIGMA and Eyeriss under ResNet-50 and MobiletNet-V3 in Layoutloop. On practical FPGA devices, FEATHER achieves 2.65/3.91x higher throughput than Xilinx DPU/Gemmini. Remarkably, such performance and energy efficiency enhancements come at only 6% area over a fixed-dataflow Eyeriss-like accelerator. Our code is available at https://github.com/maeri-project/FEATHER.
                                          </div>
                                          <div id="bib13" style="display:none;">
                                             @inproceedings{tong2024FEATHER,
                                                author = {Tong, Jianming and Itagi, Anirudh and Chatarasi, Parsanth and Krishna, Tushar},
                                                title = {FEATHER: A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip Dataflow Switching},
                                                year = {2024},
                                                publisher = {Association for Computing Machinery},
                                                address = {Argentina},
                                                abstract = {The inference of ML models composed of diverse structures, types, and sizes boils down to the execution of different dataflows (i.e. different tiling, ordering, parallelism, and shapes). Using the optimal dataflow for every layer of workload can reduce latency by up to two orders of magnitude over a suboptimal dataflow. Unfortunately, reconfiguring hardware for different dataflows involves on-chip data layout reordering and datapath reconfigurations, leading to non-trivial overhead that hinders ML accelerators from exploiting different dataflows, resulting in suboptimal performance. To address this challenge, we propose FEATHER, an innovative accelerator that leverages a novel spatial array termed Nest and a novel multi-stage reduction network called BIRRD for performing flexible data reduction with layout reordering under the hood, enabling seamless switching between optimal dataflows with negligible latency and resources overhead. For systematically evaluating the performance interaction between dataflows and layouts, we enhance Timeloop, a state-of-the-art dataflow cost modeling and search framework, with layout assessment capabilities, and term it as Layoutloop. We model FEATHER into Layoutloop and also deploy FEATHER end-to-end on the edge ZCU104 FPGA. FEATHER delivers 1.27~2.89x inference latency speedup and 1.3~6.43x energy efficiency improvement compared to various SoTAs like NVDLA, SIGMA and Eyeriss under ResNet-50 and MobiletNet-V3 in Layoutloop. On practical FPGA devices, FEATHER achieves 2.65/3.91x higher throughput than Xilinx DPU/Gemmini. Remarkably, such performance and energy efficiency enhancements come at only 6% area over a fixed-dataflow Eyeriss-like accelerator.},
                                                booktitle = {Proceedings of the 51th Annual International Symposium on Computer Architecture},
                                                keywords = {flexible accelerator, dataflow-layout coswitching},
                                                location = {Argentina},
                                                series = {ISCA '24}
                                                }
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>

                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/smartpaf.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%" valign="middle">
                                       <div class="title">
                                          <b>SmartPAF: Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic Encryption</b>
                                       </div>
                                       <div class="authors">
                                          <b>Jianming Tong*</b>,
                                          <a href="https://www.linkedin.com/in/jingtian-dang-568615207/">Jingtian Dang*</a>,
                                          <a href="https://scholar.google.com/citations?user=QLN76UUAAAAJ&hl=en">Anupam Golder</a>,
                                          <a href="https://sites.gatech.edu/ece-callie/">Callie Hao</a>,
                                          <a href="https://scholar.google.com/citations?hl=en&user=lXUce-0AAAAJ">Arijit Raychowdhury</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                                       </div>
                                       <div class="venue">
                                          <i> In Proc of Seventh Conference on Machine Learning and Systems, (<b style="color:#ff6600;">MLSys</b>), May 2024.</i>
                                       </div>
                                       <!-- <div class="insight">
                                          <i> <b> Insight 1: </b> Weights-sharing neural network enables a dynamic latency-accuracy serving.</i> <br>
                                          <i> <b> Insight 2: </b> Putting common weights on-chip (Subgraph Stationary) could save off-chip data access latency and thus push model towards computattion-bound.</i>
                                       </div> -->
                                       <div class="link">
                                          <b><a onclick="displayid('abs12');">[abstract]</a></b>
                                          <b><a href="https://arxiv.org/abs/2404.03216">[paper]</a></b>
                                          <b><a href="https://github.com/TorchFHE/SmartPAF">[code]</a></b>
                                          <b><a onclick="displayid('bib12');">[bibtex]</a></b>
                                          <div id="abs12" style="display:none;">
                                             As machine learning (ML) permeates fields like healthcare, facial recognition, and blockchain, the need to protect sensitive data intensifies. Fully Homomorphic Encryption (FHE) allows inference on encrypted data, preserving the privacy of both data and the ML model. However, it slows down non-secure inference by up to five magnitudes, with a root cause of replacing non-polynomial operators (ReLU and MaxPooling) with high-degree Polynomial Approximated Function (PAF). We propose SmartPAF, a framework to replace non-polynomial operators with low-degree PAF and then recover the accuracy of PAF-approximated model through four techniques: (1) Coefficient Tuning (CT) -- adjust PAF coefficients based on the input distributions before training, (2) Progressive Approximation (PA) -- progressively replace one non-polynomial operator at a time followed by a fine-tuning, (3) Alternate Training (AT) -- alternate the training between PAFs and other linear operators in the decoupled manner, and (4) Dynamic Scale (DS) / Static Scale (SS) -- dynamically scale PAF input value within (-1, 1) in training, and fix the scale as the running max value in FHE deployment. The synergistic effect of CT, PA, AT, and DS/SS enables SmartPAF to enhance the accuracy of the various models approximated by PAFs with various low degrees under multiple datasets. For ResNet-18 under ImageNet-1k, the Pareto-frontier spotted by SmartPAF in latency-accuracy tradeoff space achieves 1.42x ~ 13.64x accuracy improvement and 6.79x ~ 14.9x speedup than prior works. Further, SmartPAF enables a 14-degree PAF (f1^2 g_1^2) to achieve 7.81x speedup compared to the 27-degree PAF obtained by minimax approximation with the same 69.4% post-replacement accuracy. Our code is available at \url{https://github.com/EfficientFHE/SmartPAF}
                                          </div>
                                          <div id="bib12" style="display:none;">
                                             @inproceedings{tong2024accurate,
                                                author={Jianming Tong and Jingtian Dang and Anupam Golder and Callie Hao and Arijit Raychowdhury and Tushar Krishna},
                                                booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
                                                title={Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic Encryption}, 
                                                url = {https://arxiv.org/abs/2404.03216},
                                                year = {2024}
                                             }
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>

                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/sushi_micro.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%" valign="middle">
                                       <div class="title">
                                          <b>Hardware-Software co-design for real-time latency-accuracy navigation in tinyML applications</b>
                                       </div>
                                       <div class="authors">
                                          <a href="https://paymanbehnam.com/">Payman Behnam*</a>,
                                          <b>Jianming Tong*</b>,
                                          <a href="https://www.cc.gatech.edu/~akhare39/">Alind Khare</a>,
                                          <a href="https://www.linkedin.com/in/yangyu-chen/">Yangyu Chen</a>,
                                          <a href="https://www.linkedin.com/in/yue-pan-061439192/">Yue Pan</a>,
                                          <a href="https://www.linkedin.com/in/abhimanyu-bambhaniya/">Abhimanyu Bambhaniya</a>,
                                          <a>Pranav Gadikar</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>, and
                                          <a href="https://www.cc.gatech.edu/~atumanov/">Alexey Tumanov</a>
                                       </div>
                                       <div class="venue">
                                          <i> (<b style="color:#ff6600;">IEEE micro</b>), Sep 2023.</i>
                                       </div>
                                       <!-- <div class="insight">
                                          <i> <b> Insight 1: </b> Weights-sharing neural network enables a dynamic latency-accuracy serving.</i> <br>
                                          <i> <b> Insight 2: </b> Putting common weights on-chip (Subgraph Stationary) could save off-chip data access latency and thus push model towards computattion-bound.</i>
                                       </div> -->
                                       <div class="link">
                                          <b><a onclick="displayid('abs11');">[abstract]</a></b>
                                          <b><a href="https://www.computer.org/csdl/magazine/mi/5555/01/10257666/1QARI5P37ZC">[paper]</a></b>
                                          <b><a onclick="displayid('bib11');">[bibtex]</a></b>
                                          <div id="abs11" style="display:none;">
                                             tinyML applications increasingly operate in dynamically changing deployment scenarios, requiring optimizing for both accuracy and latency. Existing methods mainly target a single point in the accuracy/latency tradeoff space---insufficient as no single static point can be optimal under variable conditions. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that activates different SubNets within a SuperNet. This creates an opportunity to exploit the inherent temporal locality of different queries that use the same SuperNet. We propose a hardware-software co-design called SUSHI that introduces a novel SubGraph Stationary optimization. SUSHI consists of a novel FPGA implementation and a software scheduler that controls which SubNets to serve and what SubGraph to cache in real-time. SUSHI yields up to 32% improvement in latency, 0.98% increase in served accuracy, and achieves up to 78.7% saved off-chip energy across several neural network architectures.
                                          </div>
                                          <div id="bib11" style="display:none;">
                                             @ARTICLE {10257666,
                                                author = {P. Behnam and J. Tong and A. Khare and Y. Chen and Y. Pan and P. Gadikar and A. Bambhaniya and T. Krishna and A. Tumanov},
                                                journal = {IEEE Micro},
                                                title = {Hardware-Software co-design for real-time latency-accuracy navigation in tinyML applications},
                                                year = {5555},
                                                volume = {},
                                                number = {01},
                                                issn = {1937-4143},
                                                pages = {1-7},
                                                abstract = {tinyML applications increasingly operate in dynamically changing deployment scenarios, requiring optimizing for both accuracy and latency. Existing methods mainly target a single point in the accuracy/latency tradeoff space—insufficient as no single static point can be optimal under variable conditions. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that activates different SubNets within a SuperNet. This creates an opportunity to exploit the inherent temporal locality of different queries that use the same SuperNet. We propose a hardware-software co-design called SUSHI that introduces a novel SubGraph Stationary optimization. SUSHI consists of a novel FPGA implementation and a software scheduler that controls which SubNets to serve and what SubGraph to cache in real-time. SUSHI yields up to 32% improvement in latency, 0.98% increase in served accuracy, and achieves up to 78.7% saved off-chip energy across several neural network architectures.},
                                                keywords = {kernel;training;real-time systems;optimization;neural networks;system-on-chip;software},
                                                doi = {10.1109/MM.2023.3317243},
                                                publisher = {IEEE Computer Society},
                                                address = {Los Alamitos, CA, USA},
                                                month = {sep}
                                                }
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>


                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/sushi.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                       <div class="title">
                                          <b>SUSHI: SUbgraph Stationary Hardware-software Inference Co-design</b>
                                       </div>
                                       <div class="authors">
                                          <a href="https://paymanbehnam.com/">Payman Behnam*</a>,
                                          <b>Jianming Tong*</b>,
                                          <a href="https://www.cc.gatech.edu/~akhare39/">Alind Khare</a>,
                                          <a href="https://www.linkedin.com/in/yangyu-chen/">Yangyu Chen</a>,
                                          <a href="https://www.linkedin.com/in/yue-pan-061439192/">Yue Pan</a>,
                                          <a href="https://www.linkedin.com/in/abhimanyu-bambhaniya/">Abhimanyu Bambhaniya</a>,
                                          <a>Pranav Gadikar</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>, and
                                          <a href="https://www.cc.gatech.edu/~atumanov/">Alexey Tumanov</a>
                                       </div>
                                       <div class="venue">
                                          <i>In Proc of Sixth Conference on Machine Learning and Systems (<b style="color:#ff6600;">MLSys</b>), Jun 2023.</i>
                                       </div>
                                       <div> 
                                          <i><b style="color:#ff6600;">++Qualcomm Innovation Fellowship</b></i>
                                       </div>
                                       <div> 
                                          <i><b style="color:#ff6600;">++Best Poster Award (IAP2023@MIT)</b></i>
                                       </div>
                                       <!-- <div class="insight">
                                          <i> <b> Insight 1: </b> Weights-sharing neural network enables a dynamic latency-accuracy serving.</i> <br>
                                          <i> <b> Insight 2: </b> Putting common weights on-chip (Subgraph Stationary) could save off-chip data access latency and thus push model towards computattion-bound.</i>
                                       </div> -->
                                       <div class="link">
                                          <b><a onclick="displayid('abs7');">[abstract]</a></b>
                                          <b><a href="https://arxiv.org/abs/2306.17266">[paper]</a></b>
                                          <b><a onclick="displayid('bib7');">[bibtex]</a></b>
                                          <div id="abs7" style="display:none;">
                                             A growing number of applications depend on Machine Learning (ML) functionality and benefits from both higher quality ML predictions and better timeliness (latency) at the same time. A growing body of research in computer architecture, ML, and systems software literature focuses on reaching better latency/accuracy tradeoffs for ML models. Efforts include compression, quantization, pruning, early-exit models, mixed DNN precision, as well as ML inference accelerator designs that minimize latency and energy, while preserving delivered accuracy. All of them, however, yield improvements for a single static point in the latency/accuracy tradeoff space. We make a case for applications that operate in dynamically changing deployment scenarios, where no single static point is optimal. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that uses (activates) different SubNets within this weight-shared construct. This creates an opportunity to exploit the inherent temporal locality with our proposed SubGraph Stationary (SGS) optimization. We take a hardware-software co-design approach with a real implementation of SGS in SushiAccel and the implementation of a software scheduler SushiSched controlling which SubNets to serve and what to cache in real-time. Combined, they are vertically integrated into SUSHI---an inference serving stack. For the stream of queries, SUSHI yields up to 25% improvement in latency, 0.98% increase in served accuracy. SUSHI can achieve up to 78.7% off-chip energy savings.
                                          </div>
                                          <div id="bib7" style="display:none;">
                                             @misc{behnam2023subgraph,
                                                title={Subgraph Stationary Hardware-Software Inference Co-Design}, 
                                                author={Payman Behnam and Jianming Tong and Alind Khare and Yangyu Chen and Yue Pan and Pranav Gadikar and Abhimanyu Rajeshkumar Bambhaniya and Tushar Krishna and Alexey Tumanov},
                                                year={2023},
                                                eprint={2306.17266},
                                                archivePrefix={arXiv},
                                                primaryClass={cs.DC}
                                             }
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>


<!-- 
                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/acslam.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                       <div class="title">
                                          <b>ac2SLAM: FPGA Accelerated High-Accuracy SLAM with Heapsort and Parallel Keypoint Extractor</b>
                                       </div>
                                       <div class="authors">
                                          <a>Cheng Wang</a>,
                                          <a>Yinkun Liu</a>,
                                          <a>Kedai Zuo</a>,
                                          <b>Jianming Tong</b>,
                                          <a >Yan Ding</a>,
                                          and <a href="http://gr.xjtu.edu.cn/web/pengjuren">Pengju Ren</a>.<br>
                                       </div>
                                       <div class="venue">
                                          <i>International Conference on Field-Programmable Technology (<b style="color:#ff6600;">FPT</b>), 2021.</i> <b>Full Paper</b>
                                       </div>
                                       <div class="links">
                                          <b><a onclick="displayid('abs4');">[abstract]</a></b>
                                          <b><a href="https://ieeexplore.ieee.org/abstract/document/9609808">[paper]</a></b>
                                          <b><a href="https://github.com/SLAM-Hardware/acSLAM">[code]</a></b>
                                          <b><a onclick="displayid('bib4');">[bibtex]</a></b>
                                          <div id="abs4" style="display:none;">
                                             In order to fulfill the rich functions of the application layer, robust and accurate Simultaneous Localization and Mapping (SLAM) technique is very critical for robotics. However, due to the lack of sufficient computing power and storage capacity, it is challenging to delpoy high-accuracy SLAM in embedded devices efficiently. In this work, we propose a complete acceleration scheme, termed ac 2 SLAM, based on the ORB-SLAM2 algorithm including both front and back ends, and implement it on an FPGA platform. Specifically, the proposed ac 2 SLAM features with: 1) a scalable and parallel ORB extractor to extract sufficient keypoints and scores for throughput matching with 4% error, 2) a PingPong heapsort component (pp-heapsort) to select the significant keypoints, that could achieve single-cycle initiation interval to reduce the amount of data transfer between accelerator and the host CPU, and 3) the potential parallel acceleration strategies for the back-end optimization. Compared with running ORB-SLAM2 on the ARM processor, ac 2 SLAM achieves 2.1 × and 2.7 × faster in the TUM and KITTI datasets, while maintaining 10% error of SOTA eSLAM. In addition, the FPGA accelerated front-end achieves 4.55 × and 40 × faster than eSLAM and ARM. The ac 2 SLAM is fully open-sourced at https://github.com/SLAM-Hardware/acSLAM.
                                          </div>
                                          <div id="bib4" style="display:none;">
                                             @INPROCEEDINGS{9609808,
                                                author={Wang, Cheng and Liu, Yingkun and Zuo, Kedai and Tong, Jianming and Ding, Yan and Ren, Pengju},
                                                booktitle={2021 International Conference on Field-Programmable Technology (ICFPT)}, 
                                                title={ac2SLAM: FPGA Accelerated High-Accuracy SLAM with Heapsort and Parallel Keypoint Extractor}, 
                                                year={2021},
                                                volume={},
                                                number={},
                                                pages={1-9},
                                                doi={10.1109/ICFPT52863.2021.9609808}}
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr> -->


                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/icra.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                       <div class="title">
                                          <b>SMMR-explore: Submap-based multi-robot exploration system with multi-robot multi-target potential field exploration method</b>
                                       </div>
                                       <div class="authors">
                                          <a href="https://nicsefc.ee.tsinghua.edu.cn/people/yujinchengyu/">Jincheng Yu*</a>,
                                          <b> Jianming Tong*</b>,
                                          <a href="https://scholar.google.com/citations?user=gOuB4zQAAAAJ&hl=en">Yuanfan Xu</a>,
                                          <a href="https://nicsefc.ee.tsinghua.edu.cn/people/zhilin-xu/">Zhilin Xu</a>,
                                          <a href="http://nicsefc.ee.tsinghua.edu.cn/people/donghl17/">Haolin Dong</a>,
                                          <a href="http://nicsefc.ee.tsinghua.edu.cn/people/tianxiang-yang/">Tianxiang Yang</a>, and
                                          <a href="http://nicsefc.ee.tsinghua.edu.cn/people/yu-wang/">Yu Wang</a>.<br>
                                       </div>
                                       <div class="venue">
                                          <i>IEEE International Conference on Robotics and Automation (<b style="color:#ff6600;">ICRA</b>), 2021.</i> <b>Oral</b>
                                       </div>
                                       <div class="links">
                                          <b><a onclick="displayid('abs3');">[abstract]</a></b>
                                          <b><a href="https://ieeexplore.ieee.org/document/9561328">[paper]</a></b>
                                          <b><a href="https://github.com/efc-robot/SMMR-Explore">[code]</a></b>
                                          <!-- <b><a href="https://www.youtube.com/watch?v=H1zwRIz8OYs">[demo]</a></b> -->
                                          <b><a onclick="displayid('demo3');">[demo]</a></b>
                                          <b><a onclick="displayid('bib3');">[bibtex]</a></b>
                                          <div id="abs3" style="display:none;">
                                             Collaborative exploration in an unknown environment without external positioning under limited communication is an essential task for multi-robot applications. For inter-robot positioning, various Distributed Simultaneous Localization and Mapping (DSLAM) systems share the Place Recognition (PR) descriptors and sensor data to estimate the relative pose between robots and merge robots’ maps. As maps are constantly shared among robots in exploration, we design a map-based DSLAM framework, which only shares the submaps, eliminating the transfer of PR descriptors and sensor data. Our framework saves 30% of total communication traffic. For exploration, each robot is assigned to get much unknown information about environments with paying little travel cost. As the number of sampled points increases, the goal would change back and forth among sampled frontiers, leading to the downgrade in exploration efficiency and the overlap of trajectories. We propose an exploration strategy based on Multi-robot Multi-target Potential Field (MMPF), which can eliminate goal’s back-and-forth changes, boosting the exploration efficiency by 1.03 ×∼1.62 × with 3 % ∼ 40 % travel cost saved. Our SubMap-based Multi-robot Exploration method (SMMR-Explore) is evaluated on both Gazebo simulator and real robots. The simulator and the exploration framework are published as an open-source ROS project at https://github.com/efc-robot/SMMR-Explore.
                                          </div>
                                          <div id="bib3" style="display:none;">
                                             @INPROCEEDINGS{9561328,
                                                author={Yu, Jincheng and Tong, Jianming and Xu, Yuanfan and Xu, Zhilin and Dong, Haolin and Yang, Tianxiang and Wang, Yu},
                                                booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
                                                title={SMMR-Explore: SubMap-based Multi-Robot Exploration System with Multi-robot Multi-target Potential Field Exploration Method}, 
                                                year={2021},
                                                volume={},
                                                number={},
                                                pages={8779-8785},
                                                doi={10.1109/ICRA48506.2021.9561328}}
                                          </div>
                                          <div id="demo3" style="display:none;">
                                             <iframe width="560" height="315" src="https://www.youtube.com/embed/H1zwRIz8OYs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>



                    
                    <tr>
                      <td width="100%" valign="left">
                         <table>
                            <tbody>
                               <tr>
                                  <td width="30%">
                                     <img src="./images/ISPASS25_scalesim.png" width="95%" style="display: block;
                                     margin-left: auto; margin-left:
                                     auto;">
                                  </td>
                                  <td width="65%" valign="middle">
                                     <div class="title">
                                        <b>SCALE-Sim v3: A Modular Cycle-Accurate Systolic Accelerator Simulator for End-to-End System Analysis</b>
                                     </div>
                                     <div class="authors">
                                      <a href="https://scholar.google.com/citations?user=I8kUcx4AAAAJ&hl=en">Ritik Raj</a>,
                                      <a href="https://scholar.google.com/citations?user=bsF9XU4AAAAJ&hl=en">Sarbartha Banerjee*</a>,
                                      <a href="http://zishenwan.github.io">Nikhil Srinivas*</a>,
                                      <a href="https://zishenwan.github.io/">Zishen Wan*</a>,
                                      <strong>Jianming Tong*</strong>,
                                      <a href="https://anands09.github.io">Ananda Samajdar</a>,
                                      <a href="https://scholar.google.com/citations?user=P__ztgcAAAAJ&hl=en">Tushar Krishna</a>
                                     </div>
                                     <div class="venue">
                                      <i> IEEE International Symposium on Performance Analysis of Systems and Software (<b style="color:#ff6600;">ISPASS</b>), Sep 2025.</i>
                                     </div>
                                     <div class="link">
                                        <b><a onclick="displayid('abs14');">[abstract]</a></b>
                                        <b><a href="https://github.com/scalesim-project/scale-sim-v2">[code]</a></b> 
                                        <!-- <b><a onclick="displayid('bib14');">[bibtex]</a></b> -->
                                        <div id="abs14" style="display:none;">
                                          We present SCALE-Sim v3, a modular cycle-accurate simulator for systolic-array-based architectures, featuring multi-core architecture with spatio-temporal partitioning, sparsity, DRAM ramulator, precise data layout modeling, and energy and power estimation via Accelergy.
                                        </div>
                                        <!-- <div id="bib14" style="display:none;">
                                           @ARTICLE{10672547,
                                              author={Mao, X. and Mukherjee, M. and Mizanur Rahman, N. and DeLude, C. and Driscoll, J. and Sharma, S. and Behnam, P. and Kamal, U. and Woo, J. and Kim, D. and Khan, S. and Tong, J. and Seo, J. and Sinha, P. and Swaminathan, M. and Krishna, T. and Pande, S. and Romberg, J. and Mukhopadhyay, S.},
                                              journal={IEEE Transactions on Radar Systems}, 
                                              title={Real-time Digital RF Emulation – II: A Near Memory Custom Accelerator}, 
                                              year={2024},
                                              volume={},
                                              number={},
                                              pages={1-1},
                                              keywords={Radio frequency;Computational modeling;Emulation;Computer architecture;Hardware;Real-time systems;Pulse modulation;hardware accelerators;near-memory;radio frequency emulator;real-time},
                                              doi={10.1109/TRS.2024.3457523}}
                                        </div> -->
                                     </div>
                                  </td>
                               </tr>
                            </tbody>
                         </table>
                      </td>
                   </tr>

                    <tr>
                      <td width="100%" valign="left">
                         <table>
                            <tbody>
                               <tr>
                                  <td width="30%">
                                     <img src="./images/ISPASS25_XRBench_CDA.png" width="95%" style="display: block;
                                     margin-left: auto; margin-left:
                                     auto;">
                                  </td>
                                  <td width="65%" valign="middle">
                                     <div class="title">
                                        <b>Constrained Dataflow Accelerator for Real-Time Multi-Task Multi-Model Machine Learning Workloads</b>
                                     </div>
                                     <div class="authors">
                                       <a>Jamin Seo</a>,
                                       <b>Jianming Tong</b>,
                                       <b>Hyoukjun Kown</b>,
                                       <a href="https://scholar.google.com/citations?user=P__ztgcAAAAJ&hl=en">Tushar Krishna</a>
                                        and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
                                     </div>
                                     <div class="venue">
                                      <i> IEEE International Symposium on Performance Analysis of Systems and Software (<b style="color:#ff6600;">ISPASS</b>), Sep 2025.</i>
                                     <!-- </div>
                                     <div class="link">
                                        <b><a onclick="displayid('abs13');">[abstract]</a></b>
                                        <b><a href="https://ieeexplore.ieee.org/document/10672547">[paper]</a></b> 
                                        <b><a onclick="displayid('bib13');">[bibtex]</a></b>
                                        <div id="abs13" style="display:none;">
                                           A near memory hardware accelerator, based on a novel direct path computational model, for real-time emulation of radio frequency systems is demonstrated. Our evaluation of hardware performance uses both application-specific integrated circuits (ASIC) and field programmable gate arrays (FPGA) methodologies: 1). The ASIC test-chip implementation, using TSMC 28nm CMOS, leverages distributed autonomous control to extract concurrency in compute as well as low latency. It achieves a 518 MHz per channel bandwidth in a prototype 4-node system. The maximum emulation range supported in this paradigm is 9.5 km with 0.24 μs of per-sample emulation latency. 2). The FPGA-based implementation, evaluated on a Xilinx ZCU104 board, demonstrates a 9-node test case (two Transmitters, one Receiver, and 6 passive reflectors) with an emulation range of 1.13 km to 27.3 km at 215 MHz bandwidth.
                                        </div>
                                        <div id="bib13" style="display:none;">
                                           @ARTICLE{10672547,
                                              author={Mao, X. and Mukherjee, M. and Mizanur Rahman, N. and DeLude, C. and Driscoll, J. and Sharma, S. and Behnam, P. and Kamal, U. and Woo, J. and Kim, D. and Khan, S. and Tong, J. and Seo, J. and Sinha, P. and Swaminathan, M. and Krishna, T. and Pande, S. and Romberg, J. and Mukhopadhyay, S.},
                                              journal={IEEE Transactions on Radar Systems}, 
                                              title={Real-time Digital RF Emulation – II: A Near Memory Custom Accelerator}, 
                                              year={2024},
                                              volume={},
                                              number={},
                                              pages={1-1},
                                              keywords={Radio frequency;Computational modeling;Emulation;Computer architecture;Hardware;Real-time systems;Pulse modulation;hardware accelerators;near-memory;radio frequency emulator;real-time},
                                              doi={10.1109/TRS.2024.3457523}}
                                        </div>
                                     </div> -->
                                  </td>
                               </tr>
                            </tbody>
                         </table>
                      </td>
                   </tr>


                  </tbody>
               </table>
               <br>

               <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                     <heading>Collaborative Publications (* Equal Contribution)</heading>
                     <br>
                     <b> As Collaborator or Mentor </b>
                     

                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/real_time_RF.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%" valign="middle">
                                       <div class="title">
                                          <b>Real-time Digital RF Emulation – II: A Near Memory Custom Accelerator</b>
                                       </div>
                                       <div class="authors">
                                          <a>Xiangyu Mao</a>,
                                          <a href="https://www.linkedin.com/in/jamin-seo-8aba1a13b/?originalSubdomain=kr">Mandovi Mukherjee</a>,
                                          <a href="https://scholar.google.com/citations?user=5gRIUykAAAAJ&hl=en">Nael Mizanur Rahman</a>,
                                          <a>Coleman B DeLude</a>,
                                          <a>Joseph W. Driscoll</a>,
                                          <a>Sudarshan Sharma</a>,
                                          <a>Payman Behnam</a>,
                                          <a>Uday Kamal</a>,
                                          <a>Jongseok Woo</a>,
                                          <a>Daehyun Kim</a>,
                                          <a>Sharjeel M. Khan</a>,
                                          <b>Jianming Tong</b>,
                                          <a>Jamin Seo</a>,
                                          <a>Prachi Sinha</a>,
                                          <a>Madhavan Swaminathan</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
                                          <a>Santosh Pande</a>,
                                          <a>Justin Romberg</a>,
                                          and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
                                       </div>
                                       <div class="venue">
                                          <i> IEEE Transactions on Radar Systems (<b style="color:#ff6600;">TRadar</b>), Sep 2024.</i>
                                       </div>
                                       <div class="link">
                                          <b><a onclick="displayid('abs13');">[abstract]</a></b>
                                          <b><a href="https://ieeexplore.ieee.org/document/10672547">[paper]</a></b> 
                                          <b><a onclick="displayid('bib13');">[bibtex]</a></b>
                                          <div id="abs13" style="display:none;">
                                             A near memory hardware accelerator, based on a novel direct path computational model, for real-time emulation of radio frequency systems is demonstrated. Our evaluation of hardware performance uses both application-specific integrated circuits (ASIC) and field programmable gate arrays (FPGA) methodologies: 1). The ASIC test-chip implementation, using TSMC 28nm CMOS, leverages distributed autonomous control to extract concurrency in compute as well as low latency. It achieves a 518 MHz per channel bandwidth in a prototype 4-node system. The maximum emulation range supported in this paradigm is 9.5 km with 0.24 μs of per-sample emulation latency. 2). The FPGA-based implementation, evaluated on a Xilinx ZCU104 board, demonstrates a 9-node test case (two Transmitters, one Receiver, and 6 passive reflectors) with an emulation range of 1.13 km to 27.3 km at 215 MHz bandwidth.
                                          </div>
                                          <div id="bib13" style="display:none;">
                                             @ARTICLE{10672547,
                                                author={Mao, X. and Mukherjee, M. and Mizanur Rahman, N. and DeLude, C. and Driscoll, J. and Sharma, S. and Behnam, P. and Kamal, U. and Woo, J. and Kim, D. and Khan, S. and Tong, J. and Seo, J. and Sinha, P. and Swaminathan, M. and Krishna, T. and Pande, S. and Romberg, J. and Mukhopadhyay, S.},
                                                journal={IEEE Transactions on Radar Systems}, 
                                                title={Real-time Digital RF Emulation – II: A Near Memory Custom Accelerator}, 
                                                year={2024},
                                                volume={},
                                                number={},
                                                pages={1-1},
                                                keywords={Radio frequency;Computational modeling;Emulation;Computer architecture;Hardware;Real-time systems;Pulse modulation;hardware accelerators;near-memory;radio frequency emulator;real-time},
                                                doi={10.1109/TRS.2024.3457523}}
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>

                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/snatch.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%" valign="middle">
                                       <div class="title">
                                          <b>SNATCH: Stealing Neural Network Architecture from ML Accelerator in Intelligent Sensors</b>
                                       </div>
                                       <div class="authors">
                                          <a href="https://www.sudarshan-sh.com/"> Sudarshan Sharma</a>,
                                          <a href="https://udaykamal20.github.io/"> Uday Kamal</a>,
                                          <b>Jianming Tong</b>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
                                          and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
                                       </div>
                                       <div class="venue">
                                          <i>IEEE SENSORS conference(<b style="color:#ff6600;">SENSORS</b>), Aug 2023.</i>
                                       </div>
                                       <div class="link">
                                          <b><a onclick="displayid('abs10');">[abstract]</a></b>
                                          <!-- <b><a href="https://assets.researchsquare.com/files/rs-2910088/v1_covered_0e6c94bd-1499-4b2c-b414-902c232b490c.pdf?c=1683777877">[paper]</a></b> -->
                                          <!-- <b><a href="https://github.com/TorchFHE/PAF-FHE">[code]</a></b> -->
                                          <b><a href="https://epapers2.org/sensors2023/ESR/paper_details.php?paper_id=1451">[poster]</a></b>
                                          <!-- <b><a onclick="displayid('bib8');">[bibtex]</a></b> -->
                                          <div id="abs10" style="display:none;">
                                             The use of Machine Learning (ML) models executing on ML Accelerators (MLA) in Intelligent sensors for feature extraction has garnered substantial interest. The Neural Network (NN) architecture implemented of MLA are intellectual property for the vendors. Along with improved power-efficiency and reduced bandwidth, the hardware based ML models embedded in the sensor also provides additional security against cyber-attacks on the ML. In this paper, we introduce an attack referred as SNATCH which uses a profiling-based side channel attack (SCA) that aims to steal the NN architecture executing on a digital MLA (Deep Learning Processing Unit (DPU) IP by Xilinx). We use electromagnetic side channel leakage from a clone device to create a profiler and then attack the victim's device to steal the NN architecture. Stealing the ML model undermines the intellectual property rights of the vendors of a sensor. Further, it also allows an adversary to mount critical Denial of Service and misuse attack. 
                                          </div>
                                          <!-- <div id="bib8" style="display:none;">
                                             @misc {PPR:PPR658940,
                                                Title = {PAF-FHE: Low-Cost Accurate Non-Polynomial Operator Polynomial Approximation in Fully Homomorphic Encryption Based ML Inference},
                                                Author = {Dang, Jingtian and Tong, Jianming and Golder, Anupam and Raychowdhury, Arijit and Hao, Cong and Krishna, Tushar},
                                                DOI = {10.21203/rs.3.rs-2910088/v1},
                                                Abstract = {Machine learning (ML) is getting more pervasive. Wide adoption of ML in healthcare, facial recognition, and blockchain involves private and sensitive data. One of the most promising candidates for inference on encrypted data, termed Fully Homomorphic Encryp-tion (FHE), preserves the privacy of both data and the ML model. However, it slows down plaintext inference by six magnitudes, with a root cause of replacing non-polynomial operators with latency-prohibitive 27-degree Polynomial Approximated Function (PAF). While prior research has investigated low-degree PAFs, naive stochastic gradient descent (SGD) training fails to converge on PAFs with degrees higher than 5, leading to limited accuracy compared to the state-of-the-art 27-degree PAF. Therefore, we propose four training techniques to enable convergence in the post-approximation model using PAFs with an arbitrary degree, including (1) Dynamic Scaling (DS) and Static Scaling (SS) to enable minimal approximation error during approximation, (2) Coefficient Tuning (CT) to obtain a good initial coefficient value for each PAF, (3) Progressive Approximation (PA) to simply the two-variable regression optimization problem into single-variable for fast 1 and easy convergence, and (4) Alternate Training (AT) to retraining the post-replacement PAFs and other linear layers in a decoupled divide-and-conquer manner. A combination of DS/SS, CT, PA, and AT enables the exploration of accuracy-latency space for FHE-domain ReLU replacement. Leveraging the proposed techniques, we propose a systematic approach (PAF-FHE) to enable low-degree PAF to demonstrate the same accuracy as SotA high-degree PAFs. We evaluated PAFs with various degrees on different models and variant datasets, and PAF-FHE consistently enables low-degree PAF to achieve higher accuracy than SotA PAFs. Specifically, for ResNet-18 under the ImageNet-1k dataset, our spotted optimal 12-degree PAF reduces 56% latency compared to the SotA 27-degree PAF with the same post-replacement accuracy (69.4%). While as for VGG-19 under the CiFar-10 dataset, optimal 12-degree PAF achieves even 0.84% higher accuracy with 72% latency saving. Our code is open-sourced at: https://github.com/TorchFHE/PAF-FHE},
                                                Publisher = {Research Square},
                                                Year = {2023},
                                                URL = {https://doi.org/10.21203/rs.3.rs-2910088/v1},
                                             }
                                          </div> -->
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>


                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/tvlsi23.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                       <div class="title">
                                          <b>On Continuing DNN Accelerator Architecture Scaling Using Tightly-coupled Compute-on-Memory 3D ICs</b>
                                       </div>
                                       <div class="authors">
                                          <a href="https://www.linkedin.com/in/gauthaman-murali-4110a732/">Gauthaman Murali</a>,
                                          <a href="https://www.linkedin.com/in/adityaiyer99/">Aditya Iyer</a>,
                                          <a href="https://www.linkedin.com/in/lingjunzhu/?locale=en_US">Lingjun Zhu</a>,
                                          <b>Jianming Tong</b>,
                                          <a href="https://www.linkedin.com/in/fr-munoz-mrtinez/?originalSubdomain=es">Francisco Munoz Martinez</a>,
                                          <a href="https://www.linkedin.com/in/srivatsa-rangachar-srinivasa-8b0b4a130/">Srivatsa Rangachar Srinivasa</a>,
                                          <a href="https://ieeexplore.ieee.org/author/37268291400">Tanay Karnik</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                                          <a href="https://ece.gatech.edu/directory/sung-kyu-lim">Sung Kyu Lim</a>
                                       </div>
                                       <div class="venue">
                                          <i>IEEE Transactions on Very Large Scale Integration Systems (<b style="color:#ff6600;">TVLSI</b>), Jul 2023.</i>
                                       </div>
                                       <div class="link">
                                          <b><a onclick="displayid('abs9');">[abstract]</a></b>
                                          <b><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10221779">[paper]</a></b>
                                          <!-- <b><a href="https://github.com/TorchFHE/PAF-FHE">[code]</a></b> -->
                                          <!-- <b><a href="publications/PAF_FHE_poster.pdf">[poster]</a></b> -->
                                          <b><a onclick="displayid('bib9');">[bibtex]</a></b>
                                          <div id="abs9" style="display:none;">
                                             This work identifies the architectural and design scaling limits of 2D flexible interconnect DNN accelerators and addresses them with 3D ICs. We demonstrate how scaling up a baseline 2D accelerator in the X/Y dimension fails and how vertical stacking effectively overcomes the failure. We designed multi-tier accelerators that are 1.67X faster than the 2D design. Using our 3D architecture and circuit co-design methodology, we improve throughput, energy-efficiency, and area-efficiency by up to 5X, 1.2X, and 3.9X, respectively, over 2D counterparts. The IR-drop in our 3D designs is within 10.7% of VDD, and the temperature variation is within 12˚C.
                                          </div>
                                          <div id="bib9" style="display:none;">
                                             @ARTICLE{10221779,
                                                author={Murali, Gauthaman and Iyer, Aditya and Zhu, Lingjun and Tong, Jianming and Martínez, Francisco Muñoz and Srinivasa, Srivatsa Rangachar and Karnik, Tanay and Krishna, Tushar and Lim, Sung Kyu},
                                                journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems}, 
                                                title={On Continuing DNN Accelerator Architecture Scaling Using Tightly Coupled Compute-on-Memory 3-D ICs}, 
                                                year={2023},
                                                volume={},
                                                number={},
                                                pages={1-11},
                                                doi={10.1109/TVLSI.2023.3299564}}
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>



                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/mao.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                       <div class="title">
                                          <b>FPGA-Based High-Performance Real-Time Emulation of Radar System using Direct Path Compute Model</b>
                                       </div>
                                       <div class="authors">
                                          <a>Xiangyu Mao*</a>,
                                          <a href="https://www.linkedin.com/in/jamin-seo-8aba1a13b/?originalSubdomain=kr">Mandovi Mukherjee*</a>,
                                          <a href="https://scholar.google.com/citations?user=5gRIUykAAAAJ&hl=en">Nael Mizanur Rahman*</a>,
                                          <a>Uday Kamal</a>,
                                          <a>Sudarshan Sharma</a>,
                                          <a>Payman Behnam</a>,
                                          <b>Jianming Tong</b>,
                                          <a>Jongseok Woo</a>,
                                          <a>Coleman B DeLude</a>,
                                          <a>Joseph W. Driscoll</a>,
                                          <a>Jamin Seo</a>,
                                          <a>Santosh Pande</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
                                          <a>Justin Romberg</a>,
                                          <a>Madhavan Swaminathan</a>,
                                          and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
                                       </div>
                                       <div class="venue">
                                          <i>International Microwave Symposium (<b style="color:#ff6600;">IMS</b>), Jun 2023.</i>
                                       </div>
                                       <div class="link">
                                          <b><a onclick="displayid('abs7');">[abstract]</a></b>
                                          <b><a href="https://ieeexplore.ieee.org/document/10187950">[paper]</a></b>
                                          <b><a onclick="displayid('bib7');">[bibtex]</a></b>
                                          <div id="abs7" style="display:none;">
                                             This paper proposes a Field-Programmable Gate Array (FPGA) based platform for real-time emulation of Radio Frequency (RF) signal interactions among radars and reflectors. Unlike conventional tapped-delay Finite-Impulse-Response (FIR) models, the paper presents an FPGA realization of Direct Path Compute model for RF signal propagation. Experimental results on a Xilinx ZCU104 board demonstrate a 5-platform system of the emulation range of 2.78km to 27.3km at 180 MHz bandwidth.
                                          </div>
                                          <div id="bib7" style="display:none;">
                                             @INPROCEEDINGS{10187950,
                                                author={Mao, X. and Mukherjee, M. and Rahman, N. M. and Kamal, U. and Sharma, S. and Behnam, P. and Tong, J. and Driscoll, J. and Krishna, T. and Romberg, J. and Mukhopadhyay, S.},
                                                booktitle={2023 IEEE/MTT-S International Microwave Symposium - IMS 2023}, 
                                                title={FPGA-Based High-Performance Real-Time Emulation of Radar System Using Direct Path Compute Model}, 
                                                year={2023},
                                                volume={},
                                                number={},
                                                pages={419-422},
                                                keywords={Radio frequency;Microwave measurement;Computational modeling;Emulation;RF signals;Radar;Bandwidth;RF emulation;direct path compute model;FPGA;real-time},
                                                doi={10.1109/IMS37964.2023.10187950}}
                                              
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>

                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/mandovi.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                       <div class="title">
                                          <b>A High Performance Computing Architecture for Real-Time Digital Emulation of RF Interactions</b>
                                       </div>
                                       <div class="authors">
                                          <a href="https://www.linkedin.com/in/jamin-seo-8aba1a13b/?originalSubdomain=kr">Mandovi Mukherjee*</a>,
                                          <a href="https://scholar.google.com/citations?user=5gRIUykAAAAJ&hl=en">Nael Mizanur Rahman*</a>,
                                          <a href="https://scholar.google.com/citations?user=gQu_Pa8AAAAJ&hl=en">Coleman B. DeLude*</a>,
                                          <a href="https://www.linkedin.com/in/coleman-delude-a9962480/">Joseph W. Driscoll*</a>,
                                          <a>Uday Kamal</a>,
                                          <a>Jongseok Woo</a>,
                                          <a>Jamin Seo</a>,
                                          <a>Sudarshan Sharma</a>,
                                          <a>Xiangyu Mao</a>,
                                          <a>Payman Behnam,</a>,
                                          <a>Sharjeel M. Khan</a>,
                                          <a>Daehyun Kim</a>,
                                          <b>Jianming Tong</b>,
                                          <a>Prachi Sinha</a>,
                                          <a>Santosh Pande</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
                                          <a>Justin Romberg</a>,
                                          <a>Madhavan Swaminathan</a>,
                                          and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
                                       </div>
                                       <div class="venue">
                                          <i>In Proc of IEEE Radar Conference, (<b style="color:#ff6600;">RadarConf</b>), May 2023.</i>
                                       </div>
                                       <div class="link">
                                          <b><a onclick="displayid('abs6');">[abstract]</a></b>
                                          <b><a href="https://ieeexplore.ieee.org/abstract/document/10149577">[paper]</a></b>
                                          <b><a onclick="displayid('bib6');">[bibtex]</a></b>
                                          <div id="abs6" style="display:none;">
                                             A high performance architecture for emulating realtime radio frequency systems is presented. The architecture is developed based on a novel compute model and uses nearmemory techniques coupled with highly distributed autonomous control to simultaneously optimize throughput and minimize latency. A cycle level C++ based simulator is used to validate the proposed architecture with simulation of complex RF scenarios.
                                          </div>
                                          <div id="bib6" style="display:none;">
                                             @INPROCEEDINGS{10149577,
                                                author={Mukherjee, Mandovi and Rahman, Nael Mizanur and DeLude, Coleman and Driscoll, Joseph and Kamal, Uday and Woo, Jongseok and Seo, Jamin and Sharma, Sudarshan and Mao, Xiangyu and Behnam, Payman and Khan, Sharjeel and Kim, Daehyun and Tong, Jianming and Sinha, Prachi and Pande, Santosh and Krishna, Tushar and Romberg, Justin and Swaminathan, Madhavan and Mukhopadhyay, Saibal},
                                                booktitle={2023 IEEE Radar Conference (RadarConf23)}, 
                                                title={A High Performance Computing Architecture for Real-Time Digital Emulation of RF Interactions}, 
                                                year={2023},
                                                volume={},
                                                number={},
                                                pages={1-6},
                                                doi={10.1109/RadarConf2351548.2023.10149577}}
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>


                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/jamin.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                       <div class="title">
                                          <b>A Configurable Architecture for Efficient Sparse FIR Computation in Real-time Radio Frequency Systems</b>
                                       </div>
                                       <div class="authors">
                                          <a href="https://www.linkedin.com/in/jamin-seo-8aba1a13b/?originalSubdomain=kr">Jamin Seo</a>,
                                          <a href="https://scholar.google.com/citations?user=5gRIUykAAAAJ&hl=en">Nael Mizanur Rahman</a>,
                                          <a href="https://scholar.google.com/citations?user=gQu_Pa8AAAAJ&hl=en">Mandovi Mukherjee</a>,
                                          <a href="https://www.linkedin.com/in/coleman-delude-a9962480/">Coleman DeLude</a>,
                                          <b>Jianming Tong</b>,
                                          <a href="https://jrom.ece.gatech.edu/">Justin Romberg</a>,
                                          <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
                                          and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
                                       </div>
                                       <div class="venue">
                                          <i>International Microwave Symposium (<b style="color:#ff6600;">IMS</b>), 2021.</i>
                                       </div>
                                       <!-- <div class="insight">
                                          <i> <b> Insight 1: </b> Large-scale crossbar is implementation-prohibitive for long wires.</i> <br>
                                          <i> <b> Insight 2: </b> Using multi-stage interconnection to achieve function as crossbar could reduce half number of long wires and save power and area.</i>
                                       </div> -->
                                       <div class="link">
                                          <b><a onclick="displayid('abs5');">[abstract]</a></b>
                                          <b><a href="https://ieeexplore.ieee.org/abstract/document/9865486">[paper]</a></b>
                                          <b><a onclick="displayid('bib5');">[bibtex]</a></b>
                                          <div id="abs5" style="display:none;">
                                             A  low-latency and high-throughput, configurable architecture for computing sparse Finite Impulse Response in real-time Radio Frequency domain is proposed. The massively parallel architecture uses distributed control in association with near-memory techniques to optimize area and power. It supports configurability in filter tap locations and handling of locally dense taps, making it more adaptable to Radio Frequency environments.
                                          </div>
                                          <div id="bib5" style="display:none;">
                                             @INPROCEEDINGS{9865486,
                                                author={Seo, Jamin and Mukherjee, Mandovi and Rahman, Nael Mizanur and Tong, Jianming and DeLude, Coleman and Krishna, Tushar and Romberg, Justin and Mukhopadhyay, Saibal},
                                                booktitle={2022 IEEE/MTT-S International Microwave Symposium - IMS 2022}, 
                                                title={A Configurable Architecture for Efficient Sparse FIR Computation in Real-time Radio Frequency Systems}, 
                                                year={2022},
                                                volume={},
                                                number={},
                                                pages={998-1001},
                                                doi={10.1109/IMS37962.2022.9865486}}
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>

                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/acslam.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                       <div class="title">
                                          <b>ac2SLAM: FPGA Accelerated High-Accuracy SLAM with Heapsort and Parallel Keypoint Extractor</b>
                                       </div>
                                       <div class="authors">
                                          <a>Cheng Wang</a>,
                                          <a>Yinkun Liu</a>,
                                          <a>Kedai Zuo</a>,
                                          <b>Jianming Tong</b>,
                                          <a >Yan Ding</a>,
                                          and <a href="http://gr.xjtu.edu.cn/web/pengjuren">Pengju Ren</a>.<br>
                                       </div>
                                       <div class="venue">
            
                                          <i>International Conference on Field-Programmable Technology (<b style="color:#ff6600;">FPT</b>), 2021.</i> <b>Full Paper</b>
                                       </div>
                                       <!-- <div class="insight">
                                          <i> <b> Insight 1: </b> Back-end optimization is latency bottleneck in SLAM for large map, and it comes with great parallelism suitable for hardware acceleration.</i> <br>
                                          <i> <b> Insight 2: </b> Leveraging parallelism in front-end feature extractions could achieve 4.55X, 40X speedup than eSLAM and CPU.</i>
                                       </div> -->
                                       <div class="links">
                                          <b><a onclick="displayid('abs4');">[abstract]</a></b>
                                          <b><a href="https://ieeexplore.ieee.org/abstract/document/9609808">[paper]</a></b>
                                          <b><a href="https://github.com/SLAM-Hardware/acSLAM">[code]</a></b>
                                          <b><a onclick="displayid('bib4');">[bibtex]</a></b>
                                          <div id="abs4" style="display:none;">
                                             In order to fulfill the rich functions of the application layer, robust and accurate Simultaneous Localization and Mapping (SLAM) technique is very critical for robotics. However, due to the lack of sufficient computing power and storage capacity, it is challenging to delpoy high-accuracy SLAM in embedded devices efficiently. In this work, we propose a complete acceleration scheme, termed ac 2 SLAM, based on the ORB-SLAM2 algorithm including both front and back ends, and implement it on an FPGA platform. Specifically, the proposed ac 2 SLAM features with: 1) a scalable and parallel ORB extractor to extract sufficient keypoints and scores for throughput matching with 4% error, 2) a PingPong heapsort component (pp-heapsort) to select the significant keypoints, that could achieve single-cycle initiation interval to reduce the amount of data transfer between accelerator and the host CPU, and 3) the potential parallel acceleration strategies for the back-end optimization. Compared with running ORB-SLAM2 on the ARM processor, ac 2 SLAM achieves 2.1 × and 2.7 × faster in the TUM and KITTI datasets, while maintaining 10% error of SOTA eSLAM. In addition, the FPGA accelerated front-end achieves 4.55 × and 40 × faster than eSLAM and ARM. The ac 2 SLAM is fully open-sourced at https://github.com/SLAM-Hardware/acSLAM.
                                          </div>
                                          <div id="bib4" style="display:none;">
                                             @INPROCEEDINGS{9609808,
                                                author={Wang, Cheng and Liu, Yingkun and Zuo, Kedai and Tong, Jianming and Ding, Yan and Ren, Pengju},
                                                booktitle={2021 International Conference on Field-Programmable Technology (ICFPT)}, 
                                                title={ac2SLAM: FPGA Accelerated High-Accuracy SLAM with Heapsort and Parallel Keypoint Extractor}, 
                                                year={2021},
                                                volume={},
                                                number={},
                                                pages={1-9},
                                                doi={10.1109/ICFPT52863.2021.9609808}}
                                          </div>
                                       </div>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>


                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/PIT.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                    <div class="title">
                                       <b>PIT: Processing-In-Transmission with Fine-Grained Data Manipulation Networks</b>
                                    </div>
                                    <div class="authors">
                                       <a href="https://www.researchgate.net/profile/Pengchen-Zong"> Pengchen Zong*</a>,
                                       <a href="https://scholar.google.com/citations?hl=en&user=IHUKbtAAAAAJ"> Tian Xia*</a>,
                                       <a href="https://scholar.google.com/citations?hl=zh-CN&user=3onwdYsAAAAJ"> Haoran Zhao</a>,
                                       <b> Jianming Tong</b>,
                                       <a> Zehua Li</a>,
                                       <a href="https://www.linkedin.com/in/wenzhe-zhao-6b273779/?originalSubdomain=cn"> Wenzhe Zhao</a>,
                                       <a href="https://scholar.google.com.hk/citations?user=iqMe3p8AAAAJ&hl=zh-CN">Nanning Zheng</a>, and
                                       <a href="http://gr.xjtu.edu.cn/web/pengjuren">Pengju Ren</a>.<br>
                                    </div>
                                    <div class="venue">
                                       <i> IEEE Transactions on Computers (<b  style="color:#ff6600;">TOC</b>), 2021.</i>
                                    </div>
                                    <div class="links">
                                       <b><a onclick="displayid('abs2');">[abstract]</a></b>
                                       <b><a href="https://ieeexplore.ieee.org/abstract/document/9311185/">[paper]</a></b>
                                       <b><a onclick="displayid('bib2');">[bibtex]</a></b>
                                       <div id="abs2" style="display:none;">
                                          In the domain of data parallel computation, most works focus on data flow optimization inside the PE array and favorable memory hierarchy to pursue the maximum parallelism and efficiency, while the importance of data contents has been overlooked for a long time. As we observe, for structured data, insights on the contents (i.e., their values and locations within a structured form) can greatly benefit the computation performance, as fine-grained data manipulation can be performed. In this paper, we claim that by providing a flexible and adaptive data path, an efficient architecture with capability of fine-grained data manipulation can be built. Specifically, we design SOM, a portable and highly-adaptive data transmission network, with the capability of operand sorting, non-blocking self-route ordering and multicasting. Based on SOM, we propose the processing-in-transmission architecture (PITA), which extends the traditional SIMD architecture to perform some fundamental data processing during its transmission, by embedding multiple levels of SOM networks on the data path. We evaluate the performance of PITA in two irregular computation problems. We first map the matrix inversion task onto PITA and show considerable performance gain can be achieved, resulting in 3x-20x speedup against Intel MKL, and 20x-40x against cuBLAS. Then we evaluate our PITA on sparse CNNs. The results indicate that PITA can greatly improve computation efficiency and reduce memory bandwidth pressure. We achieved 2x-9x speedup against several state-of-art accelerators on sparse CNN, where nearly 100 percent PE efficiency is maintained under high sparsity. We believe the concept of PIT is a promising computing paradigm that can enlarge the capability of traditional parallel architecture.
                                       </div>
                                       <div id="bib2" style="display:none;">
                                          @ARTICLE{9311185,
                                             author={Zong, Pengchen and Xia, Tian and Zhao, Haoran and Tong, Jianming and Li, Zehua and Zhao, Wenzhe and Zheng, Nanning and Ren, Pengju},
                                             journal={IEEE Transactions on Computers}, 
                                             title={PIT: Processing-In-Transmission With Fine-Grained Data Manipulation Networks}, 
                                             year={2021},
                                             volume={70},
                                             number={6},
                                             pages={877-891},
                                             doi={10.1109/TC.2020.3048233}}
                                       </div>
                                    </div>
                                 </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>

                     <tr>
                        <td width="100%" valign="left">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="30%">
                                       <img src="./images/cocoa.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                                    </td>
                                    <td width="65%">
                                       <div class="title">
                                          <b>COCOA: Content-Oriented Configurable Architecture Based on Highly-Adaptive Data Transmission Networks</b>
                                       </div>
                                       <div class="authors">
                                          <a href="https://scholar.google.com/citations?hl=en&user=IHUKbtAAAAAJ"> Tian Xia</a>,
                                          <a href="https://www.researchgate.net/profile/Pengchen-Zong"> Pengchen Zong</a>,
                                          <a href="https://scholar.google.com/citations?hl=zh-CN&user=3onwdYsAAAAJ"> Haoran Zhao</a>,
                                          <b> Jianming Tong</b>,
                                          <a href="https://www.linkedin.com/in/wenzhe-zhao-6b273779/?originalSubdomain=cn"> Wenzhe Zhao</a>,
                                          <a href="https://scholar.google.com.hk/citations?user=iqMe3p8AAAAJ&hl=zh-CN">Nanning Zheng</a>, and
                                          <a href="http://gr.xjtu.edu.cn/web/pengjuren">Pengju Ren</a>.<br>
                                       </div>
                                       <div class="venue">
                                          <!-- <a href="http://amirgholami.org/">Amir Gholami</a>, -->
                                          <i> Proceedings of the 2020 on Great Lakes Symposium on VLSI (<b  style="color:#ff6600;">GLSVLSI</b>), 2020.</i>
                                       </div>
                                       <div class="insight">
                                          <!-- <a href="http://amirgholami.org/">Amir Gholami</a>, -->
                                          <i> <b> Insight: </b> Adding NoC between Mem-Cache-CPU for supporting Sorting, Ordering and Multicasting (SOM) could boost 25X CPU perfromance for matrix inversion.</i>
                                       </div>
                                       <div class="insight">
                                          <b><a onclick="displayid('abs1');">[abstract]</a></b>
                                          <b><a href="https://dl.acm.org/doi/abs/10.1145/3386263.3406924">[paper]</a></b>
                                          <b><a onclick="displayid('bib1');">[bibtex]</a></b>
                                          <div id="abs1" style="display:none;">
                                             In domain of parallel computation, most works focus on optimizing PE organization or memory hierarchy to pursue the maximum efficiency, while the importance of data contents has been overlooked for a long time. Actually for structured data, insights on data contents (i.e. values and locations within a structured form) can greatly benefit the computation performance, as fine-grained data manipulation can be performed. In this paper, we claim that by providing a flexible and adaptive data path, an efficient architecture with capability of fine-grained data manipulation can be built. Specifically, we propose COCOA, a novel content-oriented configurable architecture, which integrates multi-functional data reorganization networks in traditional computing scheme to handle the contents of data during the transmission path, so that they can be processed more efficiently. We evaluate COCOA on various problems: complex matrix algorithm (matrix inversion) and sparse DNN. The results indicates that COCOA is versatile enough to achieve high computation efficiency in both cases.
                                             </div>
                                          <div id="bib1" style="display:none;">
                                             @inproceedings{10.1145/3386263.3406924,
                                             author = {Xia, Tian and Zong, Pengchen and Zhao, Haoran and Tong, Jianming and Zhao, Wenzhe and Zheng, Nanning and Ren, Pengju}, 
                                             title = {COCOA: Content-Oriented Configurable Architecture Based on Highly-Adaptive Data Transmission Networks},
                                             year = {2020},
                                             isbn = {9781450379441},
                                             publisher = {Association for Computing Machinery},
                                             address = {New York, NY, USA},
                                             url = {https://doi.org/10.1145/3386263.3406924},
                                             doi = {10.1145/3386263.3406924},
                                             abstract = {In domain of parallel computation, most works focus on optimizing PE organization or memory hierarchy to pursue the maximum efficiency, while the importance of data contents has been overlooked for a long time. Actually for structured data, insights on data contents (i.e. values and locations within a structured form) can greatly benefit the computation performance, as fine-grained data manipulation can be performed. In this paper, we claim that by providing a flexible and adaptive data path, an efficient architecture with capability of fine-grained data manipulation can be built. Specifically, we propose COCOA, a novel content-oriented configurable architecture, which integrates multi-functional data reorganization networks in traditional computing scheme to handle the contents of data during the transmission path, so that they can be processed more efficiently. We evaluate COCOA on various problems: complex matrix algorithm (matrix inversion) and sparse DNN. The results indicates that COCOA is versatile enough to achieve high computation efficiency in both cases.},
                                             booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
                                             pages = {253–258},
                                             numpages = {6},
                                             keywords = {transmission network, computing architecture, high-performance computing, data reorgonization},
                                             location = {Virtual Event, China},
                                             series = {GLSVLSI '20}
                                             } 
                                          </div>
                                    </td>
                                 </tr>
                                 <!-- <tr>
                                    <td width="90%">
                                       <b>Xi'an Jiaotong University</b>, China<br>
                                       B.E. in Electrical Engineering and Automation (EE)
                                       • Sep. 2016 to Jun
                                       2020 <br>
                                       Advisor: Prof.<a href="http://gr.xjtu.edu.cn/web/pengjuren"> Pengju Ren </a> <br>
                                       <b>Overall GPA: 5.0/5.0</b> 
                                    </td>
                                    <td width="20%">
                                       <img src="./images/xjtu-logo.jpg" width="100%" style="display: block;
                                             margin-left: auto; margin-right:
                                             auto;">
                                    </td>
                                 </tr> -->
                              </tbody>
                           </table>
                        </td>
                     </tr>
                  </tbody>
               </table>
               <br>



               <table width="100%" align="middle" border="0" cellspacing="0" cellpadding="10">
                  <tbody>
                     <heading>Workshops</heading>
                     <!-- <tr>
                        <td width="60%" valign="middle">
                           <div class="title">
                              <a><b>FLOFA: Federated Once-for-All Networks</b></a>
                           </div>
                           <div class="authors">
                              <a href="https://www.cc.gatech.edu/~akhare39/">Alind Khare</a>,
                              <b>Jianming Tong</b>,
                              <a href="https://www.linkedin.com/in/animesh-agrawal-b68a1b170/">Animesh Agrawal</a>,
                              <a href="https://sahnimanas.github.io/">Manas Sahni</a>,
                              <a href="https://www.linkedin.com/in/shreyavarshini/">Shreya Varshini</a>,
                              <a href="https://www.cc.gatech.edu/~atumanov/">Alexey Tumanov</a>,
                              <a href="https://cs.illinois.edu/about/people/faculty/jimeng">Jimeng Sun</a>,
                              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
                              <a href="https://vsarkar.cc.gatech.edu/">Vivek Sarkar</a>, and
                              <a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a>.
                           </div>
                           <div class="venue">
                              <i><a href="https://sysml4health.github.io/">SysML4Health: Scalable Systems for ML-driven Analytics in Healthcare Workshop</a>, <b>MLSys</b></i>
                              2021
                           </div>
                        </td>
                     </tr> -->
                     <tr>
                        <td width="60%" valign="middle">
                           <div class="title">
                              <b>A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip Dataflow Switching</b>
                           </div>
                           <div class="authors">
                              <b>Jianming Tong</b>,
                              <a href="https://www.linkedin.com/in/anirudh-itagi/?originalSubdomain=in">Anirudh Itagi</a>,
                              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                           </div>
                           <div class="venue">
                              <i><a href="https://sites.google.com/g.harvard.edu/on-device-workshop-23">The 3rd On-Device Intelligence Workshop@</a><b style="color:#ff6600;">MLSys'23</b></i>
                           </div>
                           <div class="link">
                              <b><a onclick="displayid('abs_wp2');">[abstract]</a></b>
                              <b><a href="publications/LAMBDA_poster.pdf">[poster]</a></b>
                              <div id="abs_wp2" style="display:none;">
                                 The increasing prevalence of Machine Learning (ML) in various applications has led to the emergence of ML models with diverse structures, types and sizes. The ML model inference boils down to the execution of different dataflows (tiling, ordering, parallelism, and shapes), and using the optimal dataflow can reduce latency by up to two orders of magnitude over an inefficient one. Unfortunately, reconfiguring hardware for different dataflows involves on-chip data reordering and datapath reconfigurations, leading to non-trivial overheads that hinder ML accelerators from exploiting different dataflows, resulting in suboptimal performance. To address this challenge, we propose LAMBDA, an innovative accelerator that leverages a novel multi-stage reduction network called Additive Folded Fat Tree (AFFT) for reordering data in data reduction (RIR), enabling seamless switching between optimal dataflows with negligible latency and resources overhead. LAMBDA creates an opportunity to change the optimal dataflows at the granularity of layers without incurring additional latency overhead, and to explore the optimal dataflows on the real hardware with faster and more precise evaluation results. LAMBDA demonstrates a 0.5~2X speed up in end-to-end inference latency than the SotA Xilinx DPU on Xilinx ZCU 104 embedded FPGA board.
                              </div>
                           </div>
                           <!-- <div class="insight">
                              <i> <b> Insight 1: </b> Data layout in on-chip storage affects performance of dataflows significantly, and dataflows encounter up 30X longer latency under a mismatching on-chip data layout.</i> <br>
                              <i> <b> Insight 2: </b> To change data layout when switching dataflows, the additional layout reordering incur extra latency overhead. We propose Reordering in Reduction (RIR) to hide latency of layout reordering completely behind reduction, enable switching optimal dataflows with negligible switching overhead. </i>
                           </div> -->
                        </td>
                     </tr>
                     <tr>
                        <td width="60%" valign="middle">
                           <div class="title">
                              <b>ReLU-FHE: Low-cost Accurate ReLU Polynoimal Approximation in Fully Homomorphic Encryption Based ML Inference</b>
                           </div>
                           <div class="authors">
                              <a href="https://www.linkedin.com/in/jingtian-dang-568615207/">Jingtian Dang*</a>,
                              <b>Jianming Tong*</b>,
                              <a href="https://scholar.google.com/citations?user=QLN76UUAAAAJ&hl=en">Anupam Golder</a>,
                              <a href="https://sites.gatech.edu/ece-callie/">Callie Hao</a>,
                              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                           </div>
                           <div class="venue">
                              <i><a href="https://sites.google.com/g.harvard.edu/on-device-workshop-23">The 3rd On-Device Intelligence Workshop@</a><b style="color:#ff6600;">MLSys'23</b></i>
                           </div>
                           <div class="link">
                              <b><a onclick="displayid('abs_wp1');">[abstract]</a></b>
                              <b><a href="https://assets.researchsquare.com/files/rs-2910088/v1_covered_0e6c94bd-1499-4b2c-b414-902c232b490c.pdf?c=1683777877">[paper]</a></b>
                              <b><a href="https://github.com/TorchFHE/PAF-FHE.">[code]</a></b>
                              <b><a onclick="displayid('bib_wp1');">[bibtex]</a></b>
                              <div id="abs_wp1" style="display:none;">
                                 Machine learning (ML) is getting more pervasive. Wide adoption of ML in healthcare, facial recognition, and blockchain involves private and sensitive data. One of the most promising candidates for inference on encrypted data, termed Fully Homomorphic Encryption (FHE), preserves the privacy of both data and the ML model. However, it slows down plaintext inference by six magnitudes, with a root cause of replacing non-polynomial operators with latency-prohibitive 27-degree Polynomial Approximated Function (PAF). While prior research has investigated low-degree PAFs, naive stochastic gradient descent (SGD) training fails to converge on PAFs with degrees higher than 5, leading to limited accuracy compared to the state-of-the-art 27-degree PAF. Therefore, we propose four training techniques to enable convergence in the post-approximation model using PAFs with an arbitrary degree, including (1) Dynamic Scaling (DS) and Static Scaling (SS) to enable minimal approximation error during approximation, (2) Coefficient Tuning (CT) to obtain a good initial coefficient value for each PAF, (3) Progressive Approximation (PA) to simply the two-variable regression optimization problem into single-variable for fast  and easy convergence, and (4) Alternate Training (AT) to retraining the post-replacement PAFs and other linear layers in a decoupled divide-and-conquer manner. A combination of DS/SS, CT, PA, and AT enables the exploration of accuracy-latency space for FHEdomain ReLU replacement. Leveraging the proposed techniques, we propose a systematic approach (PAF-FHE) to enable low-degree PAF to demonstrate the same accuracy as SotA high-degree PAFs. We evaluated PAFs with various degrees on different models and variant datasets, and PAF-FHE consistently enables low-degree PAF to achieve higher accuracy than SotA PAFs. Specifically, for ResNet-18 under the ImageNet-1k dataset, our spotted optimal 12-degree PAF reduces 56% latency compared to the SotA 27-degree PAF with the same post-replacement accuracy (69.4%). While as for VGG-19 under the CiFar-10 dataset, optimal 12-degree PAF achieves even 0.84% higher accuracy with 72% latency saving. Our code is open-sourced at: https://github.com/TorchFHE/PAF-FHE.
                              </div>
                              <div id="bib_wp1" style="display:none;">
                                 @misc {PPR:PPR658940,
                                    Title = {PAF-FHE: Low-Cost Accurate Non-Polynomial Operator Polynomial Approximation in Fully Homomorphic Encryption Based ML Inference},
                                    Author = {Dang, Jingtian and Tong, Jianming and Golder, Anupam and Raychowdhury, Arijit and Hao, Cong and Krishna, Tushar},
                                    DOI = {10.21203/rs.3.rs-2910088/v1},
                                    Abstract = {Machine learning (ML) is getting more pervasive. Wide adoption of ML in healthcare, facial recognition, and blockchain involves private and sensitive data. One of the most promising candidates for inference on encrypted data, termed Fully Homomorphic Encryp-tion (FHE), preserves the privacy of both data and the ML model. However, it slows down plaintext inference by six magnitudes, with a root cause of replacing non-polynomial operators with latency-prohibitive 27-degree Polynomial Approximated Function (PAF). While prior research has investigated low-degree PAFs, naive stochastic gradient descent (SGD) training fails to converge on PAFs with degrees higher than 5, leading to limited accuracy compared to the state-of-the-art 27-degree PAF. Therefore, we propose four training techniques to enable convergence in the post-approximation model using PAFs with an arbitrary degree, including (1) Dynamic Scaling (DS) and Static Scaling (SS) to enable minimal approximation error during approximation, (2) Coefficient Tuning (CT) to obtain a good initial coefficient value for each PAF, (3) Progressive Approximation (PA) to simply the two-variable regression optimization problem into single-variable for fast 1 and easy convergence, and (4) Alternate Training (AT) to retraining the post-replacement PAFs and other linear layers in a decoupled divide-and-conquer manner. A combination of DS/SS, CT, PA, and AT enables the exploration of accuracy-latency space for FHE-domain ReLU replacement. Leveraging the proposed techniques, we propose a systematic approach (PAF-FHE) to enable low-degree PAF to demonstrate the same accuracy as SotA high-degree PAFs. We evaluated PAFs with various degrees on different models and variant datasets, and PAF-FHE consistently enables low-degree PAF to achieve higher accuracy than SotA PAFs. Specifically, for ResNet-18 under the ImageNet-1k dataset, our spotted optimal 12-degree PAF reduces 56% latency compared to the SotA 27-degree PAF with the same post-replacement accuracy (69.4%). While as for VGG-19 under the CiFar-10 dataset, optimal 12-degree PAF achieves even 0.84% higher accuracy with 72% latency saving. Our code is open-sourced at: https://github.com/TorchFHE/PAF-FHE},
                                    Publisher = {Research Square},
                                    Year = {2023},
                                    URL = {https://doi.org/10.21203/rs.3.rs-2910088/v1},
                                 }
                              </div>
                           </div>
                        </td>
                     </tr>
                     <tr>
                        <td width="60%" valign="middle">
                           <div class="title">
                              <b>FastSwtich: Enabling Real-time DNN Switching via Weight-Sharing</b>
                           </div>
                           <div class="authors">
                              <b>Jianming Tong</b>,
                              <a href="https://www.linkedin.com/in/yangyu-chen/">Yangyu Chen</a>,
                              <a href="https://www.linkedin.com/in/yue-pan-061439192/">Yue Pan</a>,
                              <a href="https://www.linkedin.com/in/abhimanyu-bambhaniya/">Abhimanyu Bambhaniya</a>,
                              <a href="https://www.cc.gatech.edu/~akhare39/">Alind Khare</a>,
                              <a href="https://sites.google.com/view/taekyungheo/">Taekyung Heo</a>,
                              <a href="https://www.cc.gatech.edu/~atumanov/">Alexey Tumanov</a>, and
                              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                           </div>
                           <div class="venue">
                              <i><a href="https://research.facebook.com/architecture-compiler-and-system-support-for-multimodel-dnn-workloads-workshop/">The 2nd Architecture, Compiler, and System Support for Multi-model DNN Workloads Workshop@</a><b style="color:#ff6600;">ISCA'22</b></i>
                           </div>
                           <div class="link">
                              <b><a onclick="displayid('abs_wp2');">[abstract]</a></b>
                              <b><a href="publications/SUSHI_MLSys2023.pdf">[paper]</a></b>
                              <b><a onclick="displayid('bib_wp2');">[bibtex]</a></b>
                              <div id="abs_wp2" style="display:none;">
                                 A growing number of applications depend on Machine Learning (ML) functionality and benefits from both higher quality ML predictions and better timeliness (latency) at the same time. A growing body of research in computer architecture, ML, and systems software literature focuses on reaching better latency/accuracy tradeoffs for ML models. Efforts include compression, quantization, pruning, early-exit models, mixed DNN precision, as well as ML inference accelerator designs that minimize latency and energy, while preserving delivered accuracy. All of them, however, yield improvements for a single static point in the latency/accuracy tradeoff space. We make a case for applications that operate in dynamically changing deployment scenarios, where no single static point is optimal. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that uses (activates) different SubNets within this weight-shared construct. This creates an opportunity to exploit the inherent temporal locality with our proposed SubGraph Stationary (SGS) optimization. We take a hardware-software co-design approach with a real implementation of SGS in SushiAccel and the implementation of a software scheduler SushiSched controlling which SubNets to serve and what to cache in real-time. Combined, they are vertically integrated into SUSHI---an inference serving stack. For the stream of queries, SUSHI yields up to 25% improvement in latency, 0.98% increase in served accuracy. SUSHI can achieve up to 78.7% off-chip energy savings.
                              </div>
                              <div id="bib_wp2" style="display:none;">
                                 Stay in tune
                              </div>
                           </div>
                        </td>
                     </tr>
                  </tbody>
               </table>
               <br>
               <br>
               <!-- Education -->
               <table width="100%" align="middle" border="0" cellspacing="0" cellpadding="10">
                  <tbody>
                     <heading>Education</heading>
                     <tr>
                        <td width="100%" valign="middle">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="90%">
                                       <b>Georgia Institute of Technology</b>, USA<br>
                                       Ph.D. in Computer Science
                                       • Jan. 2021 to Present <br>
                                       Advisor: Prof.<a href="https://tusharkrishna.ece.gatech.edu/"> Tushar Krishna </a> <br>
                                    </td>
                                    <td width="10%">
                                       <img src="./images/georgia_tech.png" width="105%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Massachusetts Institute of Technology</b>, USA<br>
                                       Visiting Student
                                       • Feb. 2024 to Feb. 2025 <br>
                                       Advisor: Prof.<a href="https://tusharkrishna.ece.gatech.edu/"> Tushar Krishna </a>, Host: Prof.<a href="https://csg.csail.mit.edu/Users/arvind/"> Arvind </a> <br>
                                    </td>
                                    <td width="10%">
                                       <img src="./images/MIT.png" width="105%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Georgia Institute of Technology</b>, USA<br>
                                       MS. in Computer Science
                                       • Jan. 2021 to May 2024 <br>
                                       Advisor: Prof.<a href="https://tusharkrishna.ece.gatech.edu/"> Tushar Krishna </a> <br>
                                    </td>
                                    <td width="10%">
                                       <img src="./images/georgia_tech.png" width="105%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Xi'an Jiaotong University</b>, China<br>
                                       B.E. in Electrical Engineering and Automation (EE)
                                       • Sep. 2016 to Jun
                                       2020 <br>
                                       Advisor: Prof.<a href="http://gr.xjtu.edu.cn/web/pengjuren"> Pengju Ren </a> <br>
                                       <!-- <b>Overall GPA: 5.0/5.0</b> -->
                                    </td>
                                    <td width="20%">
                                       <img src="./images/xjtu-logo.jpg" width="100%" style="display: block;
                                             margin-left: auto; margin-right:
                                             auto;">
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>
                  </tbody>
               </table>
               <br>
               <br>

               <!-- Experience -->
               <table width="100%" align="middle" border="0" cellspacing="0" cellpadding="10">
                  <tbody>
                     <heading>Experience</heading>
                     <tr>
                        <td width="100%" valign="middle">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="90%">
                                       <b>Massachusetts  Institute of Technology</b>, USA<br>
                                      Research Associative
                                       • Feb. 2024 to Present <br>
                                       Advisor: Prof.<a href="https://tusharkrishna.ece.gatech.edu/"> Tushar Krishna </a>, Host: Prof.<a href="https://people.csail.mit.edu/mengjia/"> Mengjia Yan </a> <br>
                                    </td>
                                    <td width="10%">
                                       <img src="./images/MIT.png" width="105%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Google</b>, USA<br>
                                       Student Researcher
                                       • Aug. 2024 to Apr. 2025 <br>
                                       Host: <a href="https://qconnewyork.com/speakers/asraali">Asra Ali</a> <a href="https://www.linkedin.com/in/jevin-jiang/">Jevin Jiang</a> <br>
                                    </td>
                                    <td width="10%">
                                       <img src="./images/google_logo.png" width="105%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Rivos Inc.</b>, Mountain View CA<br>
                                       Ph.D. Intern in Computer Architecture
                                       • May. 2023 to Aug 2023 <br>
                                       <!-- <b>Overall GPA: 5.0/5.0</b> -->
                                    </td>
                                    <td width="20%">
                                       <img src="./images/rivos_cit.jpg" width="100%" style="display: block;
                                             margin-left: auto; margin-right:
                                             auto;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Pacific Northwest National Lab (PNNL)</b>, Battelle WA<br>
                                       Research Intern in Computer Architecture
                                       • Jun. 2022 to Aug
                                       2022 <br>
                                       <!-- <b>Overall GPA: 5.0/5.0</b> -->
                                    </td>
                                    <td width="20%">
                                       <img src="./images/pnnl.png" width="100%" style="display: block;
                                             margin-left: auto; margin-right:
                                             auto;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Alibaba DAMO Academy</b>, Beijing<br>
                                       Research Intern in Fully Homormophic Encryption Accelerator • Jul. 2021 to
                                       Aug. 2021
                                    </td>
                                    <td width="40%">
                                       <img src="./images/damo.jpg" width="100%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Tsinghua University</b>, Beijing<br>
                                       (Vitising Student) Research Assitant in Robotics • Aug. 2020 to
                                       Jan. 2021 <br>
                                       Advisor: Prof.<a href="http://nicsefc.ee.tsinghua.edu.cn/people/yu-wang/"> Yu Wang
                                    </td>
                                    <td width="10%">
                                       <img src="./images/tsinghua.png" width="100%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>
                  </tbody>
               </table>
               <br>

               <!-- Book -->
               <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                  <tbody>
                     <tr>
                        <heading>Book</heading>
                        <td width="100%" valign="middle">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="80%">
                                       <b>On-chip Network (Chinese)</b><br>
                                       Translator<br>
                                       <b>Abstract</b><br>
                                       This book targets engineers and researchers familiar with basic computer architecture concepts who are interested in learning about on-chip networks. This work is designed to be a short synthesis of the most critical concepts in on-chip network design. It is a resource for both understanding on-chip network basics and for providing an overview of state of-the-art research in on-chip networks.
                                       <a href="http://www.zxhsd.com/kgsm/ts/2021/01/29/5329526.shtml"><b>[purchase translated version] </b></a>
                                       <a href="https://www.morganclaypool.com/doi/abs/10.2200/S00772ED1V01Y201704CAC040"><b>[English version -- Free for University] </b></a>
                                       <a href="http://www.morganclaypoolpublishers.com/catalog_Orig/product_info.php?products_id=1081"><b>[obtain original version] </b></a>
                                    </td>
                                    <td style="padding:2.5%;width:100%;max-width:100%">
                                       <a href="images/book.png"><img style="width:100%;max-width:100%" alt="On-chip Network Chinese Translation" src="images/book.png"
                                             class="hoverZoomLink"></a>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>
                  </tbody>
               </table>

               <!-- Award -->
               <table width="100%" align="middle" border="0" cellspacing="0" cellpadding="10">
                  <tbody>
                     <heading>Honors and Awards</heading>
                     <tr>
                        <td width="100%" valign="middle">
                           <table>
                              <tbody>
                                <tr>
                                   <td width="90%">
                                      <b><a href="https://www.dac.com/Attend/Students-Scholarships/Young-Student-Fellow-Program">2nd University DEMO</a></b>, USA • Jun. 2025<br>
                                   </td>
                                   <td width="10%">
                                      <img src="./images/dac.png" width="105%" style="display: flex;
                                            justify-content: center;">
                                   </td>
                                </tr>
                                 <tr>
                                    <td width="90%">
                                       <b><a href="https://mlcommons.org/2024/06/2024-mlc-rising-stars/">ML and System Rising Star</a></b>, USA • Jul. 2024<br>
                                    </td>
                                    <td width="10%">
                                       <img src="./images/mlcommon.png" width="105%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b><a href="https://www.industry-academia.org/mit-2023.html">Best Poster Award</a></b>, USA • Sep. 2023<br>
                                    </td>
                                    <td width="10%">
                                       <img src="./images/iap.png" width="105%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Winner in <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america">Qualcomm Innovation Fellowship</a></b>, USA • Jul. 2023<br>
                                    </td>
                                    <td width="10%">
                                       <img src="./images/qualcomm.png" width="105%" style="display: flex;
                                             justify-content: center;">
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>
                  </tbody>
               </table>
               <br>

               <!-- Award -->
               <table width="100%" align="middle" border="0" cellspacing="0" cellpadding="10">
                  <tbody>
                     <heading>Services</heading>
                     <tr>
                        <td width="100%" valign="middle">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="90%">
                                       <b>Artifact Evaluation Committee in <a href="https://www.asplos-conference.org/asplos2024/">ASPLOS'24</a></b>, USA • Jul. 2023<br>
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Artifact Evaluation Committee in <a href="https://www.asplos-conference.org/asplos2024/">ISCA'24</a></b>, USA • Feb. 2024<br>
                                    </td>
                                 </tr>
                                 <tr>
                                    <td width="90%">
                                       <b>Computer Architecture Student Association (CASA) <a href="https://www.sigarch.org/casa/"></a></b>, USA • Since Sep. 2023<br>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>
                  </tbody>
               </table>
               <br>

               <!-- Life -->
               <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                  <tbody>
                     <tr>
                        <heading>Life</heading>
                        <td width="100%" valign="middle">
                           <tr>
                              <td width="90%">
                                 <b>I love writing songs, playing piano, guitar, singing and fitting in.</b> I'm available on major music distributor like Apple Music, Spotify, QQ music and NetEase etc (Search my name in platforms to find me XD). Some thing about me could be also found here<br>
                                 <b><a href="https://music.apple.com/us/artist/%E4%BD%9F%E5%81%A5%E9%93%AD/1679883128">[Apple Music]</a></b>
                                 <b><a href="https://open.spotify.com/artist/4giALEGXP8Di4rHMZHsKmP?si=LYO9AGN8QYWPE8yiveWFMA">[Spotify]</a></b>
                                 <b><a href="https://y.qq.com/n/ryqq/singer/003VgCPP0VDIu0">[QQ Music]</a></b>
                                 <b><a href="https://music.163.com/#/artist?id=35727121">[Netease Music]</a></b>
                                 <b><a href="https://www.youtube.com/embed/GNeVtAYSOuI">[Youtube]</a></b> 
                                 <b><a href="https://www.youtube.com/channel/UCEr_Js-ulnM20ehogPZW0MQ">[Youtube - Magic Mushroom]</a></b>
                                 <b><a href="https://space.bilibili.com/2028910667">[Bilibili - Magic Mushroom]</a></b>
                              </td>
                           </tr>
                           <!-- <table>
                              <tbody>
                                 <tr>
                                    <td width="50%">
                                       <b>Magic Mushroom (魔力菇乐队)</b><br>
                                       Keyboarder<br>
                                       • Jan 2019 to Jun 2020 <br>
                                    </td>
                                    <td style="padding:2.5%;width:100%;max-width:100%">
                                       <a href="images/Band_Perform.jpg"><img style="width:100%;max-width:100%" alt="Band Performance" src="images/Band_Perform.jpg"
                                             class="hoverZoomLink"></a>
                                    </td>
                                 </tr>
                           </table> -->
                           <!-- <table>
                                 <tr>
                                    <td width="70%">
                                       <b>Side Business</b><br>
                                       <b>Music Composer, Arranger, mixer</b><br>
                                       • Drop me an email for interest to make music, willing to hear your story <br>
                                       <b>My Music <ins>Journey (历程)</ins> for commemorating my undergraduate journey</b>
                                       <b><a href="https://music.163.com/#/song?id=1460023067">[Music]</a></b>
                                       <b><a href="https://music.163.com/#/video?id=8D36EDF82E53F6605244A8118C60994E&userid=402073789&fbclid=IwAR1hqjadhSwoQwdymqwrm97StOKKDSrmg_8KNqfigr53W0mQDfEcq946teQ">[MV]</a></b>
                                    </td>
                                    <td style="padding:2.5%;width:100%;max-width:100%">
                                       <a href="images/Journey.jpg"><img style="width:100%;max-width:100%" alt="Band Performance" src="images/Journey.jpg"
                                             class="hoverZoomLink"></a>
                                    </td>
                                 </tr>
                              </tbody>
                           </table> -->
                           <!-- <table>
                              <tr>
                                 <td width="80%">
                                    <b>Magic Mushroom (魔力菇乐队)</b><br>
                                    <b>Singer, Keyboarder, Acoustic Guitar</b><br>
                                    • Jan 2022 to now<br>
                                    <b><a href="https://www.youtube.com/channel/UCEr_Js-ulnM20ehogPZW0MQ">[Youtube]</a></b>
                                    <b><a href="https://space.bilibili.com/2028910667">[Bilibili]</a></b>
                                 </td>
                                 <td style="padding:2.5%;width:100%;max-width:100%">
                                    <a href="images/magicmushroom.JPG"><img style="width:100%;max-width:100%" alt="Band Photo" src="images/magicmushroom.JPG"
                                          class="hoverZoomLink"></a>
                                 </td>
                              </tr>
                           </tbody> 
                           </table> -->
                        </td>
                     </tr>
                  </tbody>
               </table>
                <!-- Experience 
               <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                  <tbody>
                     <tr>
                        <heading>Experiences</heading>
                        <td width="100%" valign="middle">
                           <table>
                              <tbody>
                                 <tr>
                                    <td width="90%">
                                       <a href="https://aws.amazon.com/machine-learning/"><b>Amazon
                                             Web Services (AWS) AI Lab</b></a>,
                                       <b>Amazon</b><br>
                                       Software Developement Engineer Intern
                                       • Aug 2020 to Dec.
                                       2020 <br>
                                       Mentor: <a href="http://yidawang.org/">Yida
                                          Wang</a>
                                    </td>
                                 </tr>
                              </tbody>
                           </table>
                        </td>
                     </tr>
                  </tbody>
               </table>-->
               <!-- </table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>
                     <tr>
                        <td style="padding:0px">
                           <br>
                           <p style="text-align:right;font-size:small;">
                              Website source from <a href="https://github.com/yaohuicai/yaohuicai.github.io">Yaohui Cai</a>.
                           </p>
                        </td>
                     </tr>
                  </tbody>
               </table> -->
               <table width="100%" align="right" border="0" cellspacing="0" cellpadding="10">
                  <tbody>
                     <tr>
                        <!-- <heading>Life</heading> -->
                        <td width="100%" valign="right">
                           <b>Last Updated: Sep 10, 2025</b>
                        </td>
                     </tr>
                  </tbody>
               </table>
            </td>
         </tr>
   </table>


  
</body>
</html>
