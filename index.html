<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Jianming Tong</title>
  <meta name="author" content="Jianming Tong">
  <meta name="description" content="PhD student at
         Georgia Tech.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E9VGTE8CH0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-E9VGTE8CH0');
</script>

<script type="text/javascript">
  function displayid(id) {
    var erv = document.getElementById("" + id + "");
    if (erv.style.display == "none") {
      erv.style.display = "";
    }
    else {
      erv.style.display = "none";
    }
  }
</script>

<body>
  <div class="container">

    <!-- Profile Section -->
    <section class="profile-section">
      <div class="profile-info">
        <h1 class="profile-name">Jianming Tong</h1>
        <code class="profile-email">jianming [dot] tong [at] gatech [dot] edu</code>

        <p> Ph.D. at Georgia Tech starting from Spring 2021 </p>
        <p> Advisor: <a href="https://tusharkrishna.ece.gatech.edu/"> Tushar Krishna </a></p>
        <p> Main Developer for <a href="https://github.com/google/jaxite">CROSS</a>, <a
            href="https://github.com/maeri-project/FEATHER">FEATHER</a></p>
        <p> My research is funded by <a
            href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america">
            Qualcomm Innovation Fellowship </a> and <a href="https://www.src.org/program/jump2/"> SRC Jump 2.0 </a> </p>
        <p> FPGA and FHE Lead in Synergy Lab @ Gatech</p>

        <div class="profile-links">
          <a href="data/CV_Jianming_Tong.pdf">CV</a>
          <a href="https://scholar.google.com/citations?user=DaY9qQwAAAAJ&hl=zh-CN/"> Google Scholar </a>
          <a href="https://github.com/JianmingTong/"> GitHub </a>
          <a href="https://www.linkedin.com/in/jianming-tong-a08a59186/"> LinkedIn </a>
          <a href="https://orcid.org/my-orcid?orcid=0000-0001-8436-2946"> ORCID </a>
        </div>
      </div>

      <div class="profile-image-container">
        <a href="images/profile.jpg">
          <img class="profile-photo-large" alt="profile photo" src="images/profile.jpg">
        </a>
      </div>
    </section>
    <!-- Publications & Manuscripts -->
    <br>

    <!-- Research Interest Section -->
    <section class="section">
      <h2 class="section-title">Research Interest</h2>

      <p>I'm a <b>Computer Architect</b>, focusing on <b>system for AI and Cryptography</b> via full-stack
        optimizations.</p>
      <br>

      <ul class="interest-list">
        <li><b>Model (Software):</b> ML Model with Privacy-preserving Capability by Design, e.g. Crypto-friendly ML
          Model.
          <div class="profile-links" style="margin-top: 0.5rem;">
            <a href="https://github.com/TorchFHE/SmartPAF/blob/main/SmartPAF.pdf">SmartPAF-MLSys'24</a>
            <a
              href="https://drive.google.com/file/d/16k5ifu6K-r4wshE8Vmun1l9ab80RabEw/view?usp=drive_link">Privatar-UsenixSecurity'25</a>
          </div>
        </li>
        <li><b>System: </b> Latency/Accuracy/Privacy Navigation for Multi-Query Streams.
          <div class="profile-links" style="margin-top: 0.5rem;">
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=40">SUSHI-IEEE Micro'23</a>
            <a href="publications/SUSHI_MLSys2023.pdf">SUSHI-MLSys'23</a>
          </div>
        </li>
        <li><b>Compilation: </b> Convert Crypto Algorithm to be More Efficient on Existing Hardware.
          <div class="profile-links" style="margin-top: 0.5rem;">
            <a href="https://arxiv.org/abs/2501.07047">CROSS-HPCA'26</a>
          </div>
        </li>
        <li><b>Architecture (Hardware):</b> Reconfigurable Dataflow Accelerator for ML and Privacy-preserving ML.
          <div class="profile-links" style="margin-top: 0.5rem;">
            <a href="https://arxiv.org/abs/2405.13170">FEATHER-ISCA'24</a>
          </div>
        </li>
        <li><b>Performance Modeling:</b> Performance Analysis Tool for ML Accelerators.
          <div class="profile-links" style="margin-top: 0.5rem;">
            <a href="https://github.com/maeri-project/FEATHER/tree/main/LayoutLoop">LayoutLoop-ISCA'24</a>
            <a href="https://github.com/scalesim-project/scale-sim-v2">ScaleSim-ISPASS'25</a>
            <a href="https://github.com/maeri-project/squareloop">SquareLoop-HASP'25</a>
          </div>
        </li>
      </ul>

      <div class="mt-4">
        <img src="./images/research_overview_new.png" width="100%" loading="lazy">
      </div>
    </section>
    <!-- News Section -->
    <section class="section">
      <h2 class="section-title">News</h2>
      <div class="news-container">
        <ul class="news-list">
          <li> [Nov. 2025] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for
              Homomorphic Encryption</a> is being accepted to High-Performance Computer Architecture (<a
              href="https://conf.researchr.org/home/hpca-2026">HPCA'26</a>), and will be presented at Sydney!</a>
          </li>
          <li> [Nov. 2025] <b style="color:#9673A6;">[Teaching]</b> I gave a guest lecture of <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at the "ECE 6120: Machine
            Intelligence" course at <a href="https://www.gwu.edu/"></a>GWU</a> hosted by Prof. <a
              href="https://scholar.google.com/citations?user=nWNmtQ8AAAAJ&hl=zh-CN"><strong>Nan Wu</strong></a>.
          </li>
          <li> [Oct. 2025] <b style="color:#ff6600;">[talk]</b> I presented a poster on <a
              href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for
              Homomorphic Encryption</a> at <a href="https://acecenter.grainger.illinois.edu/"> ACE Annual Review
              @ Chicago</a></li>
          <li> [Oct. 2025] <b style="color:#ff6600;">[DEMO]</b> I give a <b>multi-university demo</b> on <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a
              href="https://acecenter.grainger.illinois.edu/"> ACE Annual Review @ Chicago</a> with Prof. <a
              href="https://www.csl.cornell.edu/~zhiruz/"> Zhiru Zhang </a>, Prof. <a href="https://charithmendis.com/">
              Charith Mendis </a>, and Prof. <a href="https://profiles.stanford.edu/subhasish-mitra"> Subhasish Mitra
            </a>, big thanks to the team
            <a href="https://devansh-dvj.github.io/"> Devansh Jain</a>, <a href="https://www.zzzdavid.tech/">Niansong
              Zhang</a>, <a href="https://chhzh123.github.io/">Hongzheng Chen</a> and <a
              href="https://scholar.google.com/citations?user=_sPARagAAAAJ&hl=en">Saranyu Chattopadhyay</a>!
          </li>
          <li> [Sep. 2025] <b style="color:#ff6600;">[Talk]</b> I gave a talk on <a
              href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for
              Homomorphic Encryption</a> at <a href="https://www.src.org/calendar/e007206/"> TechCon </a> See
            u'all at Austin!</li>
          <li> [Sep. 2025] <b style="color:#ff6600;">[Talk]</b> I give a talk on <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a
              href="https://www.escalab.org/wddsa2024/"> UT Austin </a> hosted by Prof. <a
              href="https://www.ece.utexas.edu/people/faculty/mattan-erez"> Mattan Erez</a>!</a></li>
          <li> [Sep. 2025] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://github.com/maeri-project/squareloop"><strong>SquareLoop</strong>: Explore Optimal
              Authentication Block Strategy for ML</a> is being accepted to Hardware and Architectural Support for
            Security and Privacy (<a href="https://haspworkshop.org/2025/index.html"> HASP'25 </a>), co-located
            with <a href="https://microarch.org/micro58/"> MICRO'25 </a> at Seoul!</a></li>
          <li> [Aug. 2025] <b style="color:#FF2400;">[Award]</b> Our work <a
              href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for
              Homomorphic Encryption</a> won the GT NEXT Award in recognition of our commitment to research and
            development that has the potential to significantly contribute to societal betterment! Go Yellow
            Jackets!</li>
          <li> [Jul. 2025] <b style="color:#82B366;">[Poster]</b> Our work <a
              href="https://drive.google.com/file/d/1OlTjT3jmIzLnLiKex30k_ySSOfIFwacL/view?usp=sharing"><strong>Privatar</strong>:
              Enabling Privacy-preserving Real-time Multi-user VR via Secure Outsourcing</a> is being accepted at
            <a href="https://www.usenix.org/conference/usenixsecurity25/technical-sessions"> Usenix Security'25
            </a> as <a href="https://drive.google.com/file/d/1OlTjT3jmIzLnLiKex30k_ySSOfIFwacL/view?usp=sharing">
              poster</a>! See u'all Aug 13~15 at Seattle!
          </li>
          <li> [Jul. 2025] <b style="color:#82B366;">[Poster]</b> Our work <a
              href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for
              Homomorphic Encryption and Zero Knowledge Proof</a> is being accepted at <a
              href="https://www.usenix.org/conference/usenixsecurity25/technical-sessions"> Usenix Security'25
            </a> as <a href="https://drive.google.com/file/d/1eOhzb3PBCxfMX0WGCzqqD8dwMqHoRZgV/view?usp=sharing">
              poster</a>! See u'all Aug 13~15 at Seattle!</li>
          <li> [Jul. 2025] <b style="color:#9673A6;">[Service]</b> I co-interview Prof. Mengjia Yan on behalf of
            TcuArch and IEEE Micro <a
              href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=DaY9qQwAAAAJ&citation_for_view=DaY9qQwAAAAJ:5nxA0vEk-isC">Sipping
              Matcha of Security: A Fireside Chat With Mengjia Yan</a> goes online! Check out the video recording
            here <a href="https://www.youtube.com/watch?v=oNj1fI_2tDQ"><strong>Video</strong></a>! </a></li>
          <li> [Jun. 2025] <b style="color:#FF2400;">[Award]</b> Our work <a
              href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for
              Homomorphic Encryption</a> won 2rd place at Unversity DEMO at <a href="https://www.dac.com/"> DAC'25
            </a>! U could run encrypted digit detction serving on Google Cloud with TPUv4 for free today! </li>
          <li> [Jun. 2025] <b style="color:#ff6600;">[DEMO]</b> I will give a demo on <a
              href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for
              Homomorphic Encryption</a> at <a href="https://www.dac.com/"> DAC'25 </a> See u all at SF!</li>
          <li> [May. 2025] <b style="color:#ff6600;">[Talk]</b> I gave a talk on <a
              href="https://arxiv.org/abs/2501.07047"><strong>CROSS</strong>: Enable AI Accelerator for
              Homomorphic Encryption and Zero Knowledge Proof</a> at <a href="https://cse.engin.umich.edu/"> UMich
            </a> hosted by Prof. <a href="https://web.eecs.umich.edu/~austin/"> Todd Austin</a>!</a></li>
          <li> [Mar. 2025] <b style="color:#82B366;">[Tool]</b> LayoutLoop from <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> [ISCA'24] has been integrated
            into <a href="https://github.com/NVlabs/timeloop"><strong>NVlabs/Timeloop</strong></a>, details could
            be found at this <a href="https://github.com/NVlabs/timeloop/pull/301"><strong>PR</strong></a> and
            this <a
              href="https://docs.google.com/presentation/d/1L3QLatmdSMooeVGd1ixgejq0fpmbsWQslSx8S5qZUKI/edit?usp=sharing"><strong>slide</strong></a>,
            enjoy precise layout modeling!</li>
          <li> [Mar. 2025] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://github.com/NVlabs/timeloop"><strong>Constrained Dataflow Accelerator for Real-Time
                Multi-Task Multi-Model Machine Learning Workloads</strong></a> has been accepted by ISPASS'25!
          </li>
          <li> [Mar. 2025] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://github.com/scalesim-project/scale-sim-v2"><strong>Scale-sim V3</strong></a> has been
            accepted by ISPASS'25!</li>
          <li> [Jan. 2025] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://arxiv.org/pdf/2501.07047"><strong>Leveraging ASIC AI Chips for Homomorphic
                Encryption</strong></a> has online released now!</li>
          <li> [Nov. 2024] <b style="color:#9673A6;">[Teaching]</b> I gave a guest lecture of <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at the "Advanced Computer
            Architecture for Machine Learning" course hosted by Prof. <a href="https://www.tonytgeng.com/"><strong>Tony
                Geng</strong></a>.</li>
          <li> [Nov. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on <a
              href="https://engineering.nyu.edu/events/2024/11/14/leveraging-tpu-homomorphic-encryption"><strong>Leveraging
                AI ASIC for Homomorphic Encryption</strong></a> at <a href="https://www.nyu.edu/"> NYU </a> hosted
            by <a href="https://brandonreagen.com/"><strong>Prof. Brandon Reagon</strong></a> and <a
              href="https://engineering.nyu.edu/student/karthik-garimella"><strong>Karthik Garimella</strong></a>!
          </li>
          <li> [Nov. 2024] <b style="color:#9673A6;">[Service]</b> I co-organized <strong>JOBS Workshop</strong>
            to help faciliating new grads for job hunting - go <a
              href="https://sites.google.com/cornell.edu/jobs-workshop-2024/home?authuser=0"><strong>JOBS</strong></a>!
            </a></li>
          <li> [Nov. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a
              href="https://www.escalab.org/wddsa2024/"> WDDSA </a> workshop co-located with <a
              href="https://microarch.org/micro57/"> MICRO'24 </a> at Austin!</a></li>
          <li> [Oct. 2024] <b style="color:#ff6600;">[Talk]</b> I demo <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a
              href="https://www.src.org/liaison/"> SRC </a> ACE annual review <a
              href="https://acecenter.grainger.illinois.edu/index.asp"> ACE </a> at Chicago!</a></li>
          <li> [Sep. 2024] <b style="color:#82B366;">[Paper]</b> Our work <papertitle> <a
                href="https://ieeexplore.ieee.org/document/10672547"> Real-time Digital RF Emulation – II: A Near
                Memory Custom Accelerator </papertitle> is accepted to the IEEE Transactions on Radar Systems (<a
              href="https://ieee-aess.org/publication/ieee-transactions-radar-systems"><strong>TRadar'24</strong></a>).
          </li>
          <li> [Aug. 2024] <b style="color:#FF2400;">[Award]</b> I was selected as the student for ACE Newsletter
            highlight by SRC!</li>
          <li> [Aug. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at <a
              href="https://www.src.org/liaison/"> SRC </a> Liaison Meeting of <a
              href="https://acecenter.grainger.illinois.edu/index.asp"> ACE </a> Center!</a></li>
          <li> [Aug. 2024] <b style="color:#9673A6;">[Career]</b> I join Google as a student researcher in Phazon
            team of PSS, more realistic privacy-preserving acceleration are coming, stay tuned!</li>
          <li> [Jul. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at NVidia (HQ) and NVidia
            (Westford)!</a></li>
          <li> [Jun. 2024] <b style="color:#ff6600;">[Talk]</b> We debut FEATHER <papertitle
              href="https://arxiv.org/abs/2405.13170">A Reconfigurable Accelerator with Data Reordering Support
              for Low-Cost On-Chip Dataflow Switching</papertitle> at ISCA, Buenos Aires!</a></li>
          <!-- <li> [May. 2024] <b style="color:#ff6600;">[Talk]</b> I give the talk on <papertitle>Leveraging AI ASIC for Fully Homomorphic Encryption</papertitle> at Google, IBM</a></li>  -->
          <li> [May. 2024] <b style="color:#ff6600;">[Talk]</b> I give a talk on <a
              href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a> at MIT</a></li>
          <li> [May. 2024] <b style="color:#FF2400;">[Award]</b> I am selected as "<a
              href="https://mlcommons.org/2024/06/2024-mlc-rising-stars/"><strong>ML and System Rising
                Star</strong></a>" by ML Commons, excited to meet you all at Nvidia HQ at Jul 15~16.</li>
          <li> [May. 2024] <b style="color:#FF2400;">[Award]</b> Our team "<a
              href="https://sites.gatech.edu/cipher-flit-fort/"><strong>CipherFlitFort</strong></a>" is awarded
            Startup Launch by <a href="https://create-x.gatech.edu/launch/startup-launch"><strong>CreateX</strong></a>
            at Georgia
            Tech, Go Jackets! </li>
          <li> [Apr. 2024] <b style="color:#FF2400;">[Award]</b> I am selected as <strong>DAC Young
              Fellow</strong> for DAC 2024. </li>
          <li> [Mar. 2024] <b style="color:#82B366;">[Paper]</b> Our work <papertitle> <a
                href="https://arxiv.org/abs/2405.13170"><strong>FEATHER</strong></a>: A Reconfigurable Accelerator
              with Data Reordering Support for Low-Cost On-Chip Dataflow Switching</papertitle> is accepted to the
            International Symposium on Computer Architecture (<a
              href="https://iscaconf.org/isca2024/"><strong>ISCA'24</strong></a>).</li>
          <li> [Feb. 2024] <b style="color:#82B366;">[Paper]</b> Our work <papertitle> <a
                href="https://arxiv.org/abs/2404.03216"><strong>SmartPAF</strong></a>: Accurate Low-Degree
              Polynomial Approximation of Non-polynomial Operators for Fast Private Inference in Homomorphic
              Encryption</papertitle> is accepted to the Seventh Conference on Machine Learning and Systems (<a
              href="https://mlsys.org/"><strong>MLSys'24</strong></a>).</li>
          <li> [Feb. 2024] <b style="color:#9673A6;">[Service]</b> We started course 6.192 <strong>Constructive
              Computer Architecture</strong> in three schools together this year (MIT, EPFL, GaTech) - <a
              href="https://mit.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%226c90dae2-5802-4e54-9e21-b10900d0907d%22"><strong>
                recordings </strong> </a> available online, Go Architects! </a></li>
          <li> [Jan. 2024] <b style="color:#9673A6;">[Service]</b> I served <strong>AEC</strong> for <a
              href="https://iscaconf.org/isca2024/"><strong>ISCA'24</strong></a>.</li>
          <li> [Nov. 2023] <b style="color:#9673A6;">[Service]</b> I join Computer Architecture Student
            Association (<a href="https://www.sigarch.org/casa/"><strong> CASA </strong> </a>) steering team, from
            the architects for the architects. </li>
          <li> [Oct. 2023] <b style="color:#ff6600;">[Talk]</b> I gave a talk on <papertitle
              href="https://arxiv.org/abs/2306.17266">SUSHI</papertitle> and <papertitle
              href="https://assets.researchsquare.com/files/rs-2910088/v1_covered_0e6c94bd-1499-4b2c-b414-902c232b490c.pdf?c=1683777877">
              PAF-FHE</papertitle> at <a href="https://hanlab.mit.edu/"><strong> HAN Lab @ MIT.</strong> </a></li>
          <li> [Sep. 2023] <b style="color:#FF2400;">[Award]</b> I won <strong>Best Poster Award</strong> for
            presenting our work <papertitle><a href="https://arxiv.org/abs/2306.17266">SUSHI</a></papertitle> at
            (<a href="https://www.industry-academia.org/mit-2023.html"><strong>IAP Workshop@MIT</strong></a>).
          </li>
          <li> [Sep. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>Hardware-Software co-design
              for real-time latency-accuracy navigation in tinyML applications</papertitle> is accepted to the
            Journal (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=40"><strong>IEEE
                micro</strong></a>).</li>
          <li> [Sep. 2023] <b style="color:#9673A6;">[Career]</b> I join MIT as a visiting researcher in CSAIL
            hosted by Dr. <a href="http://csg.csail.mit.edu/Users/arvind/"><strong>Arvind</strong></a>.</li>
          <li> [Aug. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>SNATCH: Stealing Neural
              Network Architecture from ML Accelerator in Intelligent Sensors</papertitle> is accepted to the IEEE
            SENSORS conference (<a href="https://2023.ieee-sensorsconference.org/"><strong>SENSORS'23</strong></a>).
          </li>
          <li> [Jul. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>On Continuing DNN
              Accelerator Architecture Scaling Using Tightly-coupled Compute-on-Memory 3D ICs</papertitle> is
            accepted to the IEEE Transactions on Very Large Scale Integration Systems (<a
              href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=92"><strong>TVLSI'23</strong></a>).
          </li>
          <li> [Jul. 2023] <b style="color:#FF2400;">[Award]</b> I win 2023 <a
              href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america"><strong>Qualcomm
                Innovation Fellowship</strong></a>, thank you Qualcomm!</li>
          <li> [Jul. 2023] <b style="color:#9673A6;">[Service]</b> I serve as <strong>AEC</strong> for <a
              href="https://www.asplos-conference.org/asplos2024/"><strong>ASPLOS'24</strong></a>.</li>
          <li> [Jun. 2023] <b style="color:#ff6600;">[Talk]</b> I gave a talk on <papertitle
              href="https://arxiv.org/abs/2306.17266">SUSHI</papertitle> and <papertitle
              href="https://assets.researchsquare.com/files/rs-2910088/v1_covered_0e6c94bd-1499-4b2c-b414-902c232b490c.pdf?c=1683777877">
              PAF-FHE</papertitle> at <a href="https://gr.xjtu.edu.cn/en/web/pengjuren/home"><strong> CAG Lab @
                XJTU University.</strong> </a></li>
          <li> [May. 2023] <b style="color:#82B366;">[Paper]</b> Our work <a href="publications/LAMBDA_ODIW2023.pdf" ,
              style="color: #000000;">
              <papertitle>A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip Dataflow
                Switching</papertitle>
            </a> accepted to the 3rd On-Device Intelligence Workshop (<a
              href="https://sites.google.com/g.harvard.edu/on-device-workshop-23">ODIW'23@<strong>MLSys'23</strong></a>).
          </li>
          <li> [May. 2023] <b style="color:#82B366;">[Paper]</b> Our work <a href="publications/ReLU_FHE_ODIW2023.pdf" ,
              style="color: #000000;">
              <papertitle>ReLU-FHE: Low-cost Accurate ReLU polynomial approximation in Fully Homomorphic
                Encryption Based ML Inference</papertitle>
            </a> accepted to the 3rd On-Device Intelligence Workshop (<a
              href="https://sites.google.com/g.harvard.edu/on-device-workshop-23">ODIW'23@<strong>MLSys23</strong></a>)
            .</li>
          <li> [Apr. 2023] <b style="color:#82B366;">[Paper]</b> Our work <a href="publications/SUSHI_MLSys2023.pdf" ,
              style="color: #000000;">
              <papertitle>SUSHI: SubGraph Stationary Hardware-Software Inference Co-design</papertitle>
            </a> accepted to the Sixth Conference on Machine Learning and Systems (<a
              href="https://mlsys.org/virtual/2023/calendar?showDetail=true&filter_events=&filter_rooms="><strong>MLSys'23</strong></a>).
          </li>
          <li> [Apr. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>FPGA-Based High-Performance
              Real-Time Emulation of Radar System using Direct Path Compute Model</papertitle> accepted to the
            International Microwave Symposium (<a href="https://ims-ieee.org/"><strong>IMS'23</strong></a>).</li>
          <!-- <li> [Jan. 2023] <b style="color:#6C8EBF;">[Service]</b> I serve on the Technical Program Committee of DAC'23, Reviewer of IEEE TCAD, Steering Committee of <a href="https://entrepreneurship.ieee.org/2023_china-region-team-welcome-message/">IEEE Entrepreneurship China</a>.</li>  -->
          <li> [Mar. 2023] <b style="color:#ff6600;">[Talk]</b> I give a talk on <papertitle>Enable Best ML
              Inference and Training: A systematic Approach</papertitle> at <a
              href="https://eiclab.scs.gatech.edu/"><strong>EIC Lab @ Georgia Tech.</strong></a></li>
          <li> [Mar. 2023] <b style="color:#82B366;">[Paper]</b> Our work <papertitle>A High Performance Computing
              Architecture for Real-Time Digital Emulation of RF Interactions</papertitle> accepted to the In Proc
            of IEEE Radar Conference (<a
              href="https://radar2023.ieee-radarconf.org/"><strong>RadarConf'23</strong></a>).</li>
          <li> [Nov. 2022] <b style="color:#ff6600;">[Talk]</b> I give a talk on <papertitle>Full-Stack ML
              Dataflow, Mapping and SW/HW Co-Design and Search</papertitle> at <a
              href="https://nicsefc.ee.tsinghua.edu.cn/"><strong> NICS-EFC Lab @ Tsinghua University.</strong></a>
          </li>
          <li> [Jul. 2022] <b style="color:#D79B00;">[Tutorial]</b> I give a tutorial on <a
              href="https://maeri-project.github.io/" , style="color: #000000;">
              <papertitle>MAERI 2.0: An End-to-end framework to explore architecture design space on FPGA
              </papertitle>
            </a> at <a href="https://maeri-project.github.io/"><strong>ICS 2022</strong></a>.</li>
          <li> [Jul. 2022] <b style="color:#ff6600;">[Talk]</b> I present our work <papertitle>FastSwtich:
              Enabling Real-time DNN Switching via Weight-Sharing</papertitle> at the 2nd Architecture, Compiler,
            and System Support for Multi-model DNN Workloads Workshop Workshop @ <a
              href="https://research.facebook.com/architecture-compiler-and-system-support-for-multimodel-dnn-workloads-workshop/">
              <strong>ISCA'23</strong></a> .</li>
          <li> [Apr. 2022] <b style="color:#FF2400;">[Award]</b> I receive Finalist in Qualcomm Innovation
            Fellowship, thank you Qualcomm!</li>
          <li> [Mar. 2022] <b style="color:#FF2400;">[Award]</b> I win 2nd place in SCS Poster Competition at
            Georgia Tech, thank you SCS!</li>
          <li> [Nov. 2021] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://ieeexplore.ieee.org/document/9561328" , style="color: #000000;">
              <papertitle>A Configurable Architecture for Efficient Sparse FIR Computation in Real-time Radio
                Frequency Systems</papertitle>
            </a> accepted to International Microwave Symposium (<a
              href="https://ims-ieee.org/"><strong>IMS'21</strong></a>).</li>
          <li> [Aug. 2021] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://ieeexplore.ieee.org/abstract/document/9609808" , style="color: #000000;">
              <papertitle>ac2SLAM: FPGA Accelerated High-Accuracy SLAM with Heapsort and Parallel Keypoint
                Extractor</papertitle>
            </a> accepted to <a href="http://www.icfpt.org/"><strong>FPT'21</strong></a>.<a
              href="https://github.com/SLAM-Hardware/acSLAM">code</a></li>
          <li> [Mar. 2021] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://ieeexplore.ieee.org/document/9561328" , style="color: #000000;">
              <papertitle>SMMR-explore: Submap-based multi-robot exploration system with multi-robot multi-target
                potential field exploration method</papertitle>
            </a> accepted to <strong>ICRA'21</strong>.<a href="https://github.com/efc-robot/SMMR-Explore">code</a><a
              href="https://www.youtube.com/watch?v=H1zwRIz8OYs">demo</a></li>
          <li> [Mar. 2021] <b style="color:#10739E;">[Book]</b> Our translated book <papertitle>On-chip Network
            </papertitle></a> publicly released <a
              href="http://www.zxhsd.com/kgsm/ts/2021/01/29/5329526.shtml"><b>[purchase translated version]
              </b></a><a href="https://www.morganclaypool.com/doi/abs/10.2200/S00772ED1V01Y201704CAC040"><b>[English
                version
                -- Free for University] </b></a></li>
          <li> [Feb. 2021] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://ieeexplore.ieee.org/abstract/document/9311185" , style="color: #000000;">
              <papertitle>PIT: Processing-In-Transmission with Fine-Grained Data Manipulation Networks
              </papertitle>
            </a> accepted to <a
              href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=12"><strong>ToC'21</strong></a>.</li>
          <li> [Jan. 2021] <b style="color:#9673A6;">[Career]</b> I kick-off my Ph.D. career at Georgia Tech, go
            Yellow Jackets!</li>
          <li> [Dec. 2020] <b style="color:#82B366;">[Paper]</b> Our work <a
              href="https://dl.acm.org/doi/abs/10.1145/3386263.3406924" , style="color: #000000;">
              <papertitle>COCOA: Content-Oriented Configurable Architecture Based on Highly-Adaptive Data
                Transmission Networks</papertitle>
            </a> accepted to <a
              href="https://www.glsvlsi.org/archive/glsvlsi21/index.html"><strong>GLSVLSI'21</strong></a>.</li>
        </ul>
      </div>
    </section>

    <!-- Publications Section -->
    <section class="section">
      <h2 class="section-title">Leading Publications (* Equal Contribution)</h2>
      <tr>
        <td width="100%" valign="left">
          <table>
            <tbody>
              <tr>
                <div class="publication-item">
                  <div class="pub-image">
                    <img src="./images/cross_overview.png" alt="CROSS Overview">
                  </div>
                  <div class="pub-content">
                    <div class="title">
                      <b>Leveraging ASIC AI Chips for Homomorphic Encryption</b>
                    </div>
                    <div class="authors">
                      <b>Jianming Tong</b>,
                      <a href="https://www.linkedin.com/in/tianhao-huang-tech/">Tianhao Huang</a>,
                      <a href="https://www.linkedin.com/in/leo-de-castro-962601190/">Leo De Castro</a>,
                      <a href="https://www.linkedin.com/in/anirudh-itagi">Anirudh Itagi</a>,
                      <a href="https://www.linkedin.com/in/jingtian-dang-568615207/">Jingtian Dang</a>,
                      <a href="https://scholar.google.com/citations?user=QLN76UUAAAAJ&hl=en">Anupam Golder</a>,
                      <a href="https://github.com/asraa">Asra Ali</a>,
                      <a href="https://www.linkedin.com/in/jevin-jiang/">Jevin Jiang</a>,
                      <a href="https://www.jeremykun.com/">Jeremy Kun</a>,
                      <a href="https://csg.csail.mit.edu/Users/arvind/">Arvind</a>,
                      <a href="https://tsg.ece.cornell.edu/people/g-edward-suh/">G. Edward Suh</a>,
                      <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                    </div>
                    <div class="venue">
                      <i> High-Performance Computer Architecture (<b style="color:#ff6600;">HPCA</b>), Jan 2026.</i>
                    </div>
                    <div class="profile-links">
                      <a onclick="displayid('abs14');">abstract</a>
                      <a href="https://arxiv.org/abs/2501.07047">paper</a>
                      <a
                        href="https://drive.google.com/file/d/1eOhzb3PBCxfMX0WGCzqqD8dwMqHoRZgV/view?usp=sharing">poster</a>
                      <a href="https://github.com/google/jaxite">code</a>
                      <a onclick="displayid('bib14');">bibtex</a>
                      <div id="abs14" style="display:none;">
                        Cloud-based services are making the outsourcing of sensitive client data increasingly common.
                        Although homomorphic encryption (HE) offers strong privacy guarantee, it requires substantially
                        more resources than computing on plaintext, often leading to unacceptably large latencies in
                        getting the results. HE accelerators have emerged to mitigate this latency issue, but with the
                        high cost of ASICs. In this paper we show that HE primitives can be converted to AI operators
                        and accelerated on existing ASIC AI accelerators, like TPUs, which are already widely deployed
                        in the cloud. Adapting such accelerators for HE requires (1) supporting modular multiplication,
                        (2) high-precision arithmetic in software, and (3) efficient mapping on matrix engines. We
                        introduce the CROSS compiler (1) to adopt Barrett reduction to provide modular reduction support
                        using multiplier and adder, (2) Basis Aligned Transformation (BAT) to convert high-precision
                        multiplication as low-precision matrix-vector multiplication, (3) Matrix Aligned Transformation
                        (MAT) to covert vectorized modular operation with reduction into matrix multiplication that can
                        be efficiently processed on 2D spatial matrix engine. Our evaluation of CROSS on a Google TPUv4
                        demonstrates significant performance improvements, with up to 161x and 5x speedup compared to
                        the previous work on many-core CPUs and V100. The kernel-level codes are open-sourced at
                        https://github.com/google/jaxite.git.
                      </div>
                      <div id="bib14" style="display:none;">
                        @misc{tong2025leveragingasicaichips,
                        title={Leveraging ASIC AI Chips for Homomorphic Encryption},
                        author={Jianming Tong and Tianhao Huang and Leo de Castro and Anirudh Itagi and Jingtian Dang
                        and Anupam Golder and Asra Ali and Jevin Jiang and Arvind and G. Edward Suh and Tushar Krishna},
                        year={2025},
                        eprint={2501.07047},
                        archivePrefix={arXiv},
                        primaryClass={cs.CR},
                        url={https://arxiv.org/abs/2501.07047},
                        }
                      </div>
                      <div>
                        <i><b style="color:#ff6600;">++CROSS is Deployed in Google TPU Cloud</b></i>
                      </div>
                      <div>
                        <i><b style="color:#ff6600;">++CROSS won 2nd place at DAC university demo</b></i>
                      </div>
                      <div>
                        <i><b style="color:#ff6600;">++CROSS won the GT NEXT Award</b></i>
                      </div>
                    </div>
                  </div>
                </div>

                <div class="publication-item">
                  <div class="pub-image">
                    <img src="./images/feather.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                  </div>
                  <div class="pub-content">
                    <div class="title">
                      <b>FEATHER: A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip
                        Dataflow Switching</b>
                    </div>
                    <div class="authors">
                      <b>Jianming Tong</b>,
                      <a href="https://www.linkedin.com/in/anirudh-itagi">Anirudh Itagi</a>,
                      <a href="https://www.linkedin.com/in/pchatarasi/">Prasanth Chatarasi</a>,
                      <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                    </div>
                    <div class="venue">
                      <i> International Symposium on Computer Architecture (<b style="color:#ff6600;">ISCA</b>),
                        Jun
                        2024.</i>
                    </div>
                    <div class="profile-links">
                      <a onclick="displayid('abs13');">abstract</a>
                      <a href="https://arxiv.org/abs/2405.13170">paper</a>
                      <a href="https://github.com/maeri-project/FEATHER">code</a>
                      <a
                        href="https://docs.google.com/presentation/d/1b_yGpuQd70Zo1Lj_AB6o9Y7BOjgpJ0oh8RGTD_7-yG4/edit?usp=sharing">slide</a>
                      <a href="https://youtu.be/FgTaYiEArrI">ISCA Talk</a>
                      <a href="https://youtu.be/FA6IMiRmQmw">Deep Dive Talk</a>
                      <a
                        href="https://docs.google.com/presentation/d/1L3QLatmdSMooeVGd1ixgejq0fpmbsWQslSx8S5qZUKI/edit?usp=sharing">LayoutLoop</a>
                      <a onclick="displayid('bib13');">bibtex</a>
                      <div>
                        <i><b style="color:#ff6600;">++LayoutLoop is Integrated into NVLabs/Timeloop</b></i>
                      </div>
                      <div id="abs13" style="display:none;">
                        The inference efficiency of diverse ML models over spatial accelerators boils down to the
                        execution of different dataflows (i.e. different tiling, ordering, parallelism, and
                        shapes).
                        Using the optimal dataflow for every layer of workload can reduce latency by up to two
                        orders
                        of
                        magnitude over a suboptimal dataflow. Unfortunately, reconfiguring hardware for different
                        dataflows involves on-chip data layout reordering and datapath reconfigurations, leading
                        to
                        non-trivial overhead that hinders ML accelerators from exploiting different dataflows,
                        resulting
                        in suboptimal performance. To address this challenge, we propose FEATHER, an innovative
                        accelerator that leverages a novel spatial array termed NEST and a novel multi-stage
                        reduction
                        network called BIRRD for performing flexible data reduction with layout reordering under
                        the
                        hood, enabling seamless switching between optimal dataflows with negligible latency and
                        resources overhead. For systematically evaluating the performance interaction between
                        dataflows
                        and layouts, we enhance Timeloop, a state-of-the-art dataflow cost modeling and search
                        framework, with layout assessment capabilities, and term it as Layoutloop. We model
                        FEATHER
                        into
                        Layoutloop and also deploy FEATHER end-to-end on the edge ZCU104 FPGA. FEATHER delivers
                        1.27~2.89x inference latency speedup and 1.3~6.43x energy efficiency improvement compared
                        to
                        various SoTAs like NVDLA, SIGMA and Eyeriss under ResNet-50 and MobiletNet-V3 in
                        Layoutloop.
                        On
                        practical FPGA devices, FEATHER achieves 2.65/3.91x higher throughput than Xilinx
                        DPU/Gemmini.
                        Remarkably, such performance and energy efficiency enhancements come at only 6% area over
                        a
                        fixed-dataflow Eyeriss-like accelerator. Our code is available at
                        https://github.com/maeri-project/FEATHER.
                      </div>
                      <div id="bib13" style="display:none;">
                        @inproceedings{tong2024FEATHER,
                        author = {Tong, Jianming and Itagi, Anirudh and Chatarasi, Parsanth and Krishna, Tushar},
                        title = {FEATHER: A Reconfigurable Accelerator with Data Reordering Support for Low-Cost
                        On-Chip
                        Dataflow Switching},
                        year = {2024},
                        publisher = {Association for Computing Machinery},
                        address = {Argentina},
                        abstract = {The inference of ML models composed of diverse structures, types, and sizes
                        boils
                        down to the execution of different dataflows (i.e. different tiling, ordering,
                        parallelism,
                        and
                        shapes). Using the optimal dataflow for every layer of workload can reduce latency by up
                        to
                        two
                        orders of magnitude over a suboptimal dataflow. Unfortunately, reconfiguring hardware for
                        different dataflows involves on-chip data layout reordering and datapath reconfigurations,
                        leading to non-trivial overhead that hinders ML accelerators from exploiting different
                        dataflows, resulting in suboptimal performance. To address this challenge, we propose
                        FEATHER,
                        an innovative accelerator that leverages a novel spatial array termed Nest and a novel
                        multi-stage reduction network called BIRRD for performing flexible data reduction with
                        layout
                        reordering under the hood, enabling seamless switching between optimal dataflows with
                        negligible
                        latency and resources overhead. For systematically evaluating the performance interaction
                        between dataflows and layouts, we enhance Timeloop, a state-of-the-art dataflow cost
                        modeling
                        and search framework, with layout assessment capabilities, and term it as Layoutloop. We
                        model
                        FEATHER into Layoutloop and also deploy FEATHER end-to-end on the edge ZCU104 FPGA.
                        FEATHER
                        delivers 1.27~2.89x inference latency speedup and 1.3~6.43x energy efficiency improvement
                        compared to various SoTAs like NVDLA, SIGMA and Eyeriss under ResNet-50 and MobiletNet-V3
                        in
                        Layoutloop. On practical FPGA devices, FEATHER achieves 2.65/3.91x higher throughput than
                        Xilinx
                        DPU/Gemmini. Remarkably, such performance and energy efficiency enhancements come at only
                        6%
                        area over a fixed-dataflow Eyeriss-like accelerator.},
                        booktitle = {Proceedings of the 51th Annual International Symposium on Computer
                        Architecture},
                        keywords = {flexible accelerator, dataflow-layout coswitching},
                        location = {Argentina},
                        series = {ISCA '24}
                        }
                      </div>
                    </div>
                  </div>
                </div>


                <div class="publication-item">
                  <div class="pub-image">
                    <img src="./images/SquareLoopHASP25.png" width="95%" style="display: block;
                                     margin-left: auto; margin-left:
                                     auto;">
                  </div>
                  <div class="pub-content">
                    <div class="title">
                      <b>SquareLoop: Explore Optimal Authentication Block Strategy for ML</b>
                    </div>
                    <div class="authors">
                      <a href="https://www.linkedin.com/in/jan-strzeszynski/">Jan Strzeszynski*</a>,
                      <b>Jianming Tong*</b>,
                      <a href="https://scholar.google.com/citations?user=m6p0KooAAAAJ&hl=en">Kyungmi Lee*</a>,
                      <a href="https://www.linkedin.com/in/nathanxiong/">Nathan Xiong</a>,
                      <a href="https://www.parashar.org/">Angshuman Parashar</a>,
                      <a href="https://people.csail.mit.edu/emer/">Joel S Emer</a>,
                      <a href="https://scholar.google.com/citations?user=P__ztgcAAAAJ&hl=en">Tushar Krishna</a>,
                      and <a href="https://people.csail.mit.edu/mengjia/">Mengjia Yan</a>
                    </div>
                    <div class="venue">
                      <i> Proceedings of the 14th International Workshop on Hardware and Architectural Support for
                        Security and Privacy (<b style="color:#ff6600;">HASP</b>), Oct 2025.</i>
                    </div>
                    <div class="profile-links">
                      <a onclick="displayid('abs15');">abstract</a>
                      <a href="https://dl.acm.org/doi/full/10.1145/3768725.3768732">paper</a>
                      <a href="https://github.com/maeri-project/squareloop">code</a>
                      <a onclick="displayid('bib15');">bibtex</a>
                      <div id="abs15" style="display:none;">
                        Off-chip memory in ML accelerators is vulnerable to both hardware and software attack,
                        which
                        needs encryption and authentication. Precise performance modeling of it requires (1)
                        representation of authentication blocks (AuthBlock) to cover the full design space of
                        shapes
                        and
                        orientations, and (2) precise memory behavior modeling, as encryption and authentication
                        mainly
                        increase memory traffic. This paper introduces S2Loop, a framework that resolves these
                        challenges by introducing (1) flexible, all-level partitioning based AuthBlocks for
                        ensuring
                        full coverage of the entire design space, (2) a realistic layout-based memory model, and
                        (3)
                        an
                        Mapping-Layout-Authentication co-search algorithm to explore the drastic combinatorial
                        design
                        space to figure out optimal mapping, layout, and AuthBlock shape choice for multi-layer
                        workloads. SquareLoop’s detailed memory model helps find better mapping to achieve 1.32 ×
                        speedup on ResNet18 compared to the SotA SecureLoop, and our latency predictions are
                        validated
                        to within 7.3% of an RTL implementation. S2Loop also achieve up-to 1.08 × /1.82 × overall
                        speedup for authenticated ResNet18/MobileNet-V3 on various accelerators with AuthBlock and
                        Mapping co-searching. We open-source S2Loop to provide a powerful and validated tool for
                        designing efficient, secure accelerators at https://github.com/maeri-project/squareloop.
                      </div>
                      <div id="bib15" style="display:none;">
                        @inproceedings{10.1145/3768725.3768732,
                        author = {Strzeszynski, Jan and Tong, Jianming and Lee, Kyungmi and Xiong, Nathan and
                        Parashar,
                        Angshuman and Emer, Joel S. and Krishna, Tushar and Yan, Mengjia},
                        title = {SquareLoop: Explore Optimal Authentication Block Strategy for ML},
                        year = {2025},
                        isbn = {9798400721984},
                        publisher = {Association for Computing Machinery},
                        address = {New York, NY, USA},
                        url = {https://doi.org/10.1145/3768725.3768732},
                        doi = {10.1145/3768725.3768732},
                        abstract = {Off-chip memory in ML accelerators is vulnerable to both hardware and software
                        attack, which needs encryption and authentication. Precise performance modeling of it
                        requires
                        (1) representation of authentication blocks (AuthBlock) to cover the full design space of
                        shapes
                        and orientations, and (2) precise memory behavior modeling, as encryption and
                        authentication
                        mainly increase memory traffic. This paper introduces S2Loop, a framework that resolves
                        these
                        challenges by introducing (1) flexible, all-level partitioning based AuthBlocks for
                        ensuring
                        full coverage of the entire design space, (2) a realistic layout-based memory model, and
                        (3)
                        an
                        Mapping-Layout-Authentication co-search algorithm to explore the drastic combinatorial
                        design
                        space to figure out optimal mapping, layout, and AuthBlock shape choice for multi-layer
                        workloads. SquareLoop’s detailed memory model helps find better mapping to achieve 1.32
                        \texttimes{} speedup on ResNet18 compared to the SotA SecureLoop, and our latency
                        predictions
                        are validated to within 7.3\% of an RTL implementation. S2Loop also achieve up-to 1.08
                        \texttimes{} /1.82 \texttimes{} overall speedup for authenticated ResNet18/MobileNet-V3 on
                        various accelerators with AuthBlock and Mapping co-searching. We open-source S2Loop to
                        provide
                        a
                        powerful and validated tool for designing efficient, secure accelerators at .},
                        booktitle = {Proceedings of the 14th International Workshop on Hardware and Architectural
                        Support for Security and Privacy},
                        pages = {37–45},
                        numpages = {9},
                        keywords = {machine learning accelerator, performance modeling, memory encryption and
                        authentication},
                        location = {
                        },
                        series = {HASP '25}
                        }
                      </div>
                    </div>
                  </div>
                </div>


                <div class="publication-item">
                  <div class="pub-image">
                    <img src="./images/ISPASS25_scalesim.png" width="95%" style="display: block;
                                     margin-left: auto; margin-left:
                                     auto;">
                  </div>
                  <div class="pub-content">
                    <div class="title">
                      <b>SCALE-Sim v3: A Modular Cycle-Accurate Systolic Accelerator Simulator for End-to-End
                        System
                        Analysis</b>
                    </div>
                    <div class="authors">
                      <a href="https://scholar.google.com/citations?user=I8kUcx4AAAAJ&hl=en">Ritik Raj</a>,
                      <a href="https://scholar.google.com/citations?user=bsF9XU4AAAAJ&hl=en">Sarbartha
                        Banerjee*</a>,
                      <a href="http://zishenwan.github.io">Nikhil Srinivas*</a>,
                      <a href="https://zishenwan.github.io/">Zishen Wan*</a>,
                      <strong>Jianming Tong*</strong>,
                      <a href="https://anands09.github.io">Ananda Samajdar</a>,
                      <a href="https://scholar.google.com/citations?user=P__ztgcAAAAJ&hl=en">Tushar Krishna</a>
                    </div>
                    <div class="venue">
                      <i> IEEE International Symposium on Performance Analysis of Systems and Software (<b
                          style="color:#ff6600;">ISPASS</b>), Sep 2025.</i>
                    </div>
                    <div class="profile-links">
                      <a onclick="displayid('abs17');">abstract</a>
                      <a href="https://arxiv.org/abs/2504.15377">paper</a>
                      <a href="https://github.com/scalesim-project/scale-sim-v2">code</a>
                      <a onclick="displayid('bib17');">bibtex</a>
                      <div id="abs17" style="display:none;">
                        We present SCALE-Sim v3, a modular cycle-accurate simulator for systolic-array-based
                        architectures, featuring multi-core architecture with spatio-temporal partitioning,
                        sparsity,
                        DRAM ramulator, precise data layout modeling, and energy and power estimation via
                        Accelergy.
                      </div>
                      <div id="bib17" style="display:none;">
                        @misc{raj2025scalesimv3modularcycleaccurate,
                        title={SCALE-Sim v3: A modular cycle-accurate systolic accelerator simulator for
                        end-to-end
                        system analysis},
                        author={Ritik Raj and Sarbartha Banerjee and Nikhil Chandra and Zishen Wan and Jianming
                        Tong
                        and
                        Ananda Samajdar and Tushar Krishna},
                        year={2025},
                        eprint={2504.15377},
                        archivePrefix={arXiv},
                        primaryClass={cs.PF},
                        url={https://arxiv.org/abs/2504.15377},
                        }
                      </div>
                    </div>
                  </div>
                </div>



                <div class="publication-item">
                  <div class="pub-image">
                    <img src="./images/smartpaf.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                  </div>
                  <div class="pub-content">
                    <div class="title">
                      <b>SmartPAF: Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for
                        Fast
                        Private Inference in Homomorphic Encryption</b>
                    </div>
                    <div class="authors">
                      <b>Jianming Tong*</b>,
                      <a href="https://www.linkedin.com/in/jingtian-dang-568615207/">Jingtian Dang*</a>,
                      <a href="https://scholar.google.com/citations?user=QLN76UUAAAAJ&hl=en">Anupam Golder</a>,
                      <a href="https://sites.gatech.edu/ece-callie/">Callie Hao</a>,
                      <a href="https://scholar.google.com/citations?hl=en&user=lXUce-0AAAAJ">Arijit
                        Raychowdhury</a>,
                      <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
                    </div>
                    <div class="venue">
                      <i> In Proc of Seventh Conference on Machine Learning and Systems, (<b
                          style="color:#ff6600;">MLSys</b>), May 2024.</i>
                    </div>
                    <!-- <div class="insight">
                                          <i> <b> Insight 1: </b> Weights-sharing neural network enables a dynamic latency-accuracy serving.</i> <br>
                                          <i> <b> Insight 2: </b> Putting common weights on-chip (Subgraph Stationary) could save off-chip data access latency and thus push model towards computattion-bound.</i>
                                       </div> -->
                    <div class="profile-links">
                      <a onclick="displayid('abs12');">abstract</a>
                      <a href="https://arxiv.org/abs/2404.03216">paper</a>
                      <a href="https://github.com/TorchFHE/SmartPAF">code</a>
                      <a onclick="displayid('bib12');">bibtex</a>
                      <div id="abs12" style="display:none;">
                        As machine learning (ML) permeates fields like healthcare, facial recognition, and
                        blockchain,
                        the need to protect sensitive data intensifies. Fully Homomorphic Encryption (FHE) allows
                        inference on encrypted data, preserving the privacy of both data and the ML model.
                        However, it
                        slows down non-secure inference by up to five magnitudes, with a root cause of replacing
                        non-polynomial operators (ReLU and MaxPooling) with high-degree Polynomial Approximated
                        Function
                        (PAF). We propose SmartPAF, a framework to replace non-polynomial operators with
                        low-degree
                        PAF
                        and then recover the accuracy of PAF-approximated model through four techniques: (1)
                        Coefficient
                        Tuning (CT) -- adjust PAF coefficients based on the input distributions before training,
                        (2)
                        Progressive Approximation (PA) -- progressively replace one non-polynomial operator at a
                        time
                        followed by a fine-tuning, (3) Alternate Training (AT) -- alternate the training between
                        PAFs
                        and other linear operators in the decoupled manner, and (4) Dynamic Scale (DS) / Static
                        Scale
                        (SS) -- dynamically scale PAF input value within (-1, 1) in training, and fix the scale as
                        the
                        running max value in FHE deployment. The synergistic effect of CT, PA, AT, and DS/SS
                        enables
                        SmartPAF to enhance the accuracy of the various models approximated by PAFs with various
                        low
                        degrees under multiple datasets. For ResNet-18 under ImageNet-1k, the Pareto-frontier
                        spotted
                        by
                        SmartPAF in latency-accuracy tradeoff space achieves 1.42x ~ 13.64x accuracy improvement
                        and
                        6.79x ~ 14.9x speedup than prior works. Further, SmartPAF enables a 14-degree PAF (f1^2
                        g_1^2)
                        to achieve 7.81x speedup compared to the 27-degree PAF obtained by minimax approximation
                        with
                        the same 69.4% post-replacement accuracy. Our code is available at
                        \url{https://github.com/EfficientFHE/SmartPAF}
                      </div>
                      <div id="bib12" style="display:none;">
                        @inproceedings{tong2024accurate,
                        author={Jianming Tong and Jingtian Dang and Anupam Golder and Callie Hao and Arijit
                        Raychowdhury
                        and Tushar Krishna},
                        booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
                        title={Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators for Fast
                        Private
                        Inference in Homomorphic Encryption},
                        url = {https://arxiv.org/abs/2404.03216},
                        year = {2024}
                        }
                      </div>
                    </div>
                  </div>
                </div>



                <div class="publication-item">
                  <div class="pub-image">
                    <img src="./images/sushi_micro.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                  </div>
                  <div class="pub-content">
                    <div class="title">
                      <b>Hardware-Software co-design for real-time latency-accuracy navigation in tinyML
                        applications</b>
                    </div>
                    <div class="authors">
                      <a href="https://paymanbehnam.com/">Payman Behnam*</a>,
                      <b>Jianming Tong*</b>,
                      <a href="https://www.cc.gatech.edu/~akhare39/">Alind Khare</a>,
                      <a href="https://www.linkedin.com/in/yangyu-chen/">Yangyu Chen</a>,
                      <a href="https://www.linkedin.com/in/yue-pan-061439192/">Yue Pan</a>,
                      <a href="https://www.linkedin.com/in/abhimanyu-bambhaniya/">Abhimanyu Bambhaniya</a>,
                      <a>Pranav Gadikar</a>,
                      <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>, and
                      <a href="https://www.cc.gatech.edu/~atumanov/">Alexey Tumanov</a>
                    </div>
                    <div class="venue">
                      <i> (<b style="color:#ff6600;">IEEE micro</b>), Sep 2023.</i>
                    </div>
                    <!-- <div class="insight">
                                          <i> <b> Insight 1: </b> Weights-sharing neural network enables a dynamic latency-accuracy serving.</i> <br>
                                          <i> <b> Insight 2: </b> Putting common weights on-chip (Subgraph Stationary) could save off-chip data access latency and thus push model towards computattion-bound.</i>
                                       </div> -->
                    <div class="profile-links">
                      <a onclick="displayid('abs11');">abstract</a>
                      <a href="https://www.computer.org/csdl/magazine/mi/5555/01/10257666/1QARI5P37ZC">paper</a>
                      <a onclick="displayid('bib11');">bibtex</a>
                      <div id="abs11" style="display:none;">
                        tinyML applications increasingly operate in dynamically changing deployment scenarios,
                        requiring
                        optimizing for both accuracy and latency. Existing methods mainly target a single point in
                        the
                        accuracy/latency tradeoff space---insufficient as no single static point can be optimal
                        under
                        variable conditions. We draw on a recently proposed weight-shared SuperNet mechanism to
                        enable
                        serving a stream of queries that activates different SubNets within a SuperNet. This
                        creates
                        an
                        opportunity to exploit the inherent temporal locality of different queries that use the
                        same
                        SuperNet. We propose a hardware-software co-design called SUSHI that introduces a novel
                        SubGraph
                        Stationary optimization. SUSHI consists of a novel FPGA implementation and a software
                        scheduler
                        that controls which SubNets to serve and what SubGraph to cache in real-time. SUSHI yields
                        up
                        to
                        32% improvement in latency, 0.98% increase in served accuracy, and achieves up to 78.7%
                        saved
                        off-chip energy across several neural network architectures.
                      </div>
                      <div id="bib11" style="display:none;">
                        @ARTICLE {10257666,
                        author = {P. Behnam and J. Tong and A. Khare and Y. Chen and Y. Pan and P. Gadikar and A.
                        Bambhaniya and T. Krishna and A. Tumanov},
                        journal = {IEEE Micro},
                        title = {Hardware-Software co-design for real-time latency-accuracy navigation in tinyML
                        applications},
                        year = {5555},
                        volume = {},
                        number = {01},
                        issn = {1937-4143},
                        pages = {1-7},
                        abstract = {tinyML applications increasingly operate in dynamically changing deployment
                        scenarios, requiring optimizing for both accuracy and latency. Existing methods mainly
                        target
                        a
                        single point in the accuracy/latency tradeoff space—insufficient as no single static point
                        can
                        be optimal under variable conditions. We draw on a recently proposed weight-shared
                        SuperNet
                        mechanism to enable serving a stream of queries that activates different SubNets within a
                        SuperNet. This creates an opportunity to exploit the inherent temporal locality of
                        different
                        queries that use the same SuperNet. We propose a hardware-software co-design called SUSHI
                        that
                        introduces a novel SubGraph Stationary optimization. SUSHI consists of a novel FPGA
                        implementation and a software scheduler that controls which SubNets to serve and what
                        SubGraph
                        to cache in real-time. SUSHI yields up to 32% improvement in latency, 0.98% increase in
                        served
                        accuracy, and achieves up to 78.7% saved off-chip energy across several neural network
                        architectures.},
                        keywords = {kernel;training;real-time systems;optimization;neural
                        networks;system-on-chip;software},
                        doi = {10.1109/MM.2023.3317243},
                        publisher = {IEEE Computer Society},
                        address = {Los Alamitos, CA, USA},
                        month = {sep}
                        }
                      </div>
                    </div>
                  </div>
                </div>


                <div class="publication-item">
                  <div class="pub-image">
                    <img src="./images/sushi.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
                  </div>
                  <div class="pub-content">
                    <div class="title">
                      <b>SUSHI: SUbgraph Stationary Hardware-software Inference Co-design</b>
                    </div>
                    <div class="authors">
                      <a href="https://paymanbehnam.com/">Payman Behnam*</a>,
                      <b>Jianming Tong*</b>,
                      <a href="https://www.cc.gatech.edu/~akhare39/">Alind Khare</a>,
                      <a href="https://www.linkedin.com/in/yangyu-chen/">Yangyu Chen</a>,
                      <a href="https://www.linkedin.com/in/yue-pan-061439192/">Yue Pan</a>,
                      <a href="https://www.linkedin.com/in/abhimanyu-bambhaniya/">Abhimanyu Bambhaniya</a>,
                      <a>Pranav Gadikar</a>,
                      <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>, and
                      <a href="https://www.cc.gatech.edu/~atumanov/">Alexey Tumanov</a>
                    </div>
                    <div class="venue">
                      <i>In Proc of Sixth Conference on Machine Learning and Systems (<b
                          style="color:#ff6600;">MLSys</b>), Jun
                        2023.</i>
                    </div>
                    <div>
                      <i><b style="color:#ff6600;">++Qualcomm Innovation Fellowship</b></i>
                    </div>
                    <div>
                      <i><b style="color:#ff6600;">++Best Poster Award (IAP2023@MIT)</b></i>
                    </div>
                    <div class="profile-links">
                      <a onclick="displayid('abs7');">abstract</a>
                      <a href="https://arxiv.org/abs/2306.17266">paper</a>
                      <a onclick="displayid('bib7');">bibtex</a>
                      <div id="abs7" style="display:none;">
                        A growing number of applications depend on Machine Learning (ML) functionality and
                        benefits
                        from
                        both higher quality ML predictions and better timeliness (latency) at the same time. A
                        growing
                        body of research in computer architecture, ML, and systems software literature focuses on
                        reaching better latency/accuracy tradeoffs for ML models. Efforts include compression,
                        quantization, pruning, early-exit models, mixed DNN precision, as well as ML inference
                        accelerator designs that minimize latency and energy, while preserving delivered accuracy.
                        All
                        of them, however, yield improvements for a single static point in the latency/accuracy
                        tradeoff
                        space. We make a case for applications that operate in dynamically changing deployment
                        scenarios, where no single static point is optimal. We draw on a recently proposed
                        weight-shared
                        SuperNet mechanism to enable serving a stream of queries that uses (activates) different
                        SubNets
                        within this weight-shared construct. This creates an opportunity to exploit the inherent
                        temporal locality with our proposed SubGraph Stationary (SGS) optimization. We take a
                        hardware-software co-design approach with a real implementation of SGS in SushiAccel and
                        the
                        implementation of a software scheduler SushiSched controlling which SubNets to serve and
                        what
                        to
                        cache in real-time. Combined, they are vertically integrated into SUSHI---an inference
                        serving
                        stack. For the stream of queries, SUSHI yields up to 25% improvement in latency, 0.98%
                        increase
                        in served accuracy. SUSHI can achieve up to 78.7% off-chip energy savings.
                      </div>
                      <div id="bib7" style="display:none;">
                        @misc{behnam2023subgraph,
                        title={Subgraph Stationary Hardware-Software Inference Co-Design},
                        author={Payman Behnam and Jianming Tong and Alind Khare and Yangyu Chen and Yue Pan and
                        Pranav
                        Gadikar and Abhimanyu Rajeshkumar Bambhaniya and Tushar Krishna and Alexey Tumanov},
                        year={2023},
                        eprint={2306.17266},
                        archivePrefix={arXiv},
                        primaryClass={cs.DC}
                        }
                      </div>
                    </div>
                  </div>
                </div>
        </td>
      </tr>
      </tbody>
      </table>
      </td>
      </tr>



      <div class="publication-item">
        <div class="pub-image">
          <img src="./images/icra.png" width="95%" style="display: block;
                                      margin-left: auto; margin-left:
                                      auto;">
        </div>
        <div class="pub-content">
          <div class="title">
            <b>SMMR-explore: Submap-based multi-robot exploration system with multi-robot multi-target
              potential field exploration method</b>
          </div>
          <div class="authors">
            <a href="https://nicsefc.ee.tsinghua.edu.cn/people/yujinchengyu/">Jincheng Yu*</a>,
            <b> Jianming Tong*</b>,
            <a href="https://scholar.google.com/citations?user=gOuB4zQAAAAJ&hl=en">Yuanfan Xu</a>,
            <a href="https://nicsefc.ee.tsinghua.edu.cn/people/zhilin-xu/">Zhilin Xu</a>,
            <a href="http://nicsefc.ee.tsinghua.edu.cn/people/donghl17/">Haolin Dong</a>,
            <a href="http://nicsefc.ee.tsinghua.edu.cn/people/tianxiang-yang/">Tianxiang Yang</a>, and
            <a href="http://nicsefc.ee.tsinghua.edu.cn/people/yu-wang/">Yu Wang</a>.<br>
          </div>
          <div class="venue">
            <i>IEEE International Conference on Robotics and Automation (<b style="color:#ff6600;">ICRA</b>),
              2021.</i> <b>Oral</b>
          </div>
          <div class="links">
            <a onclick="displayid('abs3');">abstract</a>
            <a href="https://ieeexplore.ieee.org/document/9561328">paper</a>
            <a href="https://github.com/efc-robot/SMMR-Explore">code</a>
            <!-- <a href="https://www.youtube.com/watch?v=H1zwRIz8OYs">demo</a> -->
            <a onclick="displayid('demo3');">demo</a>
            <a onclick="displayid('bib3');">bibtex</a>
            <div id="abs3" style="display:none;">
              Collaborative exploration in an unknown environment without external positioning under
              limited
              communication is an essential task for multi-robot applications. For inter-robot
              positioning,
              various Distributed Simultaneous Localization and Mapping (DSLAM) systems share the Place
              Recognition (PR) descriptors and sensor data to estimate the relative pose between robots
              and
              merge robots’ maps. As maps are constantly shared among robots in exploration, we design a
              map-based DSLAM framework, which only shares the submaps, eliminating the transfer of PR
              descriptors and sensor data. Our framework saves 30% of total communication traffic. For
              exploration, each robot is assigned to get much unknown information about environments
              with
              paying little travel cost. As the number of sampled points increases, the goal would
              change
              back
              and forth among sampled frontiers, leading to the downgrade in exploration efficiency and
              the
              overlap of trajectories. We propose an exploration strategy based on Multi-robot
              Multi-target
              Potential Field (MMPF), which can eliminate goal’s back-and-forth changes, boosting the
              exploration efficiency by 1.03 ×∼1.62 × with 3 % ∼ 40 % travel cost saved. Our
              SubMap-based
              Multi-robot Exploration method (SMMR-Explore) is evaluated on both Gazebo simulator and
              real
              robots. The simulator and the exploration framework are published as an open-source ROS
              project
              at https://github.com/efc-robot/SMMR-Explore.
            </div>
            <div id="bib3" style="display:none;">
              @INPROCEEDINGS{9561328,
              author={Yu, Jincheng and Tong, Jianming and Xu, Yuanfan and Xu, Zhilin and Dong, Haolin
              and
              Yang, Tianxiang and Wang, Yu},
              booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
              title={SMMR-Explore: SubMap-based Multi-Robot Exploration System with Multi-robot
              Multi-target
              Potential Field Exploration Method},
              year={2021},
              volume={},
              number={},
              pages={8779-8785},
              doi={10.1109/ICRA48506.2021.9561328}}
            </div>
            <div id="demo3" style="display:none;">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/H1zwRIz8OYs"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>



      <details class="collaborative-pubs">
        <summary>
          <h3>Collaborative Publications (* Equal Contribution)</h3>
        </summary>
        <p><b> As Collaborator or Mentor </b></p>

        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/ISPASS25_XRBench_CDA.png" width="95%" style="display: block;
                                      margin-left: auto; margin-left:
                                      auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>Exploring Constrained Dataflow Accelerator for Real-Time Multi-Task Multi-Model Machine
                Learning Workloads</b>
            </div>
            <div class="authors">
              <a>Jamin Seo</a>,
              <b>Jianming Tong</b>,
              <b>Hyoukjun Kwon</b>,
              <a href="https://scholar.google.com/citations?user=P__ztgcAAAAJ&hl=en">Tushar Krishna</a>
              and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
            </div>
            <div class="venue">
              <i> IEEE International Symposium on Performance Analysis of Systems and Software (<b
                  style="color:#ff6600;">ISPASS</b>), Sep 2025.</i>
            </div>
            <div class="profile-links">
              <a onclick="displayid('abs16');">abstract</a>
              <a href="https://ieeexplore.ieee.org/document/11096375">paper</a>
              <a onclick="displayid('bib16');">bibtex</a>
              <div id="abs16" style="display:none;">
                Emerging machine learning (ML) workloads, such as those in AR/VR applications or drones,
                exhibit
                real-time multitask multi-model (RT-MTMM) characteristics. These workloads demand
                efficient
                executions of diverse combinations of multiple models within strict real-time constraints
                and
                tight energy budgets. Consequently, optimizing hardware accelerator design and dataflow
                choices
                for such ML models becomes imperative. Flexible dataflows in hardware have been explored
                to
                enhance performance and energy efficiency for diverse models. However, this flexibility
                involves
                substantial hardware costs, potentially outweighing the performance benefits. While
                coarse-grained flexible accelerators, including heterogeneous dataflow accelerators [1],
                have
                been proposed to mitigate such costs, they may prove suboptimal depending on combinations
                of
                models, potentially leading to under-performance of the hardware. Hence, this paper
                investigates
                the desired balance between the benefits and costs of flexibility. We present a systematic
                and
                quantitative exploration of medium-grained flexible accelerators, demonstrating that
                constrained
                yet judicious domain-aware flexibility choices in tile sizes (T), loop order (O), parallel
                dimensions (P), and array shape (S) can allow RT-MTMM accelerators to achieve comparable
                real-time performance (×1.06/×1.13) and energy efficiency (×1.09/×1.07) to fully flexible
                designs with significantly lower area overhead (×0.67×0.91) on edge/mobilescale
                accelerators.
              </div>
              <div id="bib16" style="display:none;">
                @INPROCEEDINGS{11096375,
                author={Seo, Jamin and Tong, Jianming and Krishna, Tushar and Kwon, Hyoukjun},
                booktitle={2025 IEEE International Symposium on Performance Analysis of Systems and
                Software
                (ISPASS)},
                title={Exploring Constrained Dataflow Accelerators for Real-Time Multi-Task Multi-Model Ml
                Workloads},
                year={2025},
                volume={},
                number={},
                pages={1-11},
                keywords={Costs;Systematics;Shape;Machine learning;Multitasking;Real-time systems;Energy
                efficiency;Software;Space exploration;Resource management;Multi-task multi-model machine
                learning;flexible dataflow;machine learning system},
                doi={10.1109/ISPASS64960.2025.00014}}
              </div>
            </div>
          </div>
        </div>

        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/real_time_RF.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>Real-time Digital RF Emulation – II: A Near Memory Custom Accelerator</b>
            </div>
            <div class="authors">
              <a>Xiangyu Mao</a>,
              <a href="https://www.linkedin.com/in/jamin-seo-8aba1a13b/?originalSubdomain=kr">Mandovi
                Mukherjee</a>,
              <a href="https://scholar.google.com/citations?user=5gRIUykAAAAJ&hl=en">Nael Mizanur
                Rahman</a>,
              <a>Coleman B DeLude</a>,
              <a>Joseph W. Driscoll</a>,
              <a>Sudarshan Sharma</a>,
              <a>Payman Behnam</a>,
              <a>Uday Kamal</a>,
              <a>Jongseok Woo</a>,
              <a>Daehyun Kim</a>,
              <a>Sharjeel M. Khan</a>,
              <b>Jianming Tong</b>,
              <a>Jamin Seo</a>,
              <a>Prachi Sinha</a>,
              <a>Madhavan Swaminathan</a>,
              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
              <a>Santosh Pande</a>,
              <a>Justin Romberg</a>,
              and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
            </div>
            <div class="venue">
              <i> IEEE Transactions on Radar Systems (<b style="color:#ff6600;">TRadar</b>), Sep 2024.</i>
            </div>
            <div class="profile-links">
              <a onclick="displayid('abs13');">abstract</a>
              <a href="https://ieeexplore.ieee.org/document/10672547">paper</a>
              <a onclick="displayid('bib13');">bibtex</a>
              <div id="abs13" style="display:none;">
                A near memory hardware accelerator, based on a novel direct path computational model, for
                real-time emulation of radio frequency systems is demonstrated. Our evaluation of hardware
                performance uses both application-specific integrated circuits (ASIC) and field
                programmable
                gate arrays (FPGA) methodologies: 1). The ASIC test-chip implementation, using TSMC 28nm
                CMOS,
                leverages distributed autonomous control to extract concurrency in compute as well as low
                latency. It achieves a 518 MHz per channel bandwidth in a prototype 4-node system. The
                maximum
                emulation range supported in this paradigm is 9.5 km with 0.24 μs of per-sample emulation
                latency. 2). The FPGA-based implementation, evaluated on a Xilinx ZCU104 board,
                demonstrates a
                9-node test case (two Transmitters, one Receiver, and 6 passive reflectors) with an
                emulation
                range of 1.13 km to 27.3 km at 215 MHz bandwidth.
              </div>
              <div id="bib13" style="display:none;">
                @ARTICLE{10672547,
                author={Mao, X. and Mukherjee, M. and Mizanur Rahman, N. and DeLude, C. and Driscoll, J.
                and
                Sharma, S. and Behnam, P. and Kamal, U. and Woo, J. and Kim, D. and Khan, S. and Tong, J.
                and
                Seo, J. and Sinha, P. and Swaminathan, M. and Krishna, T. and Pande, S. and Romberg, J.
                and
                Mukhopadhyay, S.},
                journal={IEEE Transactions on Radar Systems},
                title={Real-time Digital RF Emulation – II: A Near Memory Custom Accelerator},
                year={2024},
                volume={},
                number={},
                pages={1-1},
                keywords={Radio frequency;Computational modeling;Emulation;Computer
                architecture;Hardware;Real-time systems;Pulse modulation;hardware
                accelerators;near-memory;radio
                frequency emulator;real-time},
                doi={10.1109/TRS.2024.3457523}}
              </div>
            </div>
          </div>
        </div>


        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/snatch.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>SNATCH: Stealing Neural Network Architecture from ML Accelerator in Intelligent
                Sensors</b>
            </div>
            <div class="authors">
              <a href="https://www.sudarshan-sh.com/"> Sudarshan Sharma</a>,
              <a href="https://udaykamal20.github.io/"> Uday Kamal</a>,
              <b>Jianming Tong</b>,
              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
              and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
            </div>
            <div class="venue">
              <i>IEEE SENSORS conference(<b style="color:#ff6600;">SENSORS</b>), Aug 2023.</i>
            </div>
            <div class="profile-links">
              <a onclick="displayid('abs10');">abstract</a>
              <!-- <a href="https://assets.researchsquare.com/files/rs-2910088/v1_covered_0e6c94bd-1499-4b2c-b414-902c232b490c.pdf?c=1683777877">paper</a> -->
              <!-- <a href="https://github.com/TorchFHE/PAF-FHE">code</a> -->
              <a href="https://epapers2.org/sensors2023/ESR/paper_details.php?paper_id=1451">poster</a>
              <!-- <a onclick="displayid('bib8');">bibtex</a> -->
              <div id="abs10" style="display:none;">
                The use of Machine Learning (ML) models executing on ML Accelerators (MLA) in Intelligent
                sensors for feature extraction has garnered substantial interest. The Neural Network (NN)
                architecture implemented of MLA are intellectual property for the vendors. Along with
                improved
                power-efficiency and reduced bandwidth, the hardware based ML models embedded in the
                sensor
                also
                provides additional security against cyber-attacks on the ML. In this paper, we introduce
                an
                attack referred as SNATCH which uses a profiling-based side channel attack (SCA) that aims
                to
                steal the NN architecture executing on a digital MLA (Deep Learning Processing Unit (DPU)
                IP
                by
                Xilinx). We use electromagnetic side channel leakage from a clone device to create a
                profiler
                and then attack the victim's device to steal the NN architecture. Stealing the ML model
                undermines the intellectual property rights of the vendors of a sensor. Further, it also
                allows
                an adversary to mount critical Denial of Service and misuse attack.
              </div>
            </div>
          </div>
        </div>



        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/tvlsi23.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>On Continuing DNN Accelerator Architecture Scaling Using Tightly-coupled
                Compute-on-Memory 3D
                ICs</b>
            </div>
            <div class="authors">
              <a href="https://www.linkedin.com/in/gauthaman-murali-4110a732/">Gauthaman Murali</a>,
              <a href="https://www.linkedin.com/in/adityaiyer99/">Aditya Iyer</a>,
              <a href="https://www.linkedin.com/in/lingjunzhu/?locale=en_US">Lingjun Zhu</a>,
              <b>Jianming Tong</b>,
              <a href="https://www.linkedin.com/in/fr-munoz-mrtinez/?originalSubdomain=es">Francisco Munoz
                Martinez</a>,
              <a href="https://www.linkedin.com/in/srivatsa-rangachar-srinivasa-8b0b4a130/">Srivatsa
                Rangachar
                Srinivasa</a>,
              <a href="https://ieeexplore.ieee.org/author/37268291400">Tanay Karnik</a>,
              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
              <a href="https://ece.gatech.edu/directory/sung-kyu-lim">Sung Kyu Lim</a>
            </div>
            <div class="venue">
              <i>IEEE Transactions on Very Large Scale Integration Systems (<b style="color:#ff6600;">TVLSI</b>),
                Jul 2023.</i>
            </div>
            <div class="profile-links">
              <a onclick="displayid('abs9');">abstract</a>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10221779">paper</a>
              <!-- <a href="https://github.com/TorchFHE/PAF-FHE">code</a> -->
              <!-- <a href="publications/PAF_FHE_poster.pdf">poster</a> -->
              <a onclick="displayid('bib9');">bibtex</a>
              <div id="abs9" style="display:none;">
                This work identifies the architectural and design scaling limits of 2D flexible
                interconnect
                DNN
                accelerators and addresses them with 3D ICs. We demonstrate how scaling up a baseline 2D
                accelerator in the X/Y dimension fails and how vertical stacking effectively overcomes the
                failure. We designed multi-tier accelerators that are 1.67X faster than the 2D design.
                Using
                our
                3D architecture and circuit co-design methodology, we improve throughput,
                energy-efficiency,
                and
                area-efficiency by up to 5X, 1.2X, and 3.9X, respectively, over 2D counterparts. The
                IR-drop
                in
                our 3D designs is within 10.7% of VDD, and the temperature variation is within 12˚C.
              </div>
              <div id="bib9" style="display:none;">
                @ARTICLE{10221779,
                author={Murali, Gauthaman and Iyer, Aditya and Zhu, Lingjun and Tong, Jianming and
                Martínez,
                Francisco Muñoz and Srinivasa, Srivatsa Rangachar and Karnik, Tanay and Krishna, Tushar
                and
                Lim,
                Sung Kyu},
                journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
                title={On Continuing DNN Accelerator Architecture Scaling Using Tightly Coupled
                Compute-on-Memory 3-D ICs},
                year={2023},
                volume={},
                number={},
                pages={1-11},
                doi={10.1109/TVLSI.2023.3299564}}
              </div>
            </div>
          </div>
        </div>


        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/mao.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>FPGA-Based High-Performance Real-Time Emulation of Radar System using Direct Path Compute
                Model</b>
            </div>
            <div class="authors">
              <a>Xiangyu Mao*</a>,
              <a href="https://www.linkedin.com/in/jamin-seo-8aba1a13b/?originalSubdomain=kr">Mandovi
                Mukherjee*</a>,
              <a href="https://scholar.google.com/citations?user=5gRIUykAAAAJ&hl=en">Nael Mizanur
                Rahman*</a>,
              <a>Uday Kamal</a>,
              <a>Sudarshan Sharma</a>,
              <a>Payman Behnam</a>,
              <b>Jianming Tong</b>,
              <a>Jongseok Woo</a>,
              <a>Coleman B DeLude</a>,
              <a>Joseph W. Driscoll</a>,
              <a>Jamin Seo</a>,
              <a>Santosh Pande</a>,
              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
              <a>Justin Romberg</a>,
              <a>Madhavan Swaminathan</a>,
              and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
            </div>
            <div class="venue">
              <i>International Microwave Symposium (<b style="color:#ff6600;">IMS</b>), Jun 2023.</i>
            </div>
            <div class="profile-links">
              <a onclick="displayid('abs7a');">abstract</a>
              <a href="https://ieeexplore.ieee.org/document/10187950">paper</a>
              <a onclick="displayid('bib7a');">bibtex</a>
              <div id="abs7a" style="display:none;">
                This paper proposes a Field-Programmable Gate Array (FPGA) based platform for real-time
                emulation of Radio Frequency (RF) signal interactions among radars and reflectors. Unlike
                conventional tapped-delay Finite-Impulse-Response (FIR) models, the paper presents an FPGA
                realization of Direct Path Compute model for RF signal propagation. Experimental results
                on a
                Xilinx ZCU104 board demonstrate a 5-platform system of the emulation range of 2.78km to
                27.3km
                at 180 MHz bandwidth.
              </div>
              <div id="bib7a" style="display:none;">
                @INPROCEEDINGS{10187950,
                author={Mao, X. and Mukherjee, M. and Rahman, N. M. and Kamal, U. and Sharma, S. and
                Behnam,
                P.
                and Tong, J. and Driscoll, J. and Krishna, T. and Romberg, J. and Mukhopadhyay, S.},
                booktitle={2023 IEEE/MTT-S International Microwave Symposium - IMS 2023},
                title={FPGA-Based High-Performance Real-Time Emulation of Radar System Using Direct Path
                Compute
                Model},
                year={2023},
                volume={},
                number={},
                pages={419-422},
                keywords={Radio frequency;Microwave measurement;Computational modeling;Emulation;RF
                signals;Radar;Bandwidth;RF emulation;direct path compute model;FPGA;real-time},
                doi={10.1109/IMS37964.2023.10187950}}

              </div>
            </div>
          </div>
        </div>


        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/mandovi.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>A High Performance Computing Architecture for Real-Time Digital Emulation of RF
                Interactions</b>
            </div>
            <div class="authors">
              <a href="https://www.linkedin.com/in/jamin-seo-8aba1a13b/?originalSubdomain=kr">Mandovi
                Mukherjee*</a>,
              <a href="https://scholar.google.com/citations?user=5gRIUykAAAAJ&hl=en">Nael Mizanur
                Rahman*</a>,
              <a href="https://scholar.google.com/citations?user=gQu_Pa8AAAAJ&hl=en">Coleman B.
                DeLude*</a>,
              <a href="https://www.linkedin.com/in/coleman-delude-a9962480/">Joseph W. Driscoll*</a>,
              <a>Uday Kamal</a>,
              <a>Jongseok Woo</a>,
              <a>Jamin Seo</a>,
              <a>Sudarshan Sharma</a>,
              <a>Xiangyu Mao</a>,
              <a>Payman Behnam,</a>,
              <a>Sharjeel M. Khan</a>,
              <a>Daehyun Kim</a>,
              <b>Jianming Tong</b>,
              <a>Prachi Sinha</a>,
              <a>Santosh Pande</a>,
              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
              <a>Justin Romberg</a>,
              <a>Madhavan Swaminathan</a>,
              and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
            </div>
            <div class="venue">
              <i>In Proc of IEEE Radar Conference, (<b style="color:#ff6600;">RadarConf</b>), May
                2023.</i>
            </div>
            <div class="profile-links">
              <a onclick="displayid('abs6');">abstract</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/10149577">paper</a>
              <a onclick="displayid('bib6');">bibtex</a>
              <div id="abs6" style="display:none;">
                A high performance architecture for emulating realtime radio frequency systems is
                presented.
                The
                architecture is developed based on a novel compute model and uses nearmemory techniques
                coupled
                with highly distributed autonomous control to simultaneously optimize throughput and
                minimize
                latency. A cycle level C++ based simulator is used to validate the proposed architecture
                with
                simulation of complex RF scenarios.
              </div>
              <div id="bib6" style="display:none;">
                @INPROCEEDINGS{10149577,
                author={Mukherjee, Mandovi and Rahman, Nael Mizanur and DeLude, Coleman and Driscoll,
                Joseph
                and
                Kamal, Uday and Woo, Jongseok and Seo, Jamin and Sharma, Sudarshan and Mao, Xiangyu and
                Behnam,
                Payman and Khan, Sharjeel and Kim, Daehyun and Tong, Jianming and Sinha, Prachi and Pande,
                Santosh and Krishna, Tushar and Romberg, Justin and Swaminathan, Madhavan and
                Mukhopadhyay,
                Saibal},
                booktitle={2023 IEEE Radar Conference (RadarConf23)},
                title={A High Performance Computing Architecture for Real-Time Digital Emulation of RF
                Interactions},
                year={2023},
                volume={},
                number={},
                pages={1-6},
                doi={10.1109/RadarConf2351548.2023.10149577}}
              </div>
            </div>
          </div>
        </div>



        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/jamin.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>A Configurable Architecture for Efficient Sparse FIR Computation in Real-time Radio
                Frequency
                Systems</b>
            </div>
            <div class="authors">
              <a href="https://www.linkedin.com/in/jamin-seo-8aba1a13b/?originalSubdomain=kr">Jamin
                Seo</a>,
              <a href="https://scholar.google.com/citations?user=5gRIUykAAAAJ&hl=en">Nael Mizanur
                Rahman</a>,
              <a href="https://scholar.google.com/citations?user=gQu_Pa8AAAAJ&hl=en">Mandovi
                Mukherjee</a>,
              <a href="https://www.linkedin.com/in/coleman-delude-a9962480/">Coleman DeLude</a>,
              <b>Jianming Tong</b>,
              <a href="https://jrom.ece.gatech.edu/">Justin Romberg</a>,
              <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
              and <a href="https://greenlab.ece.gatech.edu/members/professor/">Saibal Mukhopadhyay</a>.
            </div>
            <div class="venue">
              <i>International Microwave Symposium (<b style="color:#ff6600;">IMS</b>), 2021.</i>
            </div>
            <!-- <div class="insight">
                                          <i> <b> Insight 1: </b> Large-scale crossbar is implementation-prohibitive for long wires.</i> <br>
                                          <i> <b> Insight 2: </b> Using multi-stage interconnection to achieve function as crossbar could reduce half number of long wires and save power and area.</i>
                                       </div> -->
            <div class="profile-links">
              <a onclick="displayid('abs5');">abstract</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/9865486">paper</a>
              <a onclick="displayid('bib5');">bibtex</a>
              <div id="abs5" style="display:none;">
                A low-latency and high-throughput, configurable architecture for computing sparse Finite
                Impulse
                Response in real-time Radio Frequency domain is proposed. The massively parallel
                architecture
                uses distributed control in association with near-memory techniques to optimize area and
                power.
                It supports configurability in filter tap locations and handling of locally dense taps,
                making
                it more adaptable to Radio Frequency environments.
              </div>
              <div id="bib5" style="display:none;">
                @INPROCEEDINGS{9865486,
                author={Seo, Jamin and Mukherjee, Mandovi and Rahman, Nael Mizanur and Tong, Jianming and
                DeLude, Coleman and Krishna, Tushar and Romberg, Justin and Mukhopadhyay, Saibal},
                booktitle={2022 IEEE/MTT-S International Microwave Symposium - IMS 2022},
                title={A Configurable Architecture for Efficient Sparse FIR Computation in Real-time Radio
                Frequency Systems},
                year={2022},
                volume={},
                number={},
                pages={998-1001},
                doi={10.1109/IMS37962.2022.9865486}}
              </div>
            </div>
          </div>
        </div>


        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/acslam.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>ac2SLAM: FPGA Accelerated High-Accuracy SLAM with Heapsort and Parallel Keypoint
                Extractor</b>
            </div>
            <div class="authors">
              <a>Cheng Wang</a>,
              <a>Yinkun Liu</a>,
              <a>Kedai Zuo</a>,
              <b>Jianming Tong</b>,
              <a>Yan Ding</a>,
              and <a href="http://gr.xjtu.edu.cn/web/pengjuren">Pengju Ren</a>.<br>
            </div>
            <div class="venue">

              <i>International Conference on Field-Programmable Technology (<b style="color:#ff6600;">FPT</b>),
                2021.</i> <b>Full Paper</b>
            </div>
            <!-- <div class="insight">
                                          <i> <b> Insight 1: </b> Back-end optimization is latency bottleneck in SLAM for large map, and it comes with great parallelism suitable for hardware acceleration.</i> <br>
                                          <i> <b> Insight 2: </b> Leveraging parallelism in front-end feature extractions could achieve 4.55X, 40X speedup than eSLAM and CPU.</i>
                                       </div> -->
            <div class="links">
              <a onclick="displayid('abs4');">abstract</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/9609808">paper</a>
              <a href="https://github.com/SLAM-Hardware/acSLAM">code</a>
              <a onclick="displayid('bib4');">bibtex</a>
              <div id="abs4" style="display:none;">
                In order to fulfill the rich functions of the application layer, robust and accurate
                Simultaneous Localization and Mapping (SLAM) technique is very critical for robotics.
                However,
                due to the lack of sufficient computing power and storage capacity, it is challenging to
                delpoy
                high-accuracy SLAM in embedded devices efficiently. In this work, we propose a complete
                acceleration scheme, termed ac 2 SLAM, based on the ORB-SLAM2 algorithm including both
                front
                and
                back ends, and implement it on an FPGA platform. Specifically, the proposed ac 2 SLAM
                features
                with: 1) a scalable and parallel ORB extractor to extract sufficient keypoints and scores
                for
                throughput matching with 4% error, 2) a PingPong heapsort component (pp-heapsort) to
                select
                the
                significant keypoints, that could achieve single-cycle initiation interval to reduce the
                amount
                of data transfer between accelerator and the host CPU, and 3) the potential parallel
                acceleration strategies for the back-end optimization. Compared with running ORB-SLAM2 on
                the
                ARM processor, ac 2 SLAM achieves 2.1 × and 2.7 × faster in the TUM and KITTI datasets,
                while
                maintaining 10% error of SOTA eSLAM. In addition, the FPGA accelerated front-end achieves
                4.55
                ×
                and 40 × faster than eSLAM and ARM. The ac 2 SLAM is fully open-sourced at
                https://github.com/SLAM-Hardware/acSLAM.
              </div>
              <div id="bib4" style="display:none;">
                @INPROCEEDINGS{9609808,
                author={Wang, Cheng and Liu, Yingkun and Zuo, Kedai and Tong, Jianming and Ding, Yan and
                Ren,
                Pengju},
                booktitle={2021 International Conference on Field-Programmable Technology (ICFPT)},
                title={ac2SLAM: FPGA Accelerated High-Accuracy SLAM with Heapsort and Parallel Keypoint
                Extractor},
                year={2021},
                volume={},
                number={},
                pages={1-9},
                doi={10.1109/ICFPT52863.2021.9609808}}
              </div>
            </div>
          </div>
        </div>



        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/PIT.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>PIT: Processing-In-Transmission with Fine-Grained Data Manipulation Networks</b>
            </div>
            <div class="authors">
              <a href="https://www.researchgate.net/profile/Pengchen-Zong"> Pengchen Zong*</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=IHUKbtAAAAAJ"> Tian Xia*</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=3onwdYsAAAAJ"> Haoran Zhao</a>,
              <b> Jianming Tong</b>,
              <a> Zehua Li</a>,
              <a href="https://www.linkedin.com/in/wenzhe-zhao-6b273779/?originalSubdomain=cn"> Wenzhe
                Zhao</a>,
              <a href="https://scholar.google.com.hk/citations?user=iqMe3p8AAAAJ&hl=zh-CN">Nanning
                Zheng</a>,
              and
              <a href="http://gr.xjtu.edu.cn/web/pengjuren">Pengju Ren</a>.<br>
            </div>
            <div class="venue">
              <i> IEEE Transactions on Computers (<b style="color:#ff6600;">TOC</b>), 2021.</i>
            </div>
            <div class="links">
              <a onclick="displayid('abs2');">abstract</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/9311185/">paper</a>
              <a onclick="displayid('bib2');">bibtex</a>
              <div id="abs2" style="display:none;">
                In the domain of data parallel computation, most works focus on data flow optimization
                inside
                the PE array and favorable memory hierarchy to pursue the maximum parallelism and
                efficiency,
                while the importance of data contents has been overlooked for a long time. As we observe,
                for
                structured data, insights on the contents (i.e., their values and locations within a
                structured
                form) can greatly benefit the computation performance, as fine-grained data manipulation
                can
                be
                performed. In this paper, we claim that by providing a flexible and adaptive data path, an
                efficient architecture with capability of fine-grained data manipulation can be built.
                Specifically, we design SOM, a portable and highly-adaptive data transmission network,
                with
                the
                capability of operand sorting, non-blocking self-route ordering and multicasting. Based on
                SOM,
                we propose the processing-in-transmission architecture (PITA), which extends the
                traditional
                SIMD architecture to perform some fundamental data processing during its transmission, by
                embedding multiple levels of SOM networks on the data path. We evaluate the performance of
                PITA
                in two irregular computation problems. We first map the matrix inversion task onto PITA
                and
                show
                considerable performance gain can be achieved, resulting in 3x-20x speedup against Intel
                MKL,
                and 20x-40x against cuBLAS. Then we evaluate our PITA on sparse CNNs. The results indicate
                that
                PITA can greatly improve computation efficiency and reduce memory bandwidth pressure. We
                achieved 2x-9x speedup against several state-of-art accelerators on sparse CNN, where
                nearly
                100
                percent PE efficiency is maintained under high sparsity. We believe the concept of PIT is
                a
                promising computing paradigm that can enlarge the capability of traditional parallel
                architecture.
              </div>
              <div id="bib2" style="display:none;">
                @ARTICLE{9311185,
                author={Zong, Pengchen and Xia, Tian and Zhao, Haoran and Tong, Jianming and Li, Zehua and
                Zhao,
                Wenzhe and Zheng, Nanning and Ren, Pengju},
                journal={IEEE Transactions on Computers},
                title={PIT: Processing-In-Transmission With Fine-Grained Data Manipulation Networks},
                year={2021},
                volume={70},
                number={6},
                pages={877-891},
                doi={10.1109/TC.2020.3048233}}
              </div>
            </div>
          </div>
        </div>


        <div class="publication-item">
          <div class="pub-image">
            <img src="./images/cocoa.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
          </div>
          <div class="pub-content">
            <div class="title">
              <b>COCOA: Content-Oriented Configurable Architecture Based on Highly-Adaptive Data
                Transmission
                Networks</b>
            </div>
            <div class="authors">
              <a href="https://scholar.google.com/citations?hl=en&user=IHUKbtAAAAAJ"> Tian Xia</a>,
              <a href="https://www.researchgate.net/profile/Pengchen-Zong"> Pengchen Zong</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=3onwdYsAAAAJ"> Haoran Zhao</a>,
              <b> Jianming Tong</b>,
              <a href="https://www.linkedin.com/in/wenzhe-zhao-6b273779/?originalSubdomain=cn"> Wenzhe
                Zhao</a>,
              <a href="https://scholar.google.com.hk/citations?user=iqMe3p8AAAAJ&hl=zh-CN">Nanning
                Zheng</a>,
              and
              <a href="http://gr.xjtu.edu.cn/web/pengjuren">Pengju Ren</a>.<br>
            </div>
            <div class="venue">
              <!-- <a href="http://amirgholami.org/">Amir Gholami</a>, -->
              <i> Proceedings of the 2020 on Great Lakes Symposium on VLSI (<b style="color:#ff6600;">GLSVLSI</b>),
                2020.</i>
            </div>
            <div class="insight">
              <!-- <a href="http://amirgholami.org/">Amir Gholami</a>, -->
              <i> <b> Insight: </b> Adding NoC between Mem-Cache-CPU for supporting Sorting, Ordering and
                Multicasting (SOM) could boost 25X CPU perfromance for matrix inversion.</i>
            </div>
            <div class="insight">
              <a onclick="displayid('abs1');">abstract</a>
              <a href="https://dl.acm.org/doi/abs/10.1145/3386263.3406924">paper</a>
              <a onclick="displayid('bib1');">bibtex</a>
              <div id="abs1" style="display:none;">
                In domain of parallel computation, most works focus on optimizing PE organization or
                memory
                hierarchy to pursue the maximum efficiency, while the importance of data contents has been
                overlooked for a long time. Actually for structured data, insights on data contents (i.e.
                values
                and locations within a structured form) can greatly benefit the computation performance,
                as
                fine-grained data manipulation can be performed. In this paper, we claim that by providing
                a
                flexible and adaptive data path, an efficient architecture with capability of fine-grained
                data
                manipulation can be built. Specifically, we propose COCOA, a novel content-oriented
                configurable
                architecture, which integrates multi-functional data reorganization networks in
                traditional
                computing scheme to handle the contents of data during the transmission path, so that they
                can
                be processed more efficiently. We evaluate COCOA on various problems: complex matrix
                algorithm
                (matrix inversion) and sparse DNN. The results indicates that COCOA is versatile enough to
                achieve high computation efficiency in both cases.
              </div>
              <div id="bib1" style="display:none;">
                @inproceedings{10.1145/3386263.3406924,
                author = {Xia, Tian and Zong, Pengchen and Zhao, Haoran and Tong, Jianming and Zhao,
                Wenzhe
                and
                Zheng, Nanning and Ren, Pengju},
                title = {COCOA: Content-Oriented Configurable Architecture Based on Highly-Adaptive Data
                Transmission Networks},
                year = {2020},
                isbn = {9781450379441},
                publisher = {Association for Computing Machinery},
                address = {New York, NY, USA},
                url = {https://doi.org/10.1145/3386263.3406924},
                doi = {10.1145/3386263.3406924},
                abstract = {In domain of parallel computation, most works focus on optimizing PE
                organization
                or
                memory hierarchy to pursue the maximum efficiency, while the importance of data contents
                has
                been overlooked for a long time. Actually for structured data, insights on data contents
                (i.e.
                values and locations within a structured form) can greatly benefit the computation
                performance,
                as fine-grained data manipulation can be performed. In this paper, we claim that by
                providing
                a
                flexible and adaptive data path, an efficient architecture with capability of fine-grained
                data
                manipulation can be built. Specifically, we propose COCOA, a novel content-oriented
                configurable
                architecture, which integrates multi-functional data reorganization networks in
                traditional
                computing scheme to handle the contents of data during the transmission path, so that they
                can
                be processed more efficiently. We evaluate COCOA on various problems: complex matrix
                algorithm
                (matrix inversion) and sparse DNN. The results indicates that COCOA is versatile enough to
                achieve high computation efficiency in both cases.},
                booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
                pages = {253–258},
                numpages = {6},
                keywords = {transmission network, computing architecture, high-performance computing, data
                reorgonization},
                location = {Virtual Event, China},
                series = {GLSVLSI '20}
                }
              </div>

            </div> <!-- end publications-list -->
      </details>

      <details class="collaborative-pubs">
        <summary>
          <h3>Workshops</h3>
        </summary>
        <div class="news-container" style="display: block;">
          <div class="publication-item">
            <div class="pub-content">
              <div class="title">
                <a><b>FLOFA: Federated Once-for-All Networks</b></a>
              </div>
              <div class="authors">
                <a href="https://www.cc.gatech.edu/~akhare39/">Alind Khare</a>,
                <b>Jianming Tong</b>,
                <a href="https://www.linkedin.com/in/animesh-agrawal-b68a1b170/">Animesh Agrawal</a>,
                <a href="https://sahnimanas.github.io/">Manas Sahni</a>,
                <a href="https://www.linkedin.com/in/shreyavarshini/">Shreya Varshini</a>,
                <a href="https://www.cc.gatech.edu/~atumanov/">Alexey Tumanov</a>,
                <a href="https://cs.illinois.edu/about/people/faculty/jimeng">Jimeng Sun</a>,
                <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>,
                <a href="https://vsarkar.cc.gatech.edu/">Vivek Sarkar</a>, and
                <a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a>.
              </div>
              <div class="venue">
                <i><a href="https://sysml4health.github.io/">SysML4Health: Scalable Systems for ML-driven Analytics in
                    Healthcare Workshop</a>, <b>MLSys</b></i>
                2021
              </div>
            </div>
          </div>
          <div class="publication-item">
            <div class="pub-content">
              <div class="title">
                <b>A Reconfigurable Accelerator with Data Reordering Support for Low-Cost On-Chip Dataflow
                  Switching</b>
              </div>
              <div class="authors">
                <b>Jianming Tong</b>,
                <a href="https://www.linkedin.com/in/anirudh-itagi/?originalSubdomain=in">Anirudh Itagi</a>,
                <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
              </div>
              <div class="venue">
                <i><a href="https://sites.google.com/g.harvard.edu/on-device-workshop-23">The 3rd On-Device
                    Intelligence
                    Workshop@</a><b style="color:#ff6600;">MLSys'23</b></i>
              </div>
              <div class="profile-links">
                <a onclick="displayid('abs_wp2');">abstract</a>
                <a href="publications/LAMBDA_poster.pdf">poster</a>
                <div id="abs_wp2" style="display:none;">
                  The increasing prevalence of Machine Learning (ML) in various applications has led to the
                  emergence of
                  ML models with diverse structures, types and sizes. The ML model inference boils down to the
                  execution
                  of different dataflows (tiling, ordering, parallelism, and shapes), and using the optimal dataflow
                  can
                  reduce latency by up to two orders of magnitude over an inefficient one. Unfortunately,
                  reconfiguring
                  hardware for different dataflows involves on-chip data reordering and datapath reconfigurations,
                  leading
                  to non-trivial overheads that hinder ML accelerators from exploiting different dataflows,
                  resulting in
                  suboptimal performance. To address this challenge, we propose LAMBDA, an innovative accelerator
                  that
                  leverages a novel multi-stage reduction network called Additive Folded Fat Tree (AFFT) for
                  reordering
                  data in data reduction (RIR), enabling seamless switching between optimal dataflows with
                  negligible
                  latency and resources overhead. LAMBDA creates an opportunity to change the optimal dataflows at
                  the
                  granularity of layers without incurring additional latency overhead, and to explore the optimal
                  dataflows on the real hardware with faster and more precise evaluation results. LAMBDA
                  demonstrates a
                  0.5~2X speed up in end-to-end inference latency than the SotA Xilinx DPU on Xilinx ZCU 104
                  embedded
                  FPGA
                  board.
                </div>
              </div>
              <!-- <div class="insight">
                              <i> <b> Insight 1: </b> Data layout in on-chip storage affects performance of dataflows significantly, and dataflows encounter up 30X longer latency under a mismatching on-chip data layout.</i> <br>
                              <i> <b> Insight 2: </b> To change data layout when switching dataflows, the additional layout reordering incur extra latency overhead. We propose Reordering in Reduction (RIR) to hide latency of layout reordering completely behind reduction, enable switching optimal dataflows with negligible switching overhead. </i>
                           </div> -->
            </div>
          </div>
          <div class="publication-item">
            <div class="pub-content">
              <div class="title">
                <b>ReLU-FHE: Low-cost Accurate ReLU Polynoimal Approximation in Fully Homomorphic Encryption Based
                  ML
                  Inference</b>
              </div>
              <div class="authors">
                <a href="https://www.linkedin.com/in/jingtian-dang-568615207/">Jingtian Dang*</a>,
                <b>Jianming Tong*</b>,
                <a href="https://scholar.google.com/citations?user=QLN76UUAAAAJ&hl=en">Anupam Golder</a>,
                <a href="https://sites.gatech.edu/ece-callie/">Callie Hao</a>,
                <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
              </div>
              <div class="venue">
                <i><a href="https://sites.google.com/g.harvard.edu/on-device-workshop-23">The 3rd On-Device
                    Intelligence
                    Workshop@</a><b style="color:#ff6600;">MLSys'23</b></i>
              </div>
              <div class="profile-links">
                <a onclick="displayid('abs_wp1');">abstract</a>
                <a
                  href="https://assets.researchsquare.com/files/rs-2910088/v1_covered_0e6c94bd-1499-4b2c-b414-902c232b490c.pdf?c=1683777877">paper</a>
                <a href="https://github.com/TorchFHE/PAF-FHE.">code</a>
                <a onclick="displayid('bib_wp1');">bibtex</a>
                <div id="abs_wp1" style="display:none;">
                  Machine learning (ML) is getting more pervasive. Wide adoption of ML in healthcare, facial
                  recognition,
                  and blockchain involves private and sensitive data. One of the most promising candidates for
                  inference
                  on encrypted data, termed Fully Homomorphic Encryption (FHE), preserves the privacy of both data
                  and
                  the
                  ML model. However, it slows down plaintext inference by six magnitudes, with a root cause of
                  replacing
                  non-polynomial operators with latency-prohibitive 27-degree Polynomial Approximated Function
                  (PAF).
                  While prior research has investigated low-degree PAFs, naive stochastic gradient descent (SGD)
                  training
                  fails to converge on PAFs with degrees higher than 5, leading to limited accuracy compared to the
                  state-of-the-art 27-degree PAF. Therefore, we propose four training techniques to enable
                  convergence
                  in
                  the post-approximation model using PAFs with an arbitrary degree, including (1) Dynamic Scaling
                  (DS)
                  and
                  Static Scaling (SS) to enable minimal approximation error during approximation, (2) Coefficient
                  Tuning
                  (CT) to obtain a good initial coefficient value for each PAF, (3) Progressive Approximation (PA)
                  to
                  simply the two-variable regression optimization problem into single-variable for fast and easy
                  convergence, and (4) Alternate Training (AT) to retraining the post-replacement PAFs and other
                  linear
                  layers in a decoupled divide-and-conquer manner. A combination of DS/SS, CT, PA, and AT enables
                  the
                  exploration of accuracy-latency space for FHEdomain ReLU replacement. Leveraging the proposed
                  techniques, we propose a systematic approach (PAF-FHE) to enable low-degree PAF to demonstrate the
                  same
                  accuracy as SotA high-degree PAFs. We evaluated PAFs with various degrees on different models and
                  variant datasets, and PAF-FHE consistently enables low-degree PAF to achieve higher accuracy than
                  SotA
                  PAFs. Specifically, for ResNet-18 under the ImageNet-1k dataset, our spotted optimal 12-degree PAF
                  reduces 56% latency compared to the SotA 27-degree PAF with the same post-replacement accuracy
                  (69.4%).
                  While as for VGG-19 under the CiFar-10 dataset, optimal 12-degree PAF achieves even 0.84% higher
                  accuracy with 72% latency saving. Our code is open-sourced at:
                  https://github.com/TorchFHE/PAF-FHE.
                </div>
                <div id="bib_wp1" style="display:none;">
                  @misc {PPR:PPR658940,
                  Title = {PAF-FHE: Low-Cost Accurate Non-Polynomial Operator Polynomial Approximation in Fully
                  Homomorphic Encryption Based ML Inference},
                  Author = {Dang, Jingtian and Tong, Jianming and Golder, Anupam and Raychowdhury, Arijit and Hao,
                  Cong
                  and Krishna, Tushar},
                  DOI = {10.21203/rs.3.rs-2910088/v1},
                  Abstract = {Machine learning (ML) is getting more pervasive. Wide adoption of ML in healthcare,
                  facial
                  recognition, and blockchain involves private and sensitive data. One of the most promising
                  candidates
                  for inference on encrypted data, termed Fully Homomorphic Encryp-tion (FHE), preserves the privacy
                  of
                  both data and the ML model. However, it slows down plaintext inference by six magnitudes, with a
                  root
                  cause of replacing non-polynomial operators with latency-prohibitive 27-degree Polynomial
                  Approximated
                  Function (PAF). While prior research has investigated low-degree PAFs, naive stochastic gradient
                  descent
                  (SGD) training fails to converge on PAFs with degrees higher than 5, leading to limited accuracy
                  compared to the state-of-the-art 27-degree PAF. Therefore, we propose four training techniques to
                  enable
                  convergence in the post-approximation model using PAFs with an arbitrary degree, including (1)
                  Dynamic
                  Scaling (DS) and Static Scaling (SS) to enable minimal approximation error during approximation,
                  (2)
                  Coefficient Tuning (CT) to obtain a good initial coefficient value for each PAF, (3) Progressive
                  Approximation (PA) to simply the two-variable regression optimization problem into single-variable
                  for
                  fast 1 and easy convergence, and (4) Alternate Training (AT) to retraining the post-replacement
                  PAFs
                  and
                  other linear layers in a decoupled divide-and-conquer manner. A combination of DS/SS, CT, PA, and
                  AT
                  enables the exploration of accuracy-latency space for FHE-domain ReLU replacement. Leveraging the
                  proposed techniques, we propose a systematic approach (PAF-FHE) to enable low-degree PAF to
                  demonstrate
                  the same accuracy as SotA high-degree PAFs. We evaluated PAFs with various degrees on different
                  models
                  and variant datasets, and PAF-FHE consistently enables low-degree PAF to achieve higher accuracy
                  than
                  SotA PAFs. Specifically, for ResNet-18 under the ImageNet-1k dataset, our spotted optimal
                  12-degree
                  PAF
                  reduces 56% latency compared to the SotA 27-degree PAF with the same post-replacement accuracy
                  (69.4%).
                  While as for VGG-19 under the CiFar-10 dataset, optimal 12-degree PAF achieves even 0.84% higher
                  accuracy with 72% latency saving. Our code is open-sourced at:
                  https://github.com/TorchFHE/PAF-FHE},
                  Publisher = {Research Square},
                  Year = {2023},
                  URL = {https://doi.org/10.21203/rs.3.rs-2910088/v1},
                  }
                </div>
              </div>
            </div>
          </div>
          <div class="publication-item">
            <div class="pub-content">
              <div class="title">
                <b>FastSwtich: Enabling Real-time DNN Switching via Weight-Sharing</b>
              </div>
              <div class="authors">
                <b>Jianming Tong</b>,
                <a href="https://www.linkedin.com/in/yangyu-chen/">Yangyu Chen</a>,
                <a href="https://www.linkedin.com/in/yue-pan-061439192/">Yue Pan</a>,
                <a href="https://www.linkedin.com/in/abhimanyu-bambhaniya/">Abhimanyu Bambhaniya</a>,
                <a href="https://www.cc.gatech.edu/~akhare39/">Alind Khare</a>,
                <a href="https://sites.google.com/view/taekyungheo/">Taekyung Heo</a>,
                <a href="https://www.cc.gatech.edu/~atumanov/">Alexey Tumanov</a>, and
                <a href="https://tusharkrishna.ece.gatech.edu/">Tushar Krishna</a>
              </div>
              <div class="venue">
                <i><a
                    href="https://research.facebook.com/architecture-compiler-and-system-support-for-multimodel-dnn-workloads-workshop/">The
                    2nd Architecture, Compiler, and System Support for Multi-model DNN Workloads Workshop@</a><b
                    style="color:#ff6600;">ISCA'22</b></i>
              </div>
              <div class="profile-links">
                <a onclick="displayid('abs_wp2');">abstract</a>
                <a href="publications/SUSHI_MLSys2023.pdf">paper</a>
                <a onclick="displayid('bib_wp2');">bibtex</a>
                <div id="abs_wp2" style="display:none;">
                  A growing number of applications depend on Machine Learning (ML) functionality and benefits from
                  both
                  higher quality ML predictions and better timeliness (latency) at the same time. A growing body of
                  research in computer architecture, ML, and systems software literature focuses on reaching better
                  latency/accuracy tradeoffs for ML models. Efforts include compression, quantization, pruning,
                  early-exit
                  models, mixed DNN precision, as well as ML inference accelerator designs that minimize latency and
                  energy, while preserving delivered accuracy. All of them, however, yield improvements for a single
                  static point in the latency/accuracy tradeoff space. We make a case for applications that operate
                  in
                  dynamically changing deployment scenarios, where no single static point is optimal. We draw on a
                  recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that uses
                  (activates) different SubNets within this weight-shared construct. This creates an opportunity to
                  exploit the inherent temporal locality with our proposed SubGraph Stationary (SGS) optimization.
                  We
                  take
                  a hardware-software co-design approach with a real implementation of SGS in SushiAccel and the
                  implementation of a software scheduler SushiSched controlling which SubNets to serve and what to
                  cache
                  in real-time. Combined, they are vertically integrated into SUSHI---an inference serving stack.
                  For
                  the
                  stream of queries, SUSHI yields up to 25% improvement in latency, 0.98% increase in served
                  accuracy.
                  SUSHI can achieve up to 78.7% off-chip energy savings.
                </div>
                <div id="bib_wp2" style="display:none;">
                  Stay in tune
                </div>
              </div>
            </div>
          </div>
        </div>
      </details>

    </section>
    <br>

    <!-- Book -->
    <section class="section">
      <h2 class="section-title">Book</h2>
      <div class="publication-item">
        <div class="pub-image">
          <img src="./images/book.png" width="95%" style="display: block;
                                       margin-left: auto; margin-left:
                                       auto;">
        </div>
        <div class="pub-content">
          <b>On-chip Network (Chinese)</b><br>
          Translator: Pengju Ren, Tian Xia, Jianming Tong, Pengcheng Zong, Haoran Zhao<br>
          <b>Abstract</b><br>
          This book targets engineers and researchers familiar with basic computer architecture concepts who are
          interested in learning about on-chip networks. This work is designed to be a short synthesis of the most
          critical concepts in on-chip network design. It is a resource for both understanding on-chip network basics
          and for providing an overview of state of-the-art research in on-chip networks. <br>
          <div class="profile-links">
            <a href="http://www.zxhsd.com/kgsm/ts/2021/01/29/5329526.shtml"><b>Chinese Version</b></a>
            <br>
            <a href="https://www.morganclaypool.com/doi/abs/10.2200/S00772ED1V01Y201704CAC040"><b>English Version --
                Free for University</b></a>
            <br>
            <a href="http://www.morganclaypoolpublishers.com/catalog_Orig/product_info.php?products_id=1081"><b>Original
                Version</b></a>
          </div>
        </div>
    </section>



    <br>
    <br>


    <!-- Education -->
    <section class="section" id="education">
      <h2 class="section-title">Education</h2>
      <div class="profile-item">
        <div class="profile-content"><b>Georgia Institute of Technology</b>, USA<br>
          Ph.D. in Computer Science
          • Jan. 2021 to Present <br>
          Advisor: Prof.<a href="https://tusharkrishna.ece.gatech.edu/"> Tushar Krishna </a> <br>
        </div>
        <div class="profile-image"><img src="./images/gt_logo.png" width="105%" style="display: flex;
                                             justify-content: center;"></div>
      </div>
      <div class="profile-item">
        <div class="profile-content"><b>Georgia Institute of Technology</b>, USA<br>
          MS. in Computer Science
          • Jan. 2021 to May 2024 <br>
          Advisor: Prof.<a href="https://tusharkrishna.ece.gatech.edu/"> Tushar Krishna </a> <br></div>
        <div class="profile-image"><img src="./images/gt_logo.png" width="105%" style="display: flex;
                                             justify-content: center;"></div>
      </div>
      <div class="profile-item">
        <div class="profile-content"><b>Xi'An Jiaotong University</b>, China<br>
          B.E. in Electrical Engineering • Sep. 2016 to Jun. 2020 <br>
          Advisor: Prof.<a href="http://gr.xjtu.edu.cn/web/pengjuren"> Pengju Ren </a> <br></div>
        <div class="profile-image"><img src="./images/xjtu-logo.jpg" width="105%" style="display: flex;
                                             justify-content: center;"></div>
      </div>
    </section>
    <br>
    <br>

    <!-- Experience -->
    <section class="section" id="experience">
      <h2 class="section-title">Experience</h2>
      <div class="profile-item">
        <div class="profile-content"><b>Google</b>, USA<br>
          Student Researcher
          • Aug. 2024 to Apr. 2025 <br>
          Host: <a href="https://qconnewyork.com/speakers/asraali">Asra Ali</a> <a
            href="https://www.linkedin.com/in/jevin-jiang/">Jevin Jiang</a> <br>
        </div>
        <div class="profile-image"><img src="./images/google_logo.png" width="55%" style="display: flex;
                                            justify-content: center;"></div>
      </div>

      <div class="profile-item">
        <div class="profile-content"><b>Massachusetts Institute of Technology</b>, USA<br>
          Research Associative
          • Feb. 2024 to Feb. 2025 <br>
          Advisor: Prof.<a href="https://tusharkrishna.ece.gatech.edu/"> Tushar Krishna </a>, Host:
          Prof.<a href="https://csg.csail.mit.edu/Users/arvind/"> Arvind </a> <br></div>
        <div class="profile-image"><img src="./images/MIT.png" width="105%" style="display: flex;
                                            justify-content: center;"></div>
      </div>
      <div class="profile-item">
        <div class="profile-content"><b>Rivos Inc.</b>, Mountain View CA<br>
          Ph.D. Intern in Computer Architecture
          • May. 2023 to Aug 2023 <br>
          <!-- <b>Overall GPA: 5.0/5.0</b> --></div>
        <div class="profile-image"><img src="./images/rivos_cit.jpg" width="100%" style="display: block;
                                             margin-left: auto; margin-right:
                                             auto;"></div>
      </div>
      <div class="profile-item">
        <div class="profile-content"><b>Pacific Northwest National Lab (PNNL)</b>, Battelle WA<br>
          Research Intern in Computer Architecture
          • Jun. 2022 to Aug
          2022 <br>
          <!-- <b>Overall GPA: 5.0/5.0</b> --></div>
        <div class="profile-image"><img src="./images/pnnl.png" width="100%" style="display: block;
                                             margin-left: auto; margin-right:
                                             auto;"></div>
      </div>
      <div class="profile-item">
        <div class="profile-content"><b>Alibaba DAMO Academy</b>, Beijing<br>
          Research Intern in Fully Homormophic Encryption Accelerator • Jul. 2021 to
          Aug. 2021</div>
        <div class="profile-image"><img src="./images/damo.jpg" width="100%" style="display: flex;
                                             justify-content: center;"></div>
      </div>
      <div class="profile-item">
        <div class="profile-content"><b>Tsinghua University</b>, Beijing<br>
          (Visiting Student) Research Assistant in Robotics • Aug. 2020 to
          Jan. 2021 <br>
          Advisor: Prof.<a href="http://nicsefc.ee.tsinghua.edu.cn/people/yu-wang/"> Yu Wang</a></div>
        <div class="profile-image"><img src="./images/Tsinghua-University-logo.avif" width="100%" style="display: flex;
                                             justify-content: center;"></div>
      </div>
    </section>
    <br>

    <!-- Award -->
    <section class="section" id="awards">
      <h2 class="section-title">Honors and Awards</h2>
      <div class="profile-item">
        <div class="profile-content"><b><a
              href="https://www.dac.com/Attend/Students-Scholarships/Young-Student-Fellow-Program">2nd Place in
              University
              DEMO</a></b><br>
          first demonstration of superiority of AI ASICs in HE acceleration <br>
          Jun. 2025</div>
        <div class="profile-image"><img src="./images/dac.png" width="100%" style="display: flex;
                                             justify-content: center;"></div>
      </div>
      <div class="profile-item">
        <div class="profile-content"><b><a href="https://mlcommons.org/2024/06/2024-mlc-rising-stars/">ML and System
              Rising Star</a></b><br>
          in recognition of reconfigurable AI Computing <br>
          Jul. 2024</div>
        <div class="profile-image"><img src="./images/mlcommon.png" width="105%" style="display: flex;
                                             justify-content: center;"></div>
      </div>
      <div class="profile-item">
        <div class="profile-content"><b><a href="https://www.industry-academia.org/mit-2023.html">Best Poster
              Award</a></b> <br>
          in recognition of reconfigurable AI Computing <br>
          Sep. 2023</div>
        <div class="profile-image"><img src="./images/iap.png" width="105%" style="display: flex;
                                             justify-content: center;"></div>
      </div>
      <div class="profile-item">
        <div class="profile-content"><b>Winner in <a
              href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america">Qualcomm
              Innovation Fellowship</a></b><br>
          in recognition of runtime latency-accuracy navigation <br>
          Jul. 2023</div>
        <div class="profile-image"><img src="./images/qualcomm.png" width="105%" style="display: flex;
                                             justify-content: center;"></div>
      </div>
    </section>
    <br>

    <!-- Award -->
    <section class="section" id="services">
      <h2 class="section-title">Services</h2>
      <div class="publication-item">
        <div class="pub-content"><b>Artifact Evaluation Committee in
            <a href="https://www.asplos-conference.org/asplos2024/">ASPLOS'24</a>,
            <a href="https://www.asplos-conference.org/asplos2024/">ISCA'24</a>
          </b></div>
      </div>
      <div class="publication-item">
        <div class="pub-content"><b>Reviewer in
            <a href="https://www.asplos-conference.org/asplos2024/">TOC'26</a>
            <a href="https://www.asplos-conference.org/asplos2024/">MLSys'25</a>,
            <a href="https://www.asplos-conference.org/asplos2024/">IEEE Micro'25</a>,
            <a href="https://www.asplos-conference.org/asplos2024/">CAL'25</a>,
            <a href="https://www.asplos-conference.org/asplos2024/">TOC'25</a>,
            <a href="https://www.asplos-conference.org/asplos2024/">IROS'25</a>,
            <a href="https://www.asplos-conference.org/asplos2024/">AsiaCCS'25</a>,
            <a href="https://www.asplos-conference.org/asplos2024/">TVLSI'24</a>,
            <a href="https://www.asplos-conference.org/asplos2024/">ICRA'24</a>
          </b>
        </div>
      </div>
      <div class="publication-item">
        <div class="pub-content"><b>Steering Committee in Computer Architecture Student Association (CASA) <a
              href="https://www.sigarch.org/casa/"></a> </b> <br>
          interview Prof. Mengjia Yan <br>
          interview Prof. Todd Austin <br>
          Co-organize JOBS workshop </div>
      </div>
    </section>
    <br>

    <!-- Life -->
    <section class="section" id="life">
      <h2 class="section-title">Life</h2>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
        <tbody>
          <div class="profile-links">
            <b>I love writing songs, playing piano, guitar, singing and fitting in.</b> I'm available on major music
            distributor like Apple Music, Spotify, QQ music and NetEase etc (Search my name in platforms to find me
            XD). Some thing about me could be also found here<br>
            <b><a href="https://music.apple.com/us/artist/%E4%BD%9F%E5%81%A5%E9%93%AD/1679883128">Apple Music</a></b>
            <b><a
                href="https://open.spotify.com/artist/4giALEGXP8Di4rHMZHsKmP?si=LYO9AGN8QYWPE8yiveWFMA">Spotify</a></b>
            <b><a href="https://y.qq.com/n/ryqq/singer/003VgCPP0VDIu0">QQ Music</a></b>
            <b><a href="https://music.163.com/#/artist?id=35727121">Netease Music</a></b>
            <b><a href="https://www.youtube.com/embed/GNeVtAYSOuI">Youtube</a></b>
            <b><a href="https://www.youtube.com/channel/UCEr_Js-ulnM20ehogPZW0MQ">Youtube - Magic Mushroom</a></b>
            <b><a href="https://space.bilibili.com/2028910667">Bilibili - Magic Mushroom</a></b>
          </div>
        </tbody>
      </table>
    </section>

    <footer style="display: flex; justify-content: flex-end; padding: 10px;">
      <b>Last Updated: Dec 28, 2025</b>
    </footer>
    </table>

</body>

</html>